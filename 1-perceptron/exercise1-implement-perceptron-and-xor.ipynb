{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1: Implement a Single-Layer Perceptron for AND and XOR Gates\n",
        "\n",
        "Artificial neurons combine inputs with weights, add a bias, and produce an output. But can this simple computation solve any problem we throw at it? Time to find out by building one yourself and testing it on classic logic problems.\n",
        "\n",
        "> **Overview**: Implement a perceptron class in PyTorch, manually find weights that solve the AND gate (success!), try multiple approaches to solve XOR (they'll all fail...), then explore why combining multiple neurons changes everything.\n",
        "> \n",
        "> **Scenario**: Your movie recommendation system needs complex decision logic. You'll configure a neuron to implement \"recommend only when BOTH rating AND popularity are high\" (AND logic), then attempt \"recommend when EITHER is high but NOT both\" (XOR logic). To understand if single neurons can handle these rules, you'll first test them on simplified binary logic gates, which reveal the fundamental capabilities and limitations that apply to all perceptrons—whether they're classifying movies, or any other data.\n",
        "> \n",
        "> **Goal**: Discover the strengths and limitations of single-layer networks to understand why depth matters in neural networks.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, NumPy, Matplotlib\n",
        "> \n",
        "> **Estimated Time**: 15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's start by importing our libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'assets/exercise1'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understand the logic gate problems\n",
        "\n",
        "Before we start coding, let's understand what AND and XOR gates are, and why they're important test cases for neural networks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What is linear separability?** A dataset is linearly separable if you can draw a straight line (in 2D) or a flat plane (in higher dimensions) that perfectly separates the two classes. \n",
        "> \n",
        "> - AND gate: ✓ Linearly separable (one line works)\n",
        "> - XOR gate: ✗ Not linearly separable (no single line works)\n",
        "> \n",
        "> This geometric property determines whether a single neuron can solve the problem!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Truth table inputs shared by both gates\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Corresponding outputs for each logic gate\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Create side-by-side plots for AND and XOR\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Color and label settings for class 0 and class 1\n",
        "colors = ['#ff7f0e', '#1f77b4']\n",
        "labels_text = ['Output = 0', 'Output = 1']\n",
        "\n",
        "\n",
        "def plot_gate(ax, X, y, title, subtitle):\n",
        "    \"\"\"\n",
        "    Plot a truth table for a binary gate:\n",
        "    - scatters points for each input pair\n",
        "    - colors them according to their output class\n",
        "    - annotates each point with (x1,x2) and output\n",
        "    \"\"\"\n",
        "\n",
        "    # Plot points for each output class (0 and 1)\n",
        "    for cls in (0, 1):\n",
        "        mask = (y == cls)\n",
        "        ax.scatter(\n",
        "            X[mask, 0], X[mask, 1],\n",
        "            c=colors[cls],\n",
        "            s=300,\n",
        "            label=labels_text[cls],\n",
        "            edgecolors='black',\n",
        "            linewidth=2\n",
        "        )\n",
        "\n",
        "    # Annotate each point with its input and output value\n",
        "    for (x1, x2), out in zip(X, y):\n",
        "        ax.text(\n",
        "            x1, x2 - 0.18,\n",
        "            f\"({x1},{x2})\\n→{out}\",\n",
        "            ha='center',\n",
        "            fontsize=10,\n",
        "            fontweight='bold'\n",
        "        )\n",
        "\n",
        "    # Apply consistent visual formatting to the axes\n",
        "    ax.set_xlabel('Input 1', fontsize=12)\n",
        "    ax.set_ylabel('Input 2', fontsize=12)\n",
        "    ax.set_title(f'{title}\\n({subtitle})', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim(-0.3, 1.3)\n",
        "    ax.set_ylim(-0.3, 1.3)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "\n",
        "\n",
        "# Plot AND and XOR truth tables using the shared function\n",
        "plot_gate(axes[0], X, y_and, 'AND Gate', 'True if BOTH inputs are 1')\n",
        "plot_gate(axes[1], X, y_xor, 'XOR Gate', 'True if inputs DIFFER')\n",
        "\n",
        "# Layout and export\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'truth_tables.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Summary text\n",
        "print(\"\\nKey observation:\")\n",
        "print(\"• AND gate: Classes are LINEARLY SEPARABLE\")\n",
        "print(\"• XOR gate: Classes are NOT linearly separable\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why these gates matter**: The AND and XOR gates are classic test cases for neural networks. The AND gate is linearly separable—you can draw a straight line to separate the outputs. But XOR requires separating opposite corners, which needs a curved boundary. \n",
        "> \n",
        "> This is the problem that stumped early AI researchers and led to the first AI winter in 1969. Understanding why XOR fails for single neurons is key to understanding why we need deep networks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implement the Perceptron class\n",
        "\n",
        "Now you'll build a reusable Perceptron class that can be applied to any binary classification problem.\n",
        "\n",
        "**NOTE: In PyTorch, `forward()` is the standard method name for computing a neural network's output. Think of it as _\"what happens when data flows through this layer?\"_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A simple perceptron for binary classification.\n",
        "    Computes: z = w1*x1 + w2*x2 + b, then output = 1 if z > 0 else 0\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size=2):\n",
        "        \"\"\"\n",
        "        Initialize perceptron with random weights and bias.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Number of input features (default 2 for logic gates)\n",
        "        \"\"\"\n",
        "        # TODO: Initialize weights as a tensor of shape (input_size,)\n",
        "        # HINT: You need a 1D tensor with input_size random values.\n",
        "        # Reference: https://docs.pytorch.org/docs/stable/torch.html#random-sampling\n",
        "        self.weights =  # Add your code here\n",
        "        \n",
        "        # TODO: Initialize bias as a single value tensor\n",
        "        # HINT: The bias is just one (random) number, but it needs to be a tensor. What shape creates a tensor with exactly one element?\n",
        "        # Reference: https://docs.pytorch.org/docs/stable/torch.html#random-sampling\n",
        "        self.bias =  # Add your code here\n",
        "    \n",
        "    def set_weights(self, weights, bias):\n",
        "        \"\"\"\n",
        "        Manually set the weights and bias.\n",
        "        Useful for testing specific configurations.\n",
        "        \"\"\"\n",
        "        # TODO: Convert inputs to torch tensors and store them\n",
        "        # HINT: You're given weights as a list (e.g., [1.0, 1.0]). Which PyTorch function converts a Python list into a float tensor?\n",
        "        # Reference: https://docs.pytorch.org/docs/stable/tensors.html\n",
        "        self.weights =  # Add your code here\n",
        "        self.bias =  # Add your code here\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute the perceptron output for input X.\n",
        "        \n",
        "        Args:\n",
        "            X: Input tensor of shape (N, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            predictions: Binary predictions (0 or 1) of shape (N,)\n",
        "            z: Raw activations before step function of shape (N,)\n",
        "        \"\"\"\n",
        "        # TODO: Compute the weighted sum z = X @ weights + bias\n",
        "        # HINT: Think about the perceptron equation: z = w₁×x₁ + w₂×x₂ + b. In matrix form, this is a dot product. What do you multiply, and what do you add?\n",
        "        # Reference: https://docs.pytorch.org/docs/stable/tensors.html\n",
        "        z =  # Add your code here\n",
        "        \n",
        "        # TODO: Apply step activation function\n",
        "        # HINT: Output should be 1 if z > 0, else 0. How do you convert True/False to 1.0/0.0?\n",
        "        # Reference: https://docs.pytorch.org/docs/stable/generated/torch.Tensor.float.html\n",
        "        predictions =  # Add your code here\n",
        "        \n",
        "        return predictions, z\n",
        "    \n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate accuracy on given data.\n",
        "        \n",
        "        Args:\n",
        "            X: Input features\n",
        "            y: True labels\n",
        "        \n",
        "        Returns:\n",
        "            accuracy: Fraction of correct predictions\n",
        "        \"\"\"\n",
        "        # TODO: Get predictions using forward pass\n",
        "        # HINT: You already wrote the forward method above, so you can just call it with your input.\n",
        "        # Note that forward() returns two things - which one do you need for accuracy?\n",
        "        predictions, _ =  # Add your code here\n",
        "        \n",
        "        # TODO: Compare predictions with true labels and compute mean\n",
        "        # HINT: Chain three operations: (1) compare predictions to true labels with ==, (2) convert the boolean result to numbers, (3) take the average. \n",
        "        # The mean of 0s and 1s gives you the fraction that are correct!\n",
        "        # Reference: https://docs.pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html?utm_source=chatgpt.com#more-math-with-tensors\n",
        "        accuracy =  # Add your code here\n",
        "        \n",
        "        return accuracy.item()\n",
        "\n",
        "# Test the Perceptron class with random initialization\n",
        "test_perceptron = Perceptron(input_size=2)\n",
        "print(\"Perceptron class implemented!\")\n",
        "print(f\"\\nInitial random weights: {test_perceptron.weights}\")\n",
        "print(f\"Initial random bias: {test_perceptron.bias}\")\n",
        "\n",
        "# Test on AND gate data\n",
        "X_and_tensor = torch.FloatTensor(X)\n",
        "y_and_tensor = torch.FloatTensor(y_and)\n",
        "preds, z = test_perceptron.forward(X_and_tensor)\n",
        "print(f\"\\nRandom predictions on AND gate: {preds}\")\n",
        "print(f\"True labels: {y_and_tensor}\")\n",
        "print(f\"Random accuracy: {test_perceptron.accuracy(X_and_tensor, y_and_tensor):.1%}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Implementation insight**: You've just implemented the core of a neural network! Every neuron in modern deep learning performs this same computation: weighted sum + bias, followed by a decision rule. \n",
        "> \n",
        "> The step function (output 1 if positive, else 0) is the simplest decision rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Part A - Solve the AND gate\n",
        "\n",
        "Your first challenge: find weights and bias that correctly implement the AND gate logic."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Linear algebra refresher: Understanding the decision boundary**\n",
        "> \n",
        "> The perceptron creates a decision boundary defined by: **w₁×x₁ + w₂×x₂ + b = 0**\n",
        "> \n",
        "> This is the equation of a line! Here's what each component controls:\n",
        "> - The ratio **w₁/w₂** determines the **slope** (angle) of the line\n",
        "> - The **bias b** determines the **position** (shifts the line left/right, up/down)\n",
        "> - Points above the line have z > 0 (predict 1)\n",
        "> - Points below the line have z < 0 (predict 0)\n",
        "> \n",
        "> **Finding weights and bias systematically:**\n",
        "> \n",
        "> 1. **Visualize first**: Sketch the 4 points on paper: (0,0), (0,1), (1,0), (1,1)\n",
        "> 2. **Draw a line**: Try to separate the classes with a straight line\n",
        "> 3. **Start simple**: Try equal weights (w₁=1, w₂=1) first, then adjust bias\n",
        "> 4. **Test each point**: Calculate z = w₁×x₁ + w₂×x₂ + b for each input\n",
        ">    - For AND gate: You need (1,1) to give z > 0, but (0,0), (0,1), (1,0) to give z < 0\n",
        ">    - What bias value makes this work when w₁=1 and w₂=1?\n",
        "> \n",
        "> **Example calculation**: If w₁=1, w₂=1, b=-1:\n",
        "> - Input (0,0): z = 1×0 + 1×0 + (-1) = -1 → predicts 0 ✓\n",
        "> - Input (1,1): z = 1×1 + 1×1 + (-1) = 1 → predicts 1 ✓\n",
        "> - But input (0,1): z = 1×0 + 1×1 + (-1) = 0 → predicts 0 (barely!)\n",
        "> \n",
        "> **Pro tip**: Make bias slightly more negative (like -1.5) to create a safety margin.\n",
        "> \n",
        "> _**Not confident with line equations?**_ Check out [Linear Regression Equation Explained](https://statisticsbyjim.com/regression/linear-regression-equation/) for a quick refresher!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a perceptron for AND gate\n",
        "and_perceptron = Perceptron(input_size=2)\n",
        "\n",
        "# TODO: Find weights and bias that solve the AND gate\n",
        "# HINT: **Step-by-step approach**:\n",
        "#   > 1. Try equal weights first: w₁=1, w₂=1\n",
        "#   > 2. Calculate z for each input:\n",
        "#   >    - (0,0): z = 1×0 + 1×0 + b = b\n",
        "#   >    - (0,1): z = 1×0 + 1×1 + b = 1 + b  \n",
        "#   >    - (1,0): z = 1×1 + 1×0 + b = 1 + b\n",
        "#   >    - (1,1): z = 1×1 + 1×1 + b = 2 + b\n",
        "#   > 3. You need: (0,0), (0,1), (1,0) → z < 0 and (1,1) → z > 0\n",
        "#   > 4. What value of b satisfies: b < 0, (1+b) < 0, but (2+b) > 0?\n",
        "and_weights =  # Add your values here\n",
        "and_bias =  # Add your value here\n",
        "\n",
        "and_perceptron.set_weights(and_weights, and_bias)\n",
        "\n",
        "# Test your solution\n",
        "predictions, z_values = and_perceptron.forward(X_and_tensor)\n",
        "accuracy = and_perceptron.accuracy(X_and_tensor, y_and_tensor)\n",
        "\n",
        "print(\"AND Gate Solution:\")\n",
        "print(f\"Weights: {and_weights}\")\n",
        "print(f\"Bias: {and_bias}\")\n",
        "print(f\"\\nTruth Table:\")\n",
        "print(\"Input   | Expected | Predicted | z value\")\n",
        "print(\"-\" * 45)\n",
        "for i in range(len(X)):\n",
        "    print(f\"{X[i]} | {y_and[i]:.0f}        | {predictions[i]:.0f}         | {z_values[i].item():.2f}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.1%}\")\n",
        "\n",
        "if accuracy == 1.0:\n",
        "    print(\"✓ Perfect! You've solved the AND gate - every point is on the correct side!\")\n",
        "else:\n",
        "    print(\"✗ Not quite. Try adjusting the weights and bias.\")\n",
        "    print(\"Hint: The decision boundary should separate (1,1) from all other points.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Interpreting your solution**: Now that you've found weights and bias that work, let's understand what they're doing:\n",
        "> \n",
        "> - **Look at your weights**: Are they equal or different? What does that tell you about how the neuron treats each input for AND logic?\n",
        "> - **Look at your bias**: Is it positive or negative? A negative bias creates a \"high threshold\"—the neuron only activates when inputs are strong enough to overcome this barrier\n",
        "> - **The AND logic**: Your solution ensures that a single active input isn't enough to activate the neuron—you need *both* inputs firing together to cross the decision boundary\n",
        "> \n",
        "> This is the essence of the perceptron: weights control *what* the neuron responds to, bias controls *how much* is needed to activate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the AND gate solution\n",
        "def plot_gate_solution(X, y, perceptron, title, filename):\n",
        "    \"\"\"\n",
        "    Plot the data points and decision boundary for a logic gate.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    \n",
        "    colors = ['#ff7f0e', '#1f77b4']\n",
        "    labels_text = ['Output = 0', 'Output = 1']\n",
        "    \n",
        "    # Plot data points\n",
        "    for i in range(2):\n",
        "        mask = y == i\n",
        "        plt.scatter(X[mask, 0], X[mask, 1],\n",
        "                   c=colors[i], s=400, label=labels_text[i],\n",
        "                   edgecolors='black', linewidth=3, zorder=3)\n",
        "    \n",
        "    # Add coordinate labels\n",
        "    for idx, (x1, x2) in enumerate(X):\n",
        "        plt.text(x1, x2-0.15, f\"({x1},{x2})\",\n",
        "                ha='center', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    w = perceptron.weights.numpy()\n",
        "    b = perceptron.bias.item()\n",
        "    \n",
        "    x1_range = np.array([-0.5, 1.5])\n",
        "    \n",
        "    # Decision boundary equation: w1*x1 + w2*x2 + b = 0\n",
        "    # Solving for x2: x2 = -(w1*x1 + b) / w2\n",
        "    if abs(w[1]) > 1e-6:\n",
        "        x2_boundary = -(w[0] * x1_range + b) / w[1]\n",
        "        plt.plot(x1_range, x2_boundary, 'r-', linewidth=3,\n",
        "                label='Decision Boundary', zorder=2)\n",
        "        \n",
        "        # Shade the regions\n",
        "        plt.fill_between(x1_range, -0.5, x2_boundary,\n",
        "                        alpha=0.1, color='orange', label='Predicted 0')\n",
        "        plt.fill_between(x1_range, x2_boundary, 1.5,\n",
        "                        alpha=0.1, color='blue', label='Predicted 1')\n",
        "    \n",
        "    plt.xlabel('Input 1', fontsize=13)\n",
        "    plt.ylabel('Input 2', fontsize=13)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlim(-0.3, 1.3)\n",
        "    plt.ylim(-0.3, 1.3)\n",
        "    plt.xticks([0, 1])\n",
        "    plt.yticks([0, 1])\n",
        "    plt.grid(True, alpha=0.3, zorder=1)\n",
        "    plt.legend(loc='upper left', fontsize=10)\n",
        "    \n",
        "    # Add weight information\n",
        "    info_text = f\"Weights: [{w[0]:.1f}, {w[1]:.1f}]\\nBias: {b:.1f}\"\n",
        "    plt.text(0.98, 0.02, info_text, transform=plt.gca().transAxes,\n",
        "            fontsize=11, verticalalignment='bottom', horizontalalignment='right',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize AND gate solution\n",
        "plot_gate_solution(X, y_and, and_perceptron,\n",
        "                  'AND Gate: Successfully Solved with a Single Neuron',\n",
        "                  'and_gate_solution.png')\n",
        "\n",
        "print(\"\\nNotice how a single straight line perfectly separates the classes!\")\n",
        "print(\"This is a linearly separable problem.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Part B - Attempt the XOR gate\n",
        "\n",
        "Now for the challenge: try to find weights that solve XOR. Spoiler: you won't succeed, but understanding why is crucial!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create tensors for XOR gate\n",
        "X_xor_tensor = torch.FloatTensor(X)\n",
        "y_xor_tensor = torch.FloatTensor(y_xor)\n",
        "\n",
        "# TODO: Try to find weights that solve XOR (at least 3 different weight combinations)\n",
        "# HINT: Don't try too hard to find the \"right\" answer - there isn't one! \n",
        "# **Strategic exploration**: Try to isolate each corner systematically:\n",
        "#   > 1. First attempt: Can you separate (0,0) and (1,1) from (0,1) and (1,0)?\n",
        "#   > 2. Second attempt: Can you separate (0,1) and (1,1) from the other two?\n",
        "#   > 3. Third attempt: Try a diagonal line - can it work?\n",
        "# The goal is to try several different lines and see that they all fail. \n",
        "\n",
        "xor_attempts = []  # Add your choices here with format {'weights': [], 'bias': , 'name': ''}\n",
        "\n",
        "print(\"Attempting to solve XOR gate...\\n\")\n",
        "best_accuracy = 0\n",
        "best_config = None\n",
        "\n",
        "for config in xor_attempts:\n",
        "    xor_perceptron = Perceptron(input_size=2)\n",
        "    xor_perceptron.set_weights(config['weights'], config['bias'])\n",
        "    \n",
        "    predictions, z_values = xor_perceptron.forward(X_xor_tensor)\n",
        "    accuracy = xor_perceptron.accuracy(X_xor_tensor, y_xor_tensor)\n",
        "    \n",
        "    print(f\"{config['name']}:\")\n",
        "    print(f\"Weights: {config['weights']}, Bias: {config['bias']}\")\n",
        "    print(f\"Accuracy: {accuracy:.1%}\")\n",
        "    print(f\"Predictions: {predictions.numpy()} (Expected: {y_xor})\")\n",
        "    print()\n",
        "    \n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_config = config\n",
        "\n",
        "print(f\"Best accuracy achieved: {best_accuracy:.1%}\")\n",
        "print(f\"\\nNOTICE: We can't get 100% accuracy!\")\n",
        "print(f\"The best we can do is 50% (random guessing) or 75% (getting 3 out of 4 correct).\")\n",
        "print(f\"This is because XOR is NOT linearly separable!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all XOR attempts to see why they all fail\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for idx, config in enumerate(xor_attempts):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    xor_perceptron = Perceptron(input_size=2)\n",
        "    xor_perceptron.set_weights(config['weights'], config['bias'])\n",
        "    \n",
        "    colors = ['#ff7f0e', '#1f77b4']\n",
        "    labels_text = ['Output = 0', 'Output = 1']\n",
        "    \n",
        "    # Plot data points\n",
        "    for i in range(2):\n",
        "        mask = y_xor == i\n",
        "        ax.scatter(X[mask, 0], X[mask, 1],\n",
        "                  c=colors[i], s=400, label=labels_text[i] if idx == 0 else '',\n",
        "                  edgecolors='black', linewidth=3, zorder=3)\n",
        "    \n",
        "    # Add labels\n",
        "    for i, (x1, x2) in enumerate(X):\n",
        "        ax.text(x1, x2-0.15, f\"({x1},{x2})\",\n",
        "               ha='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    w = np.array(config['weights'])\n",
        "    b = config['bias']\n",
        "    x1_range = np.array([-0.5, 1.5])\n",
        "    \n",
        "    if abs(w[1]) > 1e-6:\n",
        "        x2_boundary = -(w[0] * x1_range + b) / w[1]\n",
        "        ax.plot(x1_range, x2_boundary, 'r-', linewidth=3, zorder=2)\n",
        "    else:\n",
        "        if abs(w[0]) > 1e-6:\n",
        "            x1_boundary = -b / w[0]\n",
        "            ax.axvline(x=x1_boundary, color='r', linewidth=3, zorder=2)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = xor_perceptron.accuracy(X_xor_tensor, y_xor_tensor)\n",
        "    \n",
        "    ax.set_xlabel('Input 1', fontsize=12)\n",
        "    ax.set_ylabel('Input 2', fontsize=12)\n",
        "    ax.set_title(f\"{config['name']}\\nAccuracy: {accuracy:.0%}\",\n",
        "                fontsize=12, fontweight='bold')\n",
        "    ax.set_xlim(-0.3, 1.3)\n",
        "    ax.set_ylim(-0.3, 1.3)\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.grid(True, alpha=0.3, zorder=1)\n",
        "    \n",
        "    if idx == 0:\n",
        "        ax.legend(loc='upper left', fontsize=10)\n",
        "\n",
        "plt.suptitle('XOR Gate: Why Every Single Line Fails', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'xor_gate_attempts.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insight: The XOR problem requires separating OPPOSITE CORNERS.\")\n",
        "print(\"No single straight line can do this, we need a curved boundary!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **The XOR problem and AI history**: In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons\", mathematically proving that single-layer perceptrons cannot solve XOR. The breakthrough came decades later when researchers realized that **multiple layers** with **nonlinear decision rules** could solve XOR and much more complex problems. This is the foundation of modern deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Part C - Understanding why we need depth\n",
        "\n",
        "Let's explore what happens when we use multiple neurons. Can two neurons working together solve XOR?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create two perceptrons that detect different patterns\n",
        "# Neuron 1: Detects \"at least one input is 1\" (OR-like)\n",
        "neuron1 = Perceptron(input_size=2)\n",
        "neuron1.set_weights([1.0, 1.0], -0.5)\n",
        "\n",
        "# Neuron 2: Detects \"NOT both inputs are 1\" (NAND-like)\n",
        "neuron2 = Perceptron(input_size=2)\n",
        "neuron2.set_weights([-1.0, -1.0], 1.5)\n",
        "\n",
        "# Get outputs from both neurons\n",
        "output1, z1 = neuron1.forward(X_xor_tensor)\n",
        "output2, z2 = neuron2.forward(X_xor_tensor)\n",
        "\n",
        "print(\"Two-Neuron Layer Outputs:\")\n",
        "print(\"\\nInput  | Neuron 1 | Neuron 2 | Expected XOR\")\n",
        "print(\"-\" * 50)\n",
        "for i in range(len(X)):\n",
        "    print(f\"{X[i]} | {output1[i]:.0f}        | {output2[i]:.0f}        | {y_xor[i]:.0f}\")\n",
        "\n",
        "print(\"\\nObservation:\")\n",
        "print(\"We now have TWO outputs per input, not one!\")\n",
        "print(\"Each neuron detects a different pattern:\")\n",
        "print(\"  • Neuron 1 outputs 1 when 'at least one input is 1' (OR-like)\")\n",
        "print(\"  • Neuron 2 outputs 1 when 'NOT both inputs are 1' (NAND-like)\")\n",
        "print(\"\\nThe Question: How do we combine these two outputs into a single XOR decision?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **How to combine multiple neurons**: A single layer of neurons gives you multiple outputs in parallel, but you need **another layer** to combine them into a final decision. This is why neural networks have depth: each layer transforms the problem to make it easier for the next layer."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've completed a journey through the fundamentals and limitations of single-layer perceptrons. Here's what you've achieved:\n",
        "\n",
        "- [x] **Implemented a perceptron from scratch** using PyTorch\n",
        "- [x] **Solved the AND gate** by finding appropriate weights and bias\n",
        "- [x] **Discovered XOR's impossibility** for single neurons (linearly inseparable)\n",
        "- [x] **Understood why depth matters** by exploring multiple neurons\n",
        "\n",
        "**Critical insights:**\n",
        "- **Linear separability**: Single neurons can only draw straight lines\n",
        "- **The XOR problem**: Some patterns require curved boundaries\n",
        "- **The power of depth**: Multiple layers can combine simple boundaries into complex decision regions\n",
        "\n",
        "Every neuron in every modern neural network—from image classifiers to language models—performs this same simple computation you just implemented. Understanding this building block is your key to understanding how deep learning works!\n",
        "\n",
        "> **Next steps to explore**: Now that you understand why depth is necessary, you might want to learn about activation functions that enable non-linearity, and training algorithms that automatically learn optimal weights from data instead of manual trial-and-error."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## _(Optional) Bonus: Test on movie data_\n",
        "\n",
        "Want to see this work on actual movies? The perceptron class you built works on \n",
        "ANY binary classification data—but you'll need different weights!\n",
        "\n",
        "**Why?** The weights you found for AND gate (working on binary 0/1 inputs) won't \n",
        "transfer directly to normalized movie data (continuous 0-1 values). The geometry \n",
        "is different.\n",
        "\n",
        "**Try this**:\n",
        "```python\n",
        "# Load movie data from demo (X_tensor, y_tensor)\n",
        "movie_perceptron = Perceptron(input_size=2)\n",
        "\n",
        "# Start with the demo's balanced weights that were tuned for movie data\n",
        "movie_perceptron.set_weights([0.6, 0.8], -0.8)\n",
        "\n",
        "predictions, z = movie_perceptron.forward(X_tensor)\n",
        "accuracy = movie_perceptron.accuracy(X_tensor, y_tensor)\n",
        "print(f\"Movie classification accuracy: {accuracy:.1%}\")\n",
        "\n",
        "# Try experimenting with different weights!\n",
        "```\n",
        "\n",
        "**Key insight**: Your Perceptron CLASS is reusable (same code works on any data), but the WEIGHTS need to be tuned for each specific dataset. This is exactly why model training exists, to learn the weights for you!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
