{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 2: Compare Activation Functions for Non-Linear Decision Boundaries\n",
        "\n",
        "Activation functions add non-linearity to neural networks, but not all activation functions are created equal. Each one shapes the decision boundary differently, and understanding these differences is key to choosing the right activation for your problem.\n",
        "\n",
        "> **Overview**: Explore how sigmoid, tanh, and ReLU activation functions create different curved decision boundaries on the same network architecture. Then discover the fundamental limits of single-layer networks by attempting the XOR problem.\n",
        "> \n",
        "> **Scenario**: Your smart greenhouse successfully uses activation functions to identify the elliptical \"Goldilocks zone\" for one crop (i.e., the moderate range of temperature and moisture where plants thrive: not too hot, not too cold, not too wet, not too dry). Before deploying to more greenhouses with different crops, you need to understand which activation function is most reliable for this scenario. Then, you realize that some crop combinations create even more complex patterns (like thriving in opposite environmental corners) that may require a different approach entirely.\n",
        "> \n",
        "> **Goal**: Build intuition for how different activation functions shape decision boundaries and discover why some patterns require deeper networks.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, NumPy, Matplotlib\n",
        "> \n",
        "> **Estimated Time**: 15-20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's import our libraries and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'assets/exercise2'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and prepare data\n",
        "\n",
        "> Note: This step mirrors the exact same dataset and dataset processing as for [demo 2](cd1818-intro-to-deep-learning/2-activations/demo2-adding-nonlinearity-to-single-layer.ipynb).\n",
        "\n",
        "We'll use the [Praxash1/AIgrow](https://huggingface.co/datasets/Praxash1/AIgrow) dataset from Hugging Face, which contains real agricultural data, including temperature and soil moisture measurements.\n",
        "\n",
        "For preprocessing, we'll:\n",
        "1. Load the dataset\n",
        "2. Parse temperature and moisture readings from text entries\n",
        "3. Augment the small dataset for better visualization\n",
        "4. Normalize the features for our neural network\n",
        "5. Create the final dataset X _(in numpy array format)_\n",
        "6. Create two different health label patterns:\n",
        "   - **Elliptical pattern**: Plants thrive in moderate temperature AND moisture (Goldilocks zone)\n",
        "   - **XOR pattern**: Plants thrive in opposite corners (cold+wet OR hot+dry)\n",
        "\n",
        "**IMPORTANT: Feel free to skip this section to focus on the activation functions themselves**. Just know that we end up with 400 examples, each with temperature and moisture measurements. We create two different labeling schemes: one for the elliptical Goldilocks zone, and another for the XOR pattern with disconnected healthy regions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load the dataset (from local directory)\n",
        "dataset = load_dataset('Praxash1/AIgrow', split='train')\n",
        "print(f\"✓ Dataset loaded: {len(dataset)} entries found\\n\")\n",
        "\n",
        "# 2. Parse the dataset\n",
        "def parse_aigrow_entry(text):\n",
        "    \"\"\"Extract temperature and moisture from dataset text entries.\"\"\"\n",
        "    data = {}\n",
        "    \n",
        "    # Extract soil moisture\n",
        "    moisture_match = re.search(r'Soil Moisture:\\s*([\\d.]+)', text)\n",
        "    if moisture_match:\n",
        "        data['moisture'] = float(moisture_match.group(1))\n",
        "    \n",
        "    # Extract temperature\n",
        "    temp_match = re.search(r'Temperature:\\s*([\\d.]+)', text)\n",
        "    if temp_match:\n",
        "        data['temperature'] = float(temp_match.group(1))\n",
        "    \n",
        "    return data if len(data) == 2 else None\n",
        "\n",
        "data_list = []\n",
        "for item in dataset:\n",
        "    parsed = parse_aigrow_entry(item['text'])\n",
        "    if parsed:\n",
        "        data_list.append(parsed)\n",
        "\n",
        "print(f\"✓ Parsed {len(data_list)} valid temperature-moisture readings\")\n",
        "\n",
        "# 3. Augment the dataset\n",
        "# The original dataset is small (~16 samples), so we'll augment it by adding synthetic variations around the real measurements\n",
        "print(f\"✓ Augmenting dataset to create {400} samples for better visualization...\")\n",
        "\n",
        "original_data = pd.DataFrame(data_list)\n",
        "\n",
        "# Generate augmented data by adding noise to original samples\n",
        "augmented_data = []\n",
        "target_size = 400\n",
        "\n",
        "np.random.seed(42)\n",
        "for _ in range(target_size):\n",
        "    # Pick a random original sample\n",
        "    base_idx = np.random.randint(0, len(original_data))\n",
        "    base_temp = original_data.iloc[base_idx]['temperature']\n",
        "    base_moisture = original_data.iloc[base_idx]['moisture']\n",
        "    \n",
        "    # Add realistic variation\n",
        "    new_temp = base_temp + np.random.normal(0, 3)  # ±3°C variation\n",
        "    new_moisture = base_moisture + np.random.normal(0, 5)  # ±5% variation\n",
        "    \n",
        "    # Clip to realistic ranges\n",
        "    new_temp = np.clip(new_temp, 10, 40)\n",
        "    new_moisture = np.clip(new_moisture, 5, 85)\n",
        "    \n",
        "    augmented_data.append({\n",
        "        'temperature': new_temp,\n",
        "        'moisture': new_moisture\n",
        "    })\n",
        "\n",
        "data_list = augmented_data\n",
        "\n",
        "# 3. Convert to arrays\n",
        "df = pd.DataFrame(data_list)\n",
        "temperature_raw = df['temperature'].values\n",
        "moisture_raw = df['moisture'].values\n",
        "\n",
        "# 4. Normalize features to [0, 1] range for neural network\n",
        "temperature_norm = (temperature_raw - 10) / 30  # 10-40°C -> 0-1\n",
        "moisture_norm = moisture_raw / 90  # 0-90% -> 0-1\n",
        "\n",
        "# 5. Combine features into matrix\n",
        "X = np.column_stack([temperature_norm, moisture_norm])\n",
        "X_tensor = torch.FloatTensor(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Dataset preparation**: We're loading the AIgrow dataset, which originally contains only 16 agricultural measurements. To create smooth visualizations of decision boundaries, we augment it to 400 samples by interpolating between real data points. The features (temperature and moisture) are then normalized to a 0-1 range so the neural network can process them effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Create labels \n",
        "\n",
        "# Create ELLIPTICAL labels (Goldilocks zone)\n",
        "def create_elliptical_labels(temps, moistures, center=(0.5, 0.5), radii=(0.25, 0.20)):\n",
        "    \"\"\"\n",
        "    Create elliptical pattern: healthy when BOTH temp and moisture are moderate\n",
        "    \"\"\"\n",
        "    cx, cy = center\n",
        "    rx, ry = radii\n",
        "    distances = ((temps - cx) / rx) ** 2 + ((moistures - cy) / ry) ** 2\n",
        "    return (distances <= 1).astype(int)\n",
        "\n",
        "y_ellipse = create_elliptical_labels(temperature_norm, moisture_norm)\n",
        "y_ellipse_tensor = torch.FloatTensor(y_ellipse)\n",
        "\n",
        "# Create XOR labels (opposite corners pattern)\n",
        "def create_xor_labels(temps, moistures, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Create XOR pattern: healthy when (cold AND wet) OR (hot AND dry)\n",
        "    Unhealthy when (cold AND dry) OR (hot AND wet)\n",
        "    \"\"\"\n",
        "    cold = temps < threshold\n",
        "    wet = moistures < threshold\n",
        "    # XOR logic: (cold AND wet) OR (hot AND dry)\n",
        "    healthy = (cold & wet) | (~cold & ~wet)\n",
        "    return healthy.astype(int)\n",
        "\n",
        "y_xor = create_xor_labels(temperature_norm, moisture_norm)\n",
        "y_xor_tensor = torch.FloatTensor(y_xor)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Two labeling strategies**: We create two different ways to label the same data points as healthy or unhealthy. \n",
        "> - The elliptical pattern creates one connected region (moderate conditions are healthy)\n",
        "> - The XOR pattern creates two disconnected regions in opposite corners (cold+wet OR hot+dry are healthy). \n",
        "> \n",
        "> These different patterns will reveal the capabilities and limits of single-layer networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TO CONCLUDE: Get a high-level dataset summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"\\nFeature ranges (raw):\")\n",
        "print(f\"  Temperature: {temperature_raw.min():.1f}°C to {temperature_raw.max():.1f}°C\")\n",
        "print(f\"  Moisture: {moisture_raw.min():.1f}% to {moisture_raw.max():.1f}%\")\n",
        "print(f\"\\nFeature ranges (normalized):\")\n",
        "print(f\"  Temperature: {X[:, 0].min():.3f} to {X[:, 0].max():.3f}\")\n",
        "print(f\"  Moisture: {X[:, 1].min():.3f} to {X[:, 1].max():.3f}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(f\"✓ Ellipse pattern: {np.sum(y_ellipse == 1)} healthy, {np.sum(y_ellipse == 0)} unhealthy\")\n",
        "print(f\"✓ XOR pattern: {np.sum(y_xor == 1)} healthy, {np.sum(y_xor == 0)} unhealthy\\n\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Understanding the class distributions**: Notice how the ellipse pattern has fewer healthy samples (47) because only the center region qualifies, creating an imbalanced but realistic scenario. The XOR pattern is more balanced (158 healthy) since two full quadrants are labeled as healthy. \n",
        "> \n",
        "> This difference in class distribution is important when interpreting your model's accuracy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implement Activation Functions\n",
        "\n",
        "Before we can compare how different activation functions shape decision boundaries, you need to implement the three core activation functions. You'll implement these ONCE as helper functions, then they'll be used throughout the rest of the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement three activation functions\n",
        "# These helper functions will be used throughout the exercise\n",
        "\n",
        "def apply_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Apply sigmoid activation: outputs range from 0 to 1.\n",
        "    \n",
        "    Hint: PyTorch has a built-in function for this common activation.\n",
        "    Reference: https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "    \n",
        "    Args:\n",
        "        x: Input tensor of any shape\n",
        "    Returns:\n",
        "        Tensor of same shape with sigmoid applied element-wise\n",
        "    \"\"\"\n",
        "    return  # Add your code here\n",
        "\n",
        "def apply_tanh(x):\n",
        "    \"\"\"\n",
        "    Apply tanh activation: outputs range from -1 to 1 (zero-centered).\n",
        "    \n",
        "    Hint: Tanh is the hyperbolic tangent function. PyTorch provides this.\n",
        "    Reference: https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "    \n",
        "    Args:\n",
        "        x: Input tensor of any shape\n",
        "    Returns:\n",
        "        Tensor of same shape with tanh applied element-wise\n",
        "    \"\"\"\n",
        "    return  # Add your code here\n",
        "\n",
        "def apply_relu(x):\n",
        "    \"\"\"\n",
        "    Apply ReLU activation: outputs negative values as 0, keeps positive values.\n",
        "    \n",
        "    Hint: ReLU stands for Rectified Linear Unit, and is calculated as max(0, x). PyTorch provides this.\n",
        "    Reference: https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "    \n",
        "    Args:\n",
        "        x: Input tensor of any shape\n",
        "    Returns:\n",
        "        Tensor of same shape with ReLU applied element-wise\n",
        "    \"\"\"\n",
        "    return  # Add your code here\n",
        "\n",
        "print(\"✓ Activation functions defined!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why implement these as functions?** For the purpose of this exercise, implementing this logic separately makes the code cleaner and easier to debug. In practice, you'd typically just call Torch's activation function method directly in the network architecture method!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualize Three Activation Functions\n",
        "\n",
        "Before applying activations to our network, let's understand their mathematical shapes. You'll plot sigmoid, tanh, and ReLU side-by-side to see how each transforms inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a range of input values\n",
        "x = torch.linspace(-5, 5, 200)\n",
        "\n",
        "# TODO: Compute activation function outputs\n",
        "# HINT: Use the methods defined in the previous step, on the right output\n",
        "sigmoid_output =  # Add your code here\n",
        "tanh_output =  # Add your code here\n",
        "relu_output =  # Add your code here\n",
        "\n",
        "# Plot all three functions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Sigmoid\n",
        "axes[0].plot(x.numpy(), sigmoid_output.numpy(), 'b-', linewidth=2)\n",
        "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[0].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
        "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xlabel('Input', fontsize=11)\n",
        "axes[0].set_ylabel('Output', fontsize=11)\n",
        "axes[0].set_title('Sigmoid: σ(x) = 1/(1+e⁻ˣ)\\nOutput range: [0, 1]', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylim(-0.2, 1.2)\n",
        "\n",
        "# Tanh\n",
        "axes[1].plot(x.numpy(), tanh_output.numpy(), 'g-', linewidth=2)\n",
        "axes[1].axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlabel('Input', fontsize=11)\n",
        "axes[1].set_ylabel('Output', fontsize=11)\n",
        "axes[1].set_title('Tanh: tanh(x)\\nOutput range: [-1, 1]', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylim(-1.2, 1.2)\n",
        "\n",
        "# ReLU\n",
        "axes[2].plot(x.numpy(), relu_output.numpy(), 'r-', linewidth=2)\n",
        "axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_xlabel('Input', fontsize=11)\n",
        "axes[2].set_ylabel('Output', fontsize=11)\n",
        "axes[2].set_title('ReLU: max(0, x)\\nOutput range: [0, ∞)', fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylim(-0.5, 5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'activation_functions.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey observations:\")\n",
        "print(\"• Sigmoid: Smooth S-curve, outputs between 0 and 1\")\n",
        "print(\"• Tanh: Smooth S-curve, outputs between -1 and 1 (zero-centered)\")\n",
        "print(\"• ReLU: Sharp cutoff at 0, unbounded positive outputs\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Reading the curves**: Notice how sigmoid and tanh both flatten at the extremes (this is called \"saturation\"), while ReLU maintains a constant slope for positive values. Sigmoid's output range [0,1] makes it natural for probabilities, tanh's zero-centered range [-1,1] helps with data that has both positive and negative meaning, and ReLU's unbounded positive range [0,∞) creates sparsity (many neurons output zero). \n",
        "> \n",
        "> These shape differences aren't just mathematical curiosities; they directly affect how the network combines multiple neurons into decision boundaries."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build the Network\n",
        "\n",
        "Now we'll create the network that uses your activation functions. The network architecture uses multiple parallel neurons, and the `forward()` method will call your activation helper functions to create non-linear decision boundaries.\n",
        "\n",
        "Here's something important: **different activation functions need different weight configurations** to solve the same problem."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why different weights?** Each activation has a unique output range:\n",
        "> \n",
        "> - **Sigmoid**: outputs between 0 and 1\n",
        "> - **Tanh**: outputs between -1 and 1  \n",
        "> - **ReLU**: outputs between 0 and infinity\n",
        "> \n",
        "> These different ranges mean the same weights produce completely different effects. A weight of 20 creates a smooth curve with sigmoid, but extreme unbounded values with ReLU. Our network will automatically use the right weights for each activation function.\n",
        "> \n",
        "> **Key insight**: In real training, choosing the right activation also means choosing the right weight initialization strategy. This is why PyTorch has different initialization methods (Xavier, He, etc.) for different activations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleLayerNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-layer network that can create curved boundaries.\n",
        "    It uses multiple parallel neurons and one activation function to create non-linear boundaries.\n",
        "\n",
        "    Architecture: 2 inputs -> N neurons -> fixed aggregation -> 1 output\n",
        "\n",
        "    The output aggregation uses fixed coefficients (all neurons contribute equally, i.e., AND logic),\n",
        "    so only the first layer's weights need active configuration.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_neurons=4):\n",
        "        # Single learnable layer: 2 inputs -> n_neurons\n",
        "        self.n_neurons = n_neurons\n",
        "        \n",
        "        # Pre-configured weights for each activation function\n",
        "        # Each activation needs different weights due to different output ranges\n",
        "        self.weight_configs = {\n",
        "            'sigmoid': {\n",
        "                'W': [[20.0, -20.0, 0.0, 0.0],    # Input 1 (temp) weights for 4 neurons\n",
        "                      [0.0, 0.0, 20.0, -20.0]],   # Input 2 (moisture) weights for 4 neurons\n",
        "                'b': [-5.5, 15.0, -5.5, 15.0]     # Creates boundaries at 0.25 and 0.75\n",
        "            },\n",
        "            'tanh': {\n",
        "                'W': [[10.0, -10.0, 0.0, 0.0],\n",
        "                      [0.0, 0.0, 10.0, -10.0]],\n",
        "                'b': [-2.5, 7.5, -2.5, 7.5]\n",
        "            },\n",
        "            'relu': {\n",
        "                'W': [[8.0, -8.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 8.0, -8.0]],\n",
        "                'b': [-2.0, 6.0, -2.0, 6.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def forward(self, x, activation='sigmoid'):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            X: Input tensor of shape (N, 2)\n",
        "            activation: One of 'sigmoid', 'tanh', 'relu', or 'none'\n",
        "        \n",
        "        Returns:\n",
        "            predictions: Final predictions (0 or 1)\n",
        "            activations: Neuron activations (for visualization)\n",
        "        \"\"\"\n",
        "        # Get the right weights for this activation\n",
        "        config = self.weight_configs[activation]\n",
        "        W = torch.FloatTensor(config['W'])\n",
        "        b = torch.FloatTensor(config['b'])\n",
        "        \n",
        "        # Layer: Linear transformation\n",
        "        z = torch.matmul(x, W) + b\n",
        "        \n",
        "        # Apply the selected activation function\n",
        "        if activation == 'sigmoid':\n",
        "            h = apply_sigmoid(z)\n",
        "        elif activation == 'tanh':\n",
        "            h = apply_tanh(z)\n",
        "        elif activation == 'relu':\n",
        "            h = apply_relu(z)\n",
        "        else:  # 'none'\n",
        "            h = z\n",
        "        \n",
        "        # Fixed aggregation: sum all neurons and apply threshold\n",
        "        # This creates an \"AND\" logic: all neurons must agree for positive prediction\n",
        "        # IMPORTANT: Clip ReLU outputs to [0,1] since it's unbounded, unlike sigmoid/tanh\n",
        "        #            This can make the aggregation sum grow arbitrarily large, breaking the linear threshold logic\n",
        "        if activation == 'relu':\n",
        "            h = h.clip(0, 1)  # Clip ReLU's unbounded outputs for aggregation\n",
        "        aggregation = h.sum(dim=1, keepdim=True) - (self.n_neurons - 0.5)\n",
        "        \n",
        "        # Convert to binary predictions\n",
        "        # We compare the score against 0.0 because Sigmoid(0.0) = 0.5\n",
        "        predictions = (aggregation > 0.0).float()\n",
        "        \n",
        "        return predictions, h\n",
        "    \n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"Calculate accuracy on given data.\"\"\"\n",
        "        predictions, _ = self.forward(X)\n",
        "        return (predictions.squeeze() == y).float().mean().item()\n",
        "\n",
        "# Create network\n",
        "network = SingleLayerNetwork(n_neurons=4)\n",
        "\n",
        "print(\"✓ Network created with pre-calculated weights for elliptical boundary\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **How the pre-configured weights work**: Each activation function needs different weight scales due to their output ranges. The network stores three weight configurations:\n",
        "> \n",
        "> - **Sigmoid [0, 1]**: Uses weights of ±20 with biases creating boundaries at 0.25 and 0.75\n",
        "> - **Tanh [-1, 1]**: Uses weights of ±10 (half of sigmoid) since its range is centered at zero\n",
        "> - **ReLU [0, ∞)**: Uses weights of ±4 (much smaller) to prevent unbounded outputs from exploding\n",
        "> \n",
        "> When you call `network.forward(data, activation='sigmoid')`, the network automatically selects the appropriate weights for that activation. Each of the 4 neurons creates one boundary:\n",
        "> - Neuron 1: \"Temperature > 0.25\" (left edge)\n",
        "> - Neuron 2: \"Temperature < 0.75\" (right edge)\n",
        "> - Neuron 3: \"Moisture > 0.25\" (bottom edge)\n",
        "> - Neuron 4: \"Moisture < 0.75\" (top edge)\n",
        "> \n",
        "> Together, these four boundaries combine through AND logic (via aggregation) to create the enclosed region. The different weight scales ensure that regardless of which activation you use, each neuron outputs values in a reasonable range for the aggregation to work properly."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Compare Activations on the Elliptical Pattern\n",
        "\n",
        "Now let's see how your three activation functions create different decision boundaries when applied to the elliptical \"Goldilocks zone\" pattern. The network you built will use each activation to try to separate healthy from unhealthy plants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize decision boundaries for different activation functions\n",
        "\n",
        "def visualize_activation_comparison(network, X, y, activations=['sigmoid', 'tanh', 'relu']):\n",
        "    \"\"\"\n",
        "    Compare decision boundaries for different activation functions.\n",
        "    \"\"\"\n",
        "    # Create grid for decision boundary\n",
        "    h = 0.01\n",
        "    x_min, x_max = -0.1, 1.1\n",
        "    y_min, y_max = -0.1, 1.1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    grid_tensor = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    for idx, activation in enumerate(activations):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get predictions for the grid using this activation\n",
        "        # The forward method returns (predictions, activations)\n",
        "        with torch.no_grad():\n",
        "            preds_grid, _ = network.forward(grid_tensor, activation=activation)\n",
        "            Z = preds_grid.numpy().reshape(xx.shape)\n",
        "        \n",
        "        # Plot decision regions\n",
        "        ax.contourf(xx, yy, Z, levels=[0, 0.5, 1], \n",
        "                   colors=['#ffcccc', '#ccffcc'], alpha=0.4)\n",
        "        ax.contour(xx, yy, Z, levels=[0.5], \n",
        "                  colors='red', linewidths=2.5)\n",
        "        \n",
        "        # Plot data points\n",
        "        for i, label in enumerate(['Unhealthy', 'Healthy']):\n",
        "            mask = y == i\n",
        "            color = 'red' if i == 0 else 'green'\n",
        "            ax.scatter(X_tensor[mask, 0], X_tensor[mask, 1], c=color, label=label,\n",
        "                      edgecolor='k', s=30, alpha=0.6, linewidth=0.5)\n",
        "        \n",
        "        # Calculate accuracy for this activation\n",
        "        # Get predictions on the actual data points and compare with true labels\n",
        "        with torch.no_grad():\n",
        "            preds, _ = network.forward(X_tensor, activation=activation)\n",
        "            accuracy = (preds.squeeze() == y_ellipse_tensor).float().mean().item()\n",
        "        \n",
        "        ax.set_title(f\"{activation.upper()} Activation\\nAccuracy: {accuracy:.1%}\",\n",
        "                    fontsize=13, fontweight='bold')\n",
        "        ax.set_xlabel('Temperature', fontsize=11)\n",
        "        ax.set_ylabel('Moisture', fontsize=11)\n",
        "        ax.legend(loc='upper right', fontsize=9)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'activation_comparison_ellipse.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Run the visualization\n",
        "visualize_activation_comparison(network, X_tensor, y_ellipse_tensor)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: Analysis Question 1\n",
        "\n",
        "**Based on your visualizations above, answer the following:**\n",
        "\n",
        "Which activation function(s) best capture the elliptical Goldilocks zone? What differences do you notice in the decision boundaries they create? (2-3 sentences)\n",
        "\n",
        "_Write your answer here:_\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What to look for**: Pay attention to how smooth vs. sharp the boundaries are, whether the ellipse shape is preserved, and if any activation creates unexpected artifacts. \n",
        "> \n",
        "> The activation's mathematical properties (bounded vs. unbounded, smooth vs. piecewise) directly influence these visual characteristics."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 7: The XOR Challenge - Discovering the Need for Depth\n",
        "\n",
        "Now let's test our single-layer network on a fundamentally harder pattern: XOR. This pattern requires separating opposite corners, which creates disconnected decision regions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, visualize the XOR pattern\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
        "\n",
        "for i, label in enumerate(['Unhealthy', 'Healthy']):\n",
        "    mask = y_xor == i\n",
        "    color = 'red' if i == 0 else 'green'\n",
        "    ax.scatter(X[mask, 0], X[mask, 1], c=color, label=label,\n",
        "               edgecolor='k', s=40, alpha=0.7, linewidth=0.5)\n",
        "\n",
        "ax.set_xlabel('Temperature (normalized)', fontsize=12)\n",
        "ax.set_ylabel('Moisture (normalized)', fontsize=12)\n",
        "ax.set_title('XOR Pattern: Opposite Corners\\n(Cold+Wet OR Hot+Dry = Healthy)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Quadrant divider')\n",
        "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'xor_pattern.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nXOR Pattern Analysis:\")\n",
        "print(\"• Top-left quadrant (cold+wet): Healthy\")\n",
        "print(\"• Bottom-right quadrant (hot+dry): Healthy\")\n",
        "print(\"• Top-right quadrant (hot+wet): Unhealthy\")\n",
        "print(\"• Bottom-left quadrant (cold+dry): Unhealthy\")\n",
        "print(\"\\nThis creates TWO DISCONNECTED healthy regions!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **The XOR challenge**: Unlike the ellipse, which is one connected region, XOR requires separating two disconnected regions (opposite corners). This is a fundamentally harder problem that has important implications for network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test all three activation functions on the XOR pattern\n",
        "\n",
        "def test_xor_with_activations(network, X, y_xor, activations=['sigmoid', 'tanh', 'relu']):\n",
        "    \"\"\"\n",
        "    Test if single-layer network with different activations can solve XOR.\n",
        "    \"\"\"\n",
        "    # Create grid\n",
        "    h = 0.01\n",
        "    x_min, x_max = -0.1, 1.1\n",
        "    y_min, y_max = -0.1, 1.1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    grid_tensor = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for idx, activation in enumerate(activations):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get predictions with this activation\n",
        "        with torch.no_grad():\n",
        "            preds_grid, _ = network.forward(grid_tensor, activation=activation)\n",
        "            Z = preds_grid.numpy().reshape(xx.shape)\n",
        "        \n",
        "        # Plot decision regions\n",
        "        ax.contourf(xx, yy, Z, levels=[0, 0.5, 1],\n",
        "                   colors=['#ffcccc', '#ccffcc'], alpha=0.4)\n",
        "        ax.contour(xx, yy, Z, levels=[0.5],\n",
        "                  colors='blue', linewidths=2.5)\n",
        "        \n",
        "        # Plot XOR data\n",
        "        for i, label in enumerate(['Unhealthy', 'Healthy']):\n",
        "            mask = y_xor == i\n",
        "            color = 'red' if i == 0 else 'green'\n",
        "            ax.scatter(X[mask, 0], X[mask, 1], c=color, label=label,\n",
        "                      edgecolor='k', s=30, alpha=0.7, linewidth=0.5)\n",
        "        \n",
        "        # Calculate accuracy on XOR data\n",
        "        with torch.no_grad():\n",
        "            preds, _ = network.forward(X_tensor, activation=activation)\n",
        "            accuracy = (preds.squeeze() == y_xor_tensor).float().mean().item()\n",
        "        \n",
        "        results[activation] = accuracy\n",
        "        \n",
        "        ax.set_title(f\"{activation.upper()} on XOR\\nAccuracy: {accuracy:.1%}\",\n",
        "                    fontsize=13, fontweight='bold')\n",
        "        ax.set_xlabel('Temperature', fontsize=11)\n",
        "        ax.set_ylabel('Moisture', fontsize=11)\n",
        "        ax.legend(loc='upper right', fontsize=9)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
        "        ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'xor_attempts.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run XOR tests\n",
        "xor_results = test_xor_with_activations(network, X, y_xor)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"XOR Results Summary\")\n",
        "print(\"=\"*60)\n",
        "for activation, acc in xor_results.items():\n",
        "    print(f\"{activation.upper():8s}: {acc:>6.1%} accuracy\")\n",
        "print(\"\\nConclusion: All three activations fail on XOR!\")\n",
        "print(\"Even with non-linearity, single layers cannot separate\")\n",
        "print(\"multiple disconnected regions.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: Analysis Question 2\n",
        "\n",
        "**Based on your XOR experiments, answer the following:**\n",
        "\n",
        "Why can single-layer networks solve the ellipse pattern but not XOR, even with activation functions? What does this tell us about the fundamental limits of single layers? (2-3 sentences)\n",
        "\n",
        "_Write your answer here:_\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why better weights won't help XOR**: You might wonder, _\"What if we just fine-tuned the weights better for XOR?\"_ To see why that wouldn't help, follow this thought experiment: Look at the decision boundaries above. Imagine you could grab that boundary line and move it anywhere on the plot—slide it left, right, up, down, rotate it, or even curve it into any single continuous shape you want (circle, ellipse, wavy line, whatever). \n",
        "> \n",
        "> No matter how you adjust the box, that boundary can capture ONLY one of the two healthy regions (opposite corners) without including the unhealthy regions between them."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've completed a comprehensive exploration of activation functions and discovered their capabilities and limitations.\n",
        "\n",
        "**What you've accomplished:**\n",
        "\n",
        "- [x] **Visualized three major activation functions** - sigmoid, tanh, and ReLU\n",
        "- [x] **Compared their decision boundaries** on the elliptical pattern\n",
        "- [x] **Discovered activation-specific characteristics** - smooth vs. sharp, bounded vs. unbounded\n",
        "- [x] **Hit the XOR wall** - found the fundamental limit of single-layer networks\n",
        "- [x] **Understood why depth matters** - some patterns require multiple transformation steps\n",
        "\n",
        "**Critical insights:** \n",
        "\n",
        "- **Activation functions enable curved boundaries**: They transform linear combinations into non-linear patterns\n",
        "- **Different activations have different characteristics**: Sigmoid/tanh are smooth and bounded; ReLU is piecewise linear and unbounded\n",
        "- **Single layers have fundamental limits**: Even with activation functions, they can only create one connected decision region\n",
        "\n",
        "Activation functions unlock curved boundaries (solving the limitations of linear perceptrons), but even with activation, single layers hit a wall at disconnected regions (revealing the need for depth). Understanding these limits helps you design networks that are exactly as complex as needed: not too simple (can't solve the problem), not too complex (wastes computation and risks overfitting).\n",
        "\n",
        "> **Next steps to explore**:  Now that you understand the power and limits of single layers with activation functions, you're ready to explore multi-layer architectures that can solve problems like XOR by stacking multiple transformation steps."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
