{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [SOLUTION] Exercise 3: Experiment with MLP Architectures (Wider vs. Deeper)\n",
        "\n",
        "Every neural network architect faces a fundamental question: _Should I make my network wider (more neurons per layer) or deeper (more layers)?_ This isn't just an aesthetic choice: it determines what your model can learn, how efficiently it uses parameters, and whether it fits deployment constraints. \n",
        "\n",
        "- Depth enables hierarchical learning: each layer builds increasingly abstract representations from simpler ones. \n",
        "- Width enables parallel pattern detection: more neurons capture more diverse features simultaneously. \n",
        "\n",
        "Understanding this trade-off is essential for effective network design.\n",
        "\n",
        "> **Overview**: Build and compare multiple MLP architectures with different depth and width configurations. Analyze how architectural choices affect parameter count, model capacity, and design trade-offs.\n",
        "> \n",
        "> **Scenario**: Your food delivery platform needs a restaurant success predictor that runs on mobile devices. You're evaluating different network architectures to find the right balance between capability and efficiency. Should you build a wide network that captures many patterns at once? A deep network that learns hierarchical features? Or something in between?\n",
        "> \n",
        "> **Goal**: Develop practical intuition for architectural design decisions and understand how depth vs. width affects both capability and resource usage.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, NumPy, Pandas\n",
        "> \n",
        "> **Estimated Time**: 15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's import our libraries and prepare the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n",
            "PyTorch version: 2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and prepare data\n",
        "\n",
        "> Note: This step mirrors the exact same dataset and dataset processing as for [demo 3](/cd1818-intro-to-deep-learning/3-mlp/demo3-building-first-mlp-sequential.ipynb).\n",
        "\n",
        "We'll use the [MongoDB/whatscooking.restaurants](https://huggingface.co/datasets/MongoDB/whatscooking.restaurants) dataset from Hugging Face, which contains detailed information on restaurants across New York City.\n",
        "\n",
        "For preprocessing, we'll:\n",
        "\n",
        "1. Load the dataset from Hugging Face\n",
        "2. Remove rows with null values for key features\n",
        "3. Encode categorical variables (cuisine, borough) as numbers\n",
        "4. Extract key features: stars, review_count, PriceRange, cuisine, borough\n",
        "5. Normalize features for the neural network\n",
        "6. Create binary success labels: high rating (4+ stars) AND proven popularity (50+ reviews)\n",
        "7. Create the final feature and label datasets _(in tensor format)_\n",
        "\n",
        "**IMPORTANT: Feel free to skip this data preparation section to focus on the MLP architecture itself**. Just know that we end up with a few thousand restaurants, each with 5 features (stars, reviews, price, encoded cuisine, encoded borough) always populated, and a binary success label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading restaurant dataset...\n",
            "Total restaurants in dataset: 25,361\n",
            "\n",
            "Dataset columns: ['restaurant_id', 'location', '_id', 'review_count', 'DogsAllowed', 'embedding', 'PriceRange', 'menu', 'HappyHour', 'TakeOut']...\n",
            "\n",
            "First few entries:\n",
            "                     name   cuisine  stars  review_count        borough\n",
            "0      Baby Bo'S Burritos   Tex-Mex    2.5            10      Manhattan\n",
            "1      Buddy'S Wonder Bar  American    3.5            62  Staten Island\n",
            "2  Great Kills Yacht Club  American    4.0            72  Staten Island\n",
            "3        Keats Restaurant  American    4.0           149      Manhattan\n",
            "4                 Olive'S    Bakery    5.0             7      Manhattan\n"
          ]
        }
      ],
      "source": [
        "# 1. Load restaurant data from HuggingFace\n",
        "print(\"Loading restaurant dataset...\")\n",
        "dataset = load_dataset(\"MongoDB/whatscooking.restaurants\", split=\"train\")\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# # Sample 1000 restaurants\n",
        "# df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Total restaurants in dataset: {len(df):,}\")\n",
        "print(f\"\\nDataset columns: {df.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
        "print(f\"\\nFirst few entries:\")\n",
        "print(df[['name', 'cuisine', 'stars', 'review_count', 'borough']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 22947 restaurants with 5 features\n",
            "\n",
            "Feature names: ['stars', 'review_count', 'PriceRange', 'cuisine_encoded', 'borough_encoded']\n",
            "Target: success (0 or 1)\n",
            "Success rate: 23.3%\n"
          ]
        }
      ],
      "source": [
        "# 2. Remove entries with any null value\n",
        "df = df.dropna(subset=['stars', 'review_count', 'PriceRange', 'cuisine', 'borough'])\n",
        "\n",
        "# 3. Encode categorical features\n",
        "cuisine_encoder = LabelEncoder()\n",
        "borough_encoder = LabelEncoder()\n",
        "\n",
        "df['cuisine_encoded'] = cuisine_encoder.fit_transform(df['cuisine'])\n",
        "df['borough_encoded'] = borough_encoder.fit_transform(df['borough'])\n",
        "\n",
        "# 4. Select subset of features\n",
        "feature_columns = ['stars', 'review_count', 'PriceRange', 'cuisine_encoded', 'borough_encoded']\n",
        "df = df[feature_columns]\n",
        "\n",
        "# 5. Normalize features (important for neural networks!)\n",
        "scaler = StandardScaler()\n",
        "X_raw = df.to_numpy()\n",
        "X_scaled = scaler.fit_transform(X_raw)\n",
        "\n",
        "# 6. Create success label: high rating (4+ stars) AND proven popularity (50+ reviews)\n",
        "df['success'] = ((df['stars'] >= 4.0) & \n",
        "                        (df['review_count'] >= 50)).astype(int)\n",
        "y = df['success'].values\n",
        "\n",
        "# 7. Define final datasets as PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X_scaled)\n",
        "y_tensor = torch.FloatTensor(y).unsqueeze(1)\n",
        "\n",
        "print(f\"Prepared {X_tensor.shape[0]} restaurants with {X_tensor.shape[1]} features\")\n",
        "print(f\"\\nFeature names: {feature_columns}\")\n",
        "print(f\"Target: success (0 or 1)\")\n",
        "print(f\"Success rate: {df['success'].mean():.1%}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Understanding data preparation for neural networks:**\n",
        "> \n",
        "> - **Encoding categorical features**: Neural networks only understand numbers, not text. We converted cuisine types (e.g., \"Italian\", \"Chinese\") and boroughs (e.g., \"Manhattan\", \"Brooklyn\") into numerical codes (0, 1, 2, ...). This is called label encoding: each unique category gets a unique integer.\n",
        "> \n",
        "> - **Normalization**: We scaled all features to have similar ranges using standardization (mean=0, standard deviation=1). Why? Features like `review_count` (range: 5-9185) would dominate features like `stars` (range: 1-5) without normalization. Neural networks learn better when all inputs are on comparable scales.\n",
        "> \n",
        "> **The result**: Just like XOR, we now have numerical data that the network can process: features in columns, examples in rows, all as numbers. The difference? XOR had 2 simple binary features. Our restaurant data has 5 features with richer patterns: continuous values (stars, reviews), ordinal values (price), and encoded categories (cuisine, borough). Same data structure, but with more complexity."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "> **Why these features?** We selected 5 features to create enough input dimensions that architectural choices become meaningful. With too few features (like 2-3), there's not enough complexity for depth vs. width to matter. With too many (50+), the exercise becomes dominated by the input layer's size rather than the architectural decisions in hidden layers."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build and compare network architectures\n",
        "\n",
        "Now comes the core of the exercise: you'll build three different MLP architectures and systematically compare them. This hands-on experimentation will reveal how depth, width, and activation choices affect network design.\n",
        "\n",
        "**What you'll build:**\n",
        "- A **wide & shallow** network (few layers, many neurons)\n",
        "- A **deep & narrow** network (many layers, few neurons)\n",
        "- A **deep network with different activation** (exploring activation functions)\n",
        "\n",
        "**What you'll discover:**\n",
        "- How to construct different MLPs using `nn.Sequential`\n",
        "- How architectural choices affect parameter count\n",
        "- Where parameters are distributed across layers\n",
        "- Why depth is more parameter-efficient than width\n",
        "\n",
        "Let's start building!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part A: Build a \"Wide and Shallow\" network\n",
        "\n",
        "A wide network uses few layers but many neurons per layer. This architecture can learn many different patterns simultaneously in a single transformation step.\n",
        "\n",
        "**Your task**: Build a 2-layer MLP with one wide hidden layer.\n",
        "\n",
        "**Target architecture**: 5 inputs → 512 hidden neurons → 1 output\n",
        "\n",
        "**Design choices to make**:\n",
        "- What activation function should you use in the hidden layer?\n",
        "- What activation function should you use for the output?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wide & Shallow Network:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=5, out_features=512, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n",
            "\n",
            "Architecture: 5 → 512 → 1\n"
          ]
        }
      ],
      "source": [
        "# Define network dimensions\n",
        "input_size = X_tensor.shape[1]  # 5 features\n",
        "hidden_size_wide = 512          # Many neurons in one layer\n",
        "output_size = 1                 # Binary classification\n",
        "\n",
        "# TODO: Build the wide network using nn.Sequential\n",
        "# Hint: Your first Linear layer connects inputs to the hidden layer. What dimensions does it need?\n",
        "# Hint: Then, the second layer connects the hidden neurons (same size!) to the single output.\n",
        "# Hint: We want a probability output (0 to 1). Which activation function squashes numbers into that range?\n",
        "# Reference: https://docs.pytorch.org/docs/stable/nn.html\n",
        "\n",
        "# SOLUTION\n",
        "model_wide = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size_wide),  # 5 → 512\n",
        "    nn.ReLU(),                                # Activation for non-linearity\n",
        "    nn.Linear(hidden_size_wide, output_size), # 512 → 1\n",
        "    nn.Sigmoid()                              # Output probability [0,1]\n",
        ")\n",
        "\n",
        "print(\"Wide & Shallow Network:\")\n",
        "print(model_wide)\n",
        "print(f\"\\nArchitecture: {input_size} → {hidden_size_wide} → {output_size}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Wide network as a parallel processor.** Our \"Wide and Shallow\" network tries to detect 512 different patterns simultaneously based on the 5 input features. While this network has immense capacity to learn many simple, distinct rules right away, what might be the cost associated with having every one of those 512 neurons connected to every single input feature? \n",
        "> <br>_(Hint: The cost is directly related to the number of weights you need to store and train.)_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: Build the \"Deep and Narrow\" network\n",
        "\n",
        "A deep network uses many layers but fewer neurons per layer. This architecture can learn hierarchical representations: each layer builds on the previous layer's features to create increasingly abstract patterns.\n",
        "\n",
        "**Your task**: Build a 4-layer MLP with three narrow hidden layers.\n",
        "\n",
        "**Target architecture**: 5 inputs → 32 neurons → 32 neurons → 32 neurons → 1 output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep & Narrow Network:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=5, out_features=32, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n",
            "\n",
            "Architecture: 5 → 32 → 32 → 32 → 1\n"
          ]
        }
      ],
      "source": [
        "# Define network dimensions\n",
        "hidden_size_deep = 32  # Fewer neurons per layer\n",
        "\n",
        "# TODO: Build the deep network using nn.Sequential\n",
        "# Hint: Think about the data flow: Input → Hidden1 → Hidden2 → Hidden3 → Output.\n",
        "# Hint: Ensure the 'out_features' of one layer matches the 'in_features' of the next.\n",
        "# Hint: And, remember: every hidden layer needs an activation function, or else they collapse into a single linear layer!\n",
        "# Reference: https://docs.pytorch.org/docs/stable/nn.html\n",
        "\n",
        "# SOLUTION\n",
        "model_deep = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size_deep),  # 5 → 32\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size_deep, hidden_size_deep),  # 32 → 32\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size_deep, hidden_size_deep),  # 32 → 32\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size_deep, output_size),  # 32 → 1\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "print(\"Deep & Narrow Network:\")\n",
        "print(model_deep)\n",
        "print(f\"\\nArchitecture: {input_size} → {hidden_size_deep} → {hidden_size_deep} → {hidden_size_deep} → {output_size}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Deep network as hierarchical learner**: Our \"Deep and Narrow\" Network achieves its power through composition (stacking functions). Each of the three hidden layers (32 → 32 → 32) only learns 32 patterns, but the second layer learns patterns from the output of the first layer, and the third layer learns patterns from the second layer. Why might learning a \"complex pattern\" via three smaller, stacked steps be more efficient than trying to learn that same complex pattern in a single, massive step? \n",
        "> <br>_(Hint: Relate this to how you would solve a long math equation versus a complex logic puzzle.)_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Part B.1: Experiment with activation functions - Do they add parameters?\n",
        "\n",
        "You chose one of many possible activations for your hidden layers. But what would happen with different activations? Let's explore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deep Network with Tanh Activation:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=5, out_features=32, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (5): Tanh()\n",
            "  (6): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# TODO: Build a deep network with a different activation \n",
        "# Hint: Simply replace your current activation with a different one\n",
        "# Hint: Keep everything else the same (same architecture, same output activation)\n",
        "# Reference: https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "\n",
        "# SOLUTION\n",
        "model_deep_alternative = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size_deep),  # 5 → 32\n",
        "    nn.Tanh(),  # Changed from ReLU to Tanh\n",
        "    nn.Linear(hidden_size_deep, hidden_size_deep),  # 32 → 32\n",
        "    nn.Tanh(),  # Changed from ReLU to Tanh\n",
        "    nn.Linear(hidden_size_deep, hidden_size_deep),  # 32 → 32\n",
        "    nn.Tanh(),  # Changed from ReLU to Tanh\n",
        "    nn.Linear(hidden_size_deep, output_size),  # 32 → 1\n",
        "    nn.Sigmoid()  # Output activation stays the same\n",
        ")\n",
        "\n",
        "print(\"\\nDeep Network with Alternative Activation:\")\n",
        "print(model_deep_alternative)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **How do activation functions contribute to network complexity?**: Do you expect the two versions with different activation functions to have the same number of parameters, or different? Why? \n",
        "> <br>_(Think about whether the activation function itself has any trainable weights.)_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part C: Analyze parameter count\n",
        "\n",
        "Now that you've built three different architectures, let's measure one concrete aspect: **how many parameters does each have?**\n",
        "\n",
        "Parameter count matters for two reasons:\n",
        "1. **Resource constraints**: More parameters = more memory and computation\n",
        "2. **Architectural efficiency**: Reveals how different designs use their parameter budget\n",
        "\n",
        "But remember: Parameters are just ONE way to measure architecture. The real power comes from what patterns the network can learn through its depth and width (something you'll explore when you learn about model training)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter Counts:\n",
            "============================================================\n",
            "Wide & Shallow (ReLU):    3,585 parameters\n",
            "Deep & Narrow (ReLU):     2,337 parameters\n",
            "Deep & Narrow (Tanh):     2,337 parameters\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# TODO: Count total parameters in the wide network\n",
        "# Hint: The model object has a generator for parameters. Can you loop through it?\n",
        "# Hint: Each item in that loop is a tensor. And, you need to sum the number of elements of every tensor in the model.\n",
        "# Reference: \n",
        "# 1. https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters\n",
        "# 2. https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count total trainable parameters in a model.\"\"\"\n",
        "    # SOLUTION\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Count parameters for each model\n",
        "wide_params = count_parameters(model_wide)\n",
        "deep_params = count_parameters(model_deep)\n",
        "deep_alternative_params = count_parameters(model_deep_alternative)\n",
        "\n",
        "print(\"Parameter Counts:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Wide & Shallow:    {wide_params:,} parameters\")\n",
        "print(f\"Deep & Narrow :     {deep_params:,} parameters\")\n",
        "print(f\"Deep & Narrow (Alternative Activation):     {deep_alternative_params:,} parameters\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What to notice**: Compare the wide vs. deep networks. Which has more parameters? The difference reveals how width and depth scale differently: \n",
        "> \n",
        "> - Width increases parameters dramatically because every neuron connects to every input AND every output. \n",
        "> - Depth adds parameters more gradually because each new layer only connects to the previous layer's size.\n",
        "> \n",
        "> Even though the deep network has more steps of abstraction (more layers), hierarchical learning makes learning patterns sequentially more efficient!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part D: Understand parameter distribution\n",
        "\n",
        "Total parameter count tells one story, but WHERE those parameters live tells another. Let's break down each layer to see how parameters are distributed across the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Wide & Shallow - Layer-by-Layer Breakdown:\n",
            "======================================================================\n",
            "\n",
            "Layer 0: Linear(in_features=5, out_features=512, bias=True)\n",
            "  → Weight shape: torch.Size([512, 5]) = 2,560 parameters\n",
            "  → Bias shape: torch.Size([512]) = 512 parameters\n",
            "  → Layer total: 3,072 parameters\n",
            "\n",
            "Layer 1: ReLU()\n",
            "  → No trainable parameters (activation function)\n",
            "\n",
            "Layer 2: Linear(in_features=512, out_features=1, bias=True)\n",
            "  → Weight shape: torch.Size([1, 512]) = 512 parameters\n",
            "  → Bias shape: torch.Size([1]) = 1 parameters\n",
            "  → Layer total: 513 parameters\n",
            "\n",
            "Layer 3: Sigmoid()\n",
            "  → No trainable parameters (activation function)\n",
            "======================================================================\n",
            "Total: 3,585 parameters\n",
            "\n",
            "\n",
            "Deep & Narrow - Layer-by-Layer Breakdown:\n",
            "======================================================================\n",
            "\n",
            "Layer 0: Linear(in_features=5, out_features=32, bias=True)\n",
            "  → Weight shape: torch.Size([32, 5]) = 160 parameters\n",
            "  → Bias shape: torch.Size([32]) = 32 parameters\n",
            "  → Layer total: 192 parameters\n",
            "\n",
            "Layer 1: ReLU()\n",
            "  → No trainable parameters (activation function)\n",
            "\n",
            "Layer 2: Linear(in_features=32, out_features=32, bias=True)\n",
            "  → Weight shape: torch.Size([32, 32]) = 1,024 parameters\n",
            "  → Bias shape: torch.Size([32]) = 32 parameters\n",
            "  → Layer total: 1,056 parameters\n",
            "\n",
            "Layer 3: ReLU()\n",
            "  → No trainable parameters (activation function)\n",
            "\n",
            "Layer 4: Linear(in_features=32, out_features=32, bias=True)\n",
            "  → Weight shape: torch.Size([32, 32]) = 1,024 parameters\n",
            "  → Bias shape: torch.Size([32]) = 32 parameters\n",
            "  → Layer total: 1,056 parameters\n",
            "\n",
            "Layer 5: ReLU()\n",
            "  → No trainable parameters (activation function)\n",
            "\n",
            "Layer 6: Linear(in_features=32, out_features=1, bias=True)\n",
            "  → Weight shape: torch.Size([1, 32]) = 32 parameters\n",
            "  → Bias shape: torch.Size([1]) = 1 parameters\n",
            "  → Layer total: 33 parameters\n",
            "\n",
            "Layer 7: Sigmoid()\n",
            "  → No trainable parameters (activation function)\n",
            "======================================================================\n",
            "Total: 2,337 parameters\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print a detailed breakdown showing parameters in each layer\n",
        "# Hint: Loop through enumerate(model) to get both index and layer\n",
        "# Hint: Extract both .weight and .bias, but ONLY for layers with learnable parameters\n",
        "# Hint: Print the shapes and parameter counts for each with a final total\n",
        "# Reference: https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\n",
        "def print_parameter_breakdown(model, model_name):\n",
        "    \"\"\"Print detailed parameter breakdown for each layer.\"\"\"\n",
        "    print(f\"\\n{model_name} - Layer-by-Layer Breakdown:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # SOLUTION\n",
        "    total = 0\n",
        "    for idx, layer in enumerate(model):\n",
        "        print(f\"\\nLayer {idx}: {layer}\")\n",
        "        \n",
        "        if isinstance(layer, nn.Linear):\n",
        "            weight_params = layer.weight.numel()\n",
        "            bias_params = layer.bias.numel()\n",
        "            layer_total = weight_params + bias_params\n",
        "            \n",
        "            print(f\"  → Weight shape: {layer.weight.shape} = {weight_params:,} parameters\")\n",
        "            print(f\"  → Bias shape: {layer.bias.shape} = {bias_params:,} parameters\")\n",
        "            print(f\"  → Layer total: {layer_total:,} parameters\")\n",
        "            \n",
        "            total += layer_total\n",
        "        else:\n",
        "            print(f\"  → No trainable parameters (activation function)\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Total: {count_parameters(model):,} parameters\\n\")\n",
        "\n",
        "# Print breakdowns\n",
        "print_parameter_breakdown(model_wide, \"Wide & Shallow\")\n",
        "print_parameter_breakdown(model_deep, \"Deep & Narrow\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Parameter math for Linear layers**: For any `nn.Linear(in_features, out_features)`, the parameter count is:\n",
        "> - **Weights**: `in_features × out_features` (one weight per connection)\n",
        "> - **Biases**: `out_features` (one bias per output neuron)  \n",
        "> - **Total**: `(in_features × out_features) + out_features`\n",
        "> \n",
        "> For example, `nn.Linear(5, 512)` has (5 × 512) + 512 = 3072 parameters.\n",
        ">\n",
        "> **Key observation**: The first layer in the wide network (5→512) dominates the parameter budget because it connects many inputs to many outputs. In the deep network, parameters are distributed more evenly: each 32→32 layer has (32 × 32) + 32 = 1,056 parameters."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Gather insights from architectural experiments\n",
        "\n",
        "Now that you've built multiple architectures and analyzed their parameter counts, let's synthesize what you've discovered about the **Depth vs. Width** trade-off.\n",
        "\n",
        "**TODO**: Provide an answer in markdown for each of the following questions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Parameter efficiency\n",
        "\n",
        "Look at your parameter counts for the wide vs. deep networks.\n",
        "\n",
        "**Answer the following** (write your responses in this markdown cell):\n",
        "\n",
        "1. Which network has more parameters? By approximately what factor (1.5x, 2x, 5x, 10x)?\n",
        "2. If you had a strict memory constraint (say, 5,000 parameters maximum), which architecture style would fit better?\n",
        "\n",
        "_Write your answer here:_\n",
        "1. The Wide network has more parameters (~3,585) compared to the Deep network (~2,337). The difference is approximately 1.5x – the wide network has about 7 times as many parameters\n",
        "2. The Deep architecture is more memory efficient. We added 3 layers of depth and kept the model size small. Expanding the Wide network would cause the parameter count to jump up much faster."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Think about**: The wide network's first layer (5→512) creates a massive weight matrix of 2560 connections plus 512 biases. Meanwhile, the deep network's largest layer (32→32) only has 1,024 connections. This reveals why width scales parameters much faster than depth!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Parameter distribution\n",
        "\n",
        "Look at your layer-by-layer breakdowns.\n",
        "\n",
        "**Answer the following**:\n",
        "\n",
        "1. In the wide network, which layer dominates the parameter budget?\n",
        "2. In the deep network, how are parameters distributed—mostly in one layer or spread evenly?\n",
        "3. Why does the first layer in ANY network tend to have many parameters when connecting to a large hidden layer?\n",
        "\n",
        "_Write your answer here:_\n",
        "1. In the wide network, the first layer (5→512) dominates with 3072 parameters, representing over half of the total parameter budget\n",
        "2. In the deep network, parameters are distributed fairly evenly across the three 32→32 hidden layers (each has ~1,056 parameters)\n",
        "3. The first layer has many parameters when connecting to a large hidden layer because parameters = (inputs × neurons) + neurons. Even with just 5 inputs, connecting to 512 neurons creates (5 × 512) + 512 = 3072 parameters."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Consider the math**: In a Linear layer, `parameters = (inputs × outputs) + outputs`. So when you connect 5 inputs to 512 outputs, you get `(5 × 512) + 512 = 3072` parameters. But when you connect 32 to 32, you get `(32 × 32) + 32 = 1,056`. The quadratic growth of width (neurons²) vs. linear growth of depth (neurons × layers) explains the efficiency difference!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Architectural trade-offs\n",
        "\n",
        "Based on everything you've observed, think about the broader implications.\n",
        "\n",
        "**Reflect on the following**:\n",
        "\n",
        "1. **Width vs. Depth for parameters**: If you wanted to add more model capacity but had limited memory, would you add width or depth? Why?\n",
        "\n",
        "2. **Width vs. Depth for learning**: From the conceptual videos, you learned that depth enables hierarchical feature learning (simple → complex transformations), while width enables learning many parallel patterns. Which seems more powerful for complex problems?\n",
        "\n",
        "3. **Real-world constraints**: Imagine deploying to a mobile app with a 10MB size limit. How would this constraint influence your architectural decisions?\n",
        "\n",
        "_Write your answer here:_\n",
        "1. I would add depth rather than width if memory is limited. Adding a 32-neuron layer adds ~1,056 parameters, while doubling width from 64 to 128 neurons adds much more (roughly quadruples parameters in that layer).\n",
        "2. Depth seems more powerful for complex problems. The conceptual videos explained that depth enables hierarchical transformations—each layer builds increasingly abstract features. Width captures many parallel patterns but at one level of abstraction. For complex real-world problems that have hierarchical structure (like images: edges → shapes → objects), depth provides a more natural fit.\n",
        "3. A 10MB limit would severely constrain architecture choices. I would:\n",
        "    -   Favor deeper, narrower networks for parameter efficiency\n",
        "    -   Carefully measure parameter counts for each design\n",
        "    -   Consider the model's parameter count vs. the file size (PyTorch models include architecture + parameters, so roughly 4 bytes per parameter means 2.5 million parameters ≈ 10MB)\n",
        "Test whether the model can achieve acceptable performance within the constraint, or whether the problem requires a different approach (like model compression technique"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What depth and width should you start with?** The best starting point is a moderately Deep and moderately Wide network (e.g., 2-3 hidden layers, each with 50-100 neurons). This is the balanced approach that solves most common structured data problems, providing both hierarchical learning and a rich capacity for parallel pattern detection.\n",
        "> \n",
        "> <details> <summary><i>Curious to see how experts combine the two?</i></summary>\n",
        "> In production systems, the Wide & Deep architecture jointly trains both styles: the Wide component handles fast memorization, while the Deep component ensures superior generalization. Look at the original implementation used by Google for app recommendations: <a href=\"https://research.google/blog/wide-amp-deep-learning-better-together-with-tensorflow/\">Wide & Deep Learning: Better Together with TensorFlow</a>.\n",
        "> </details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've explored the fundamental architectural trade-off in neural network design: depth vs. width.\n",
        "\n",
        "**What you've accomplished:**\n",
        "\n",
        "- [x] Built a wide & shallow network (few layers, many neurons per layer)\n",
        "- [x] Built a deep & narrow network (many layers, few neurons per layer)  \n",
        "- [x] Experimented with different activation functions\n",
        "- [x] Measured parameter counts to understand resource requirements\n",
        "- [x] Analyzed parameter distribution across layers\n",
        "- [x] Discovered that depth is more parameter-efficient than width\n",
        "\n",
        "**Critical insights:**\n",
        "\n",
        "1. **Depth enables hierarchical learning**: Multiple layers transform problems through increasing abstraction  \n",
        "2. **Width enables parallel patterns**: More neurons capture more diverse features simultaneously\n",
        "3. **Depth is parameter-efficient**: Adding layers scales parameters linearly; adding width scales quadratically\n",
        "4. **Activations add zero parameters**: You can experiment with different activations without changing model size\n",
        "5. **First layers often dominate**: Connecting inputs to wide hidden layers creates parameter bottlenecks\n",
        "6. **Research favors depth**: Studies show deeper networks often outperform wider ones, even with fewer parameters\n",
        "\n",
        "The real power of depth comes from what it enables the network to learn: hierarchical representations where each layer builds increasingly abstract features. For now, you've gained the crucial insight that **architectural choices have concrete consequences**: both for what patterns a network *can* learn, and for whether it *fits* your deployment constraints.\n",
        "\n",
        "> **Next steps to explore**: Explore balanced architectures, vary hidden-layer widths, and mix activation functions while comparing parameter counts to build intuition about how network design choices affect model complexity."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
