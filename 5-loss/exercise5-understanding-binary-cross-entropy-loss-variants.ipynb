{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 5: Understanding Binary Cross-Entropy Loss Variants\n",
        "\n",
        "Binary classification seems simple: just predict class 1 or class 0. But choosing the right loss function variant matters more than you might think. Different variants expect different input formats (raw logits vs probabilities), and using the wrong one can cause errors or silent numerical instability. That's why PyTorch provides many different specialized loss variants.\n",
        "\n",
        "> **Overview**: Explore how different Binary Cross-Entropy loss variants behave when your model outputs raw logits versus probabilities. Discover why numerical stability matters and the importance of choosing the right PyTorch loss functions.\n",
        "> \n",
        "> **Scenario**: Your agricultural weather system predicts whether it will rain tomorrow (precipitation > 0) so farmers can schedule outdoor work and protect equipment. You've built a model that outputs a single raw value (logit) representing rain likelihood. Now you need to measure how well it's performing, but which loss function should you use?\n",
        "> \n",
        "> **Goal**: Build intuition for matching loss functions to model output formats and understand why numerical stability matters in deep learning.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, Pandas, Matplotlib\n",
        "> \n",
        "> **Estimated Time**: 20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's import our libraries and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and prepare the data\n",
        "\n",
        "We'll use the [VerisimilitudeX/Weather](https://huggingface.co/datasets/VerisimilitudeX/Weather) dataset, which contains daily weather observations from 300+ US weather stations.\n",
        "\n",
        "For this exercise, we'll:\n",
        "1. Load the dataset\n",
        "2. Create binary rain labels (1 = rain, 0 = no rain) based on precipitation measurements\n",
        "3. Extract weather features for prediction\n",
        "4. Sample a subset of 100 examples\n",
        "5. Convert features and labels to tensors\n",
        "6. Normalize the features for stability\n",
        "\n",
        "**IMPORTANT: Feel free to skip to Step 3 if you want to focus on loss functions**. Just know you end up with two normalized weather features (temperature, wind speed) and binary rain labels for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load the dataset\n",
        "dataset = load_dataset('VerisimilitudeX/Weather', split='train')\n",
        "print(f\"✓ Dataset loaded: {len(dataset)} weather observations\\n\")\n",
        "\n",
        "# Convert to pandas\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# 2. Create binary rain labels (1 if precipitation > 0, else 0)\n",
        "df['rain'] = (df['Data.Precipitation'] > 0).astype(int)\n",
        "\n",
        "# 3. Extract features: temperature, wind speed\n",
        "df_clean = df[['Data.Temperature.Avg Temp', 'Data.Wind.Speed', 'rain']].dropna()\n",
        "\n",
        "print(f\"✓ Created {len(df_clean)} samples with rain labels\")\n",
        "print(f\"  Rain days: {df_clean['rain'].sum()} ({df_clean['rain'].mean()*100:.1f}%)\")\n",
        "print(f\"  No-rain days: {(1-df_clean['rain']).sum()} ({(1-df_clean['rain'].mean())*100:.1f}%)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Binary classification setup**: We've converted a continuous precipitation measurement into a binary decision (rain/no-rain). This is a classic classification problem now : given temperature and wind speed, predict whether it will rain. \n",
        "> \n",
        "> The model will output a single number representing rain likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Sample a batch of data for our experiments\n",
        "sample_size = 100\n",
        "sample_df = df_clean.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# 5. Convert features and labels to tensors\n",
        "X = sample_df[['Data.Temperature.Avg Temp', 'Data.Wind.Speed']].values\n",
        "y = sample_df['rain'].values\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# 6. Normalize features (important for neural networks)\n",
        "X_tensor = (X_tensor - X_tensor.mean(dim=0)) / X_tensor.std(dim=0)\n",
        "\n",
        "print(f\"✓ Prepared batch of {sample_size} samples\")\n",
        "print(f\"  Features shape: {X_tensor.shape}\")\n",
        "print(f\"  Labels shape: {y_tensor.shape}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why normalize for loss computation?** Loss functions compare predictions to targets numerically. If features have very different scales (e.g., temperature in degrees vs wind speed in mph), the model's raw outputs can vary wildly in magnitude, leading to unstable loss values during training. \n",
        "> \n",
        "> Normalization ensures all features contribute equally, keeping loss values in a reasonable range and making model training smoother."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a simple model\n",
        "\n",
        "You'll use a simple neural network that takes weather features as input and outputs a single raw value (logit) representing rain likelihood. This model hasn't been trained yet, but it still produces outputs you can understand (and improve on) with loss functions.\n",
        "\n",
        "> **Understanding the model architecture**:  You're given a simple 2-layer network with a critical detail: the final layer has **no activation function**. This means it outputs raw logits (unbounded real numbers like -2.5 or 3.8), not probabilities. When you call the model, it returns shape [100, 1], but [`squeeze()`](https://docs.pytorch.org/docs/master/generated/torch.squeeze.html) removes that extra dimension to get [100], matching your labels' shape. This squeeze step is a common PyTorch pattern for when a layer or operation returns an unnecessary singleton dimension that doesn’t match the shape you need.\n",
        "\n",
        "**Key point**: The model outputs raw logits, which is the standard PyTorch pattern for binary classification. Why combine activation and loss computation into one step? You're about to find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple model that outputs raw logits (no sigmoid activation)\n",
        "class RainPredictor(nn.Module):\n",
        "    def __init__(self, input_size=2):\n",
        "        super(RainPredictor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1)  # Output layer: NO activation = raw logits\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze()  # Remove extra dimension\n",
        "\n",
        "# Create model and get predictions\n",
        "model = RainPredictor()\n",
        "\n",
        "# Get predictions (logits) from the untrained model\n",
        "with torch.no_grad():\n",
        "    logits = model(X_tensor)\n",
        "\n",
        "print(\"✓ Model created and predictions generated\\n\")\n",
        "print(f\"Sample logits (first 10):\")\n",
        "print(logits[:10].numpy())\n",
        "print(f\"\\nLogit range: [{logits.min():.2f}, {logits.max():.2f}]\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Understanding logits**: The model outputs raw, unbounded numbers called \"logits\". These are NOT probabilities:\n",
        "> - Logits can be any real number: negative, zero, or positive\n",
        "> - Positive logits suggest \"rain,\" negative logits suggest \"no rain\"\n",
        "> - The magnitude indicates confidence\n",
        "> \n",
        "> To convert logits to probabilities, you need to apply an activation function like the sigmoid. But should you do this manually before computing loss, or let the loss function handle it?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Experiment with binary cross-entropy loss variants\n",
        "\n",
        "Now comes the core investigation: _Which loss function should you use for a model that outputs raw logits?_ You'll test different approaches and discover why PyTorch provides specialized loss variants.\n",
        "\n",
        "You'll explore:\n",
        "- **Step 4.1**: Choose and test a loss function for binary classification\n",
        "- **Step 4.2**: Troubleshooting different outcomes\n",
        "- **Step 4.3**: Compare loss approaches across batches\n",
        "- **Step 4.4**: Test numerical stability with extreme values\n",
        "- **Step 4.5**: Reflect on what you learned\n",
        "\n",
        "Let's start experimenting!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.1: Compute a binary classification loss\n",
        "\n",
        "You have raw logits from your model and binary rain labels. Your first task is to compute a loss that measures how well the predictions match the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a loss function for binary classification and compute the loss\n",
        "# HINT: PyTorch has several loss functions for classification in the nn module.\n",
        "# What seems appropriate for binary classification (two classes: rain vs no-rain)?\n",
        "# Reference: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "# Create your loss function by calling the right pytorch class\n",
        "loss_fn =  # Add your code here\n",
        "\n",
        "# Compute the loss with predicted logits and true y tensor labels\n",
        "try:\n",
        "    loss_value =  # Add your code here\n",
        "    print(f\"   Loss function {loss_fn} successfully called.\")\n",
        "    print(f\"\\n→ Continue to Step 4.2 to verify this is the right approach and see alternatives!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠︎  Error occurred: {e}\")\n",
        "    print(f\"\\n→ Continue to Step 4.2 for guided troubleshooting based on your error.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.2: Troubleshooting your loss\n",
        "\n",
        "Check what happened in Step 4.1 and follow the matching path below:\n",
        "\n",
        "##### ✖ Path A: Got an error: \"all elements of input should be between 0 and 1\"\n",
        "\n",
        "Your loss function expects probabilities (values in [0, 1]), but your model outputs raw logits (unbounded values). You need to convert logits to probabilities first.\n",
        "\n",
        "_[Continue to [Path A: Add activation function](#path-a-add-activation-function)]_\n",
        "\n",
        "##### ✖ Path B: Got a different error (TypeError, RuntimeError, shape mismatch, etc.)\n",
        "\n",
        "This is likely an issue with variable names, types, or argument order rather than the loss function choice itself.\n",
        "\n",
        "_[Continue to [Path B: Calling loss function on the right inputs](#path-b-calling-loss-function-on-the-right-inputs)]_\n",
        "\n",
        "##### ✓ Path C: No error - loss computed successfully!\n",
        "\n",
        "Great! Your code ran without errors. But the loss function could still run successfully even if you're using the wrong loss function for binary classification with logits.\n",
        "\n",
        "**Run this check:**\n",
        "```python\n",
        "print(f\"Loss function used: {loss_fn}\")\n",
        "print(f\"Loss value: {loss_value.item():.4f}\")\n",
        "```\n",
        "\n",
        "**Interpret your result:**\n",
        "- **Loss between 0.3 - 2.0?** → Typical range for binary classification losses ✓\n",
        "- **Getting a different loss range?** → You may not be using the right loss: Think of classification vs. regression, for example. Try again Step 4.1 with a different loss, or continue to the recommended approach!\n",
        "\n",
        "> **Want to see PyTorch's recommended approach?** _Continue to [Path C: Discover PyTorch's recommended approach](#path-c-discover-pytorchs-recommended-approach)._\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Path A: Add activation function\n",
        "\n",
        "If you saw an error mentioning \"target must be in [0, 1]\" or similar, your loss function expects probabilities (values between 0 and 1) [e.g., `BCELoss`], but your model outputs raw logits (unbounded values).\n",
        "\n",
        "**Want to fix this?** Convert logits to probabilities using an activation function that maps any real number to [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path A: Fix the input mismatch\n",
        "# TODO: Convert logits to probabilities, then compute BCELoss\n",
        "# HINT: You need an activation function that maps any real number to the range [0, 1].\n",
        "# Reference: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "\n",
        "# Step 1: Convert logits to probabilities using the right activation function\n",
        "probabilities =  # Add your code here\n",
        "\n",
        "# Step 2: Re-create your loss (expects probabilities as input)\n",
        "probability_based_loss =  # Add your code here\n",
        "\n",
        "# Step 3: Compute loss with the probabilities and true y tensor labels\n",
        "try:\n",
        "    loss_manual_fix = probability_based_loss(probabilities, y_tensor) # Add your code here\n",
        "\n",
        "    print(f\"✓ Fixed approach: {loss_manual_fix.item():.4f}\")\n",
        "    print(f\"\\nProbability range: [{probabilities.min():.4f}, {probabilities.max():.4f}]\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠︎  Still getting an error: {e}\")\n",
        "    print(f\"\\nMake sure your probabilities are in the range [0, 1], and check [Path B] for possible input mismatches\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Path B: Calling loss function on the right inputs\n",
        "\n",
        "If you saw a type error, runtime error, or similar, this is likely an issue with variable names, types, or argument order rather than the loss function choice itself.\n",
        "\n",
        "**Want to fix this?** Check your inputs are correct, and fix your loss based on checks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's check your inputs are correct\n",
        "print(\"Debugging - checking inputs:\")\n",
        "print(f\"Logits - Type: {type(logits)}, Shape: {logits.shape}\")\n",
        "print(f\"Labels - Type: {type(y_tensor)}, Shape: {y_tensor.shape}\")\n",
        "print(f\"Sample logits: {logits[:3]}\")\n",
        "print(f\"Sample labels: {y_tensor[:3]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Fix your loss computation based on the checks above\n",
        "# Make sure you're using:\n",
        "# - logits (model predictions)\n",
        "# - y_tensor (binary labels)\n",
        "# - Correct order: loss_fn(predictions/logits, labels)\n",
        "\n",
        "loss_fn =  # Add your code here\n",
        "\n",
        "try:\n",
        "    fixed_loss =  # Add your code here\n",
        "\n",
        "    print(f\"✓ Loss computed successfully: {loss_value.item():.4f}\")\n",
        "    print(f\"\\nIf this worked, continue to Path C to verify you're using the right loss.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠︎  Still getting an error: {e}\")\n",
        "    print(f\"\\nMake sure your inputs to loss_fn are correct and, if they are, check [Path A] for possible activation requirements\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Path C: Discover PyTorch's recommended approach\n",
        "\n",
        "The recommended approach for binary classification in PyTorch is to use [`BCEWithLogitsLoss()`](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) which combines the activation function and loss computation into one numerically stable operation. It expects raw logits as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Use BCEWithLogitsLoss on raw logits\n",
        "# HINT: This loss function expects raw logits and handles the activation internally\n",
        "# Reference: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
        "\n",
        "bce_with_logits_loss =  # Add your code here\n",
        "\n",
        "try:\n",
        "    # Compute the loss with predicted logits and true y tensor labels (no activation needed!)\n",
        "    loss_with_logits =  # Add your code here\n",
        "\n",
        "    print(f\"✓ BCEWithLogitsLoss on raw logits: {loss_with_logits.item():.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠︎  Error: {e}\")\n",
        "    print(f\"Check that you are using the BCEWithLogitsLoss loss function and passing (logits, y_tensor).\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**BCELoss vs BCEWithLogitLoss**: If you completed Path A, you'll notice the loss values are nearly identical (if not identical). Both approaches compute the same loss mathematically. So why does PyTorch provide BCEWithLogitsLoss? We'll explore the answer (numerical stability) in Step 4.4."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.3: Compare loss approaches across batches\n",
        "\n",
        "Let's systematically compare the two common approaches for binary classification (manual sigmoid + BCELoss vs BCEWithLogitsLoss) across multiple random batches to see if they behave consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute both losses across multiple random batches\n",
        "num_batches = 10\n",
        "losses_manual = []\n",
        "losses_with_logits = []\n",
        "\n",
        "# Create loss functions\n",
        "bce_loss = nn.BCELoss()\n",
        "bce_with_logits_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Get new predictions from model (simulating different batches)\n",
        "    batch_indices = torch.randperm(len(X_tensor))[:50]\n",
        "    X_batch = X_tensor[batch_indices]\n",
        "    y_batch = y_tensor[batch_indices]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits_batch = model(X_batch)\n",
        "    \n",
        "    # Approach 1: Manual sigmoid + BCELoss\n",
        "    probs_batch = torch.sigmoid(logits_batch)\n",
        "    loss_manual = bce_loss(probs_batch, y_batch)\n",
        "    losses_manual.append(loss_manual.item())\n",
        "    \n",
        "    # Approach 2: BCEWithLogitsLoss\n",
        "    loss_logits = bce_with_logits_loss(logits_batch, y_batch)\n",
        "    losses_with_logits.append(loss_logits.item())\n",
        "\n",
        "print(f\"Manual sigmoid + BCELoss across {num_batches} batches:\")\n",
        "print(f\"Mean loss: {np.mean(losses_manual):.4f} ± {np.std(losses_manual):.4f}\")\n",
        "\n",
        "print(f\"\\nBCEWithLogitsLoss across {num_batches} batches:\")\n",
        "print(f\"Mean loss: {np.mean(losses_with_logits):.4f} ± {np.std(losses_with_logits):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create side-by-side comparison plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Loss values across batches\n",
        "x = np.arange(num_batches)\n",
        "axes[0].plot(x, losses_manual, 'o-', label='Manual sigmoid + BCELoss', \n",
        "            color='orange', linewidth=2, markersize=8)\n",
        "axes[0].plot(x, losses_with_logits, 's--', label='BCEWithLogitsLoss', \n",
        "            color='blue', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Batch Number', fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('Loss Value', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Loss Values Across Batches', fontsize=12, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Side-by-side histograms\n",
        "bin_edges = np.linspace(min(min(losses_manual), min(losses_with_logits)),\n",
        "                        max(max(losses_manual), max(losses_with_logits)), 16)\n",
        "axes[1].hist([losses_manual, losses_with_logits], bins=bin_edges,\n",
        "            label=['Manual sigmoid + BCELoss', 'BCEWithLogitsLoss'],\n",
        "            color=['orange', 'blue'], alpha=0.7, edgecolor='black')\n",
        "axes[1].set_xlabel('Loss Value', fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Loss Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "print(\"\\nBoth methods produce the exact same results on normal data.\")\n",
        "print(\"But BCEWithLogitsLoss has a hidden advantage: numerical stability...\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.4 Testing numerical stability\n",
        "\n",
        "The real difference between these approaches appears with extreme logit values. Let's test with very large positive and negative logits to see where manual sigmoid fails.\n",
        "\n",
        "**Your task**: Test both approaches with extreme logits and observe what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with extreme logits that reveal the clamping behavior\n",
        "extreme_logits = torch.tensor([-100.0, -50.0, -10.0, 0.0, 10.0, 50.0, 100.0])\n",
        "extreme_labels = torch.tensor([1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0])\n",
        "\n",
        "print(\"Testing numerical stability with extreme logits:\\n\")\n",
        "print(f\"{'Logit':<10} {'Sigmoid':<20} {'Manual+BCE':<20} {'BCEWithLogits':<20} {'Difference':<15}\")\n",
        "print(\"=\"*85)\n",
        "\n",
        "for logit, label in zip(extreme_logits, extreme_labels):\n",
        "    # Manual sigmoid + BCELoss\n",
        "    prob = torch.sigmoid(logit)\n",
        "    loss_manual = bce_loss(prob.unsqueeze(0), label.unsqueeze(0))\n",
        "    \n",
        "    # BCEWithLogitsLoss\n",
        "    loss_logits = bce_with_logits_loss(logit.unsqueeze(0), label.unsqueeze(0))\n",
        "    \n",
        "    # Calculate difference\n",
        "    diff = abs(loss_manual.item() - loss_logits.item())\n",
        "    \n",
        "    sigmoid_str = f\"{prob.item():.15f}\"\n",
        "    manual_str = f\"{loss_manual.item():.6f}\"\n",
        "    logits_str = f\"{loss_logits.item():.6f}\"\n",
        "    diff_str = f\"{diff:.6f}\" if diff > 1e-6 else \"~0\"\n",
        "    \n",
        "    print(f\"{logit.item():<10.1f} {sigmoid_str:<20} {manual_str:<20} {logits_str:<20} {diff_str:<15}\")\n",
        "\n",
        "print(\"\\nKey observations:\")\n",
        "print(\"   • Logit = 10: Small difference (0.0005) - start of numerical issues\")\n",
        "print(\"   • Logit = 50: Manual gives 100, BCEWithLogits gives 50 (50-point error!)\")\n",
        "print(\"   • BCEWithLogitsLoss maintains accuracy across all extreme values\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why BCEWithLogitsLoss is more stable**: When logits are extreme (±50, ±100), sigmoid returns exactly 0.0 or 1.0, making log(0) = -∞. \n",
        "> \n",
        "> - `BCELoss` clamps this to -100 to prevent infinity, but creates approximated loss values (notice the 50-point error at logit=50). \n",
        "> - `BCEWithLogitsLoss` uses the log-sum-exp trick—computing `log(sigmoid(x)) = -log(1 + e^(-x))`—, which never evaluates log(0) and gives mathematically correct values. \n",
        "> \n",
        "> This matters because, during training, confident models can produce extreme logits and this numerical accuracy matters for reliable convergence.\n",
        ">\n",
        "> <details> <summary><i>But why do numerical differences not show at logit=100, even though the logit is even more extreme?</i></summary> \n",
        "> \n",
        "> Actually, they do! But both methods hit their limits. At such extreme values, even the log-sum-exp trick can't avoid floating-point overflow (e^100 ≈ infinity), so both approaches converge to the same approximation. The key insight: BCEWithLogitsLoss stays accurate **longer** (through logit=50), degrading only at values rarely seen in practice (100+).\n",
        "> </details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.5: Collect your thoughts\n",
        "\n",
        "Now that you've experimented with different loss approaches, reflect on what you've learned by answering these questions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: Analysis Question 1\n",
        "\n",
        "**If you encountered an error in Step 4.1, what caused it? What range of values does BCELoss expect as input, and why did raw logits cause a problem?**\n",
        "\n",
        "_Write your answer below:_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: Analysis Question 2\n",
        "\n",
        "**Manual sigmoid + BCELoss and BCEWithLogitsLoss produced similar results in Step 4.3. So why does PyTorch recommend BCEWithLogitsLoss? What advantage did you observe in Step 4.4?**\n",
        "\n",
        "_Write your answer below:_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: Analysis Question 3\n",
        "\n",
        "**When would you use CrossEntropyLoss instead of BCEWithLogitsLoss? What's the fundamental difference between binary and multi-class classification?**\n",
        "\n",
        "_Write your answer below:_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've discovered firsthand why choosing the right loss function variant matters for deep learning.\n",
        "\n",
        "**What you've accomplished:**\n",
        "\n",
        "- [x] **Experimented with loss function selection** - Chose and tested a loss function for binary classification\n",
        "- [x] **Experienced potential mismatches** - Discovered what happens when loss expects probabilities but receives logits\n",
        "- [x] **Learned multiple valid approaches** - Applied sigmoid manually or used BCEWithLogitsLoss\n",
        "- [x] **Tested numerical stability** - Observed how extreme logits break manual sigmoid but not combined losses\n",
        "- [x] **Built loss function intuition** - Understand why PyTorch provides specialized variants\n",
        "\n",
        "**Critical insights:**\n",
        "\n",
        "- **Loss functions must match model output format** - Raw logits need BCEWithLogitsLoss, probabilities need BCELoss\n",
        "- **Numerical stability matters in production** - Combined sigmoid+loss implementations prevent overflow/underflow\n",
        "- **Trust library implementations** - They handle edge cases and optimizations you might miss\n",
        "- **Output layer design affects loss choice** - No activation (logits) vs sigmoid activation determines which loss to use\n",
        "\n",
        "Loss functions are PyTorch objects that compare predictions to targets and output one scalar number: the smaller it is, the better your model's predictions. But choosing the right loss variant is just as important as choosing the right loss family. Always match your loss function to both your problem type (regression vs classification) and your model's output format (logits vs probabilities).\n",
        "\n",
        "> **Next steps to explore**: Experiment with multi-class classification with CrossEntropyLoss, test the model’s behavior under class imbalance using weighted losses, and visualize the loss surface to better understand how training adjusts model parameters."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
