{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 6: Compare Optimizers - SGD vs. Adam\n",
        "\n",
        "When training neural networks, the optimizer you choose determines how weights get updated to reduce loss. But does the choice between SGD and Adam really make a difference? In this exercise, you'll train the same model twice, once with each optimizer, to discover how optimization strategy affects training speed and final performance.\n",
        "\n",
        "> **Overview**: Train identical neural networks using two different optimizers (SGD and Adam) on the same dataset, then compare their loss curves to understand how optimization strategies affect learning dynamics.\n",
        "> \n",
        "> **Scenario**: Your automotive pricing team needs faster model updates as inventory changes daily. The current model trains with SGD, and you are wondering whether modern optimizers like Adam can speed up training. Your task: run a controlled experiment to see if switching optimizers can reduce training time while maintaining accuracy.\n",
        "> \n",
        "> **Goal**: Build intuition for how different optimizers learn by comparing their convergence speed, stability, and final performance on the same task.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, NumPy, Matplotlib\n",
        "> \n",
        "> **Estimated Time**: 15-20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's import our libraries and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create output directory for visualizations\n",
        "output_dir = 'assets/exercise6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and prepare the data\n",
        "\n",
        "> Note: This step mirrors the exact same dataset and dataset processing as for [demo 6](/cd1818-intro-to-deep-learning/6-training/demo6-coding-the-5-step-training-loop-pytorch.ipynb).\n",
        "\n",
        "We'll use the [Vishaltiwari2019/Car-Price-Prediction](https://huggingface.co/datasets/Vishaltiwari2019/Car-Price-Prediction) dataset that contains specifications and prices for 1,000 vehicles. Our goal is to predict the price based on features like make, model, year, engine size, mileage, fuel type, and transmission.\n",
        "\n",
        "For preprocessing, we'll:\n",
        "1. Load the dataset\n",
        "2. Encode categorical features (Make, Model, Fuel Type, Transmission)\n",
        "3. Separate features and label\n",
        "3. Normalize numerical features for stable training\n",
        "5. Convert features and labels to PyTorch tensors\n",
        "\n",
        "**IMPORTANT: Feel free to skip this section to focus on the training loop itself**. Just know that we end up with 1,000 training examples, each with 7 features (encoded and normalized), predicting a continuous price value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load the dataset\n",
        "dataset = load_dataset('Vishaltiwari2019/Car-Price-Prediction', split='train')\n",
        "df = pd.DataFrame(dataset)\n",
        "print(f\"✓ Dataset loaded: {len(df)} entries found\\n\")\n",
        "\n",
        "# 2. Encode categorical features\n",
        "categorical_columns = ['Make', 'Model', 'Fuel Type', 'Transmission']\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 3. Separate features and target\n",
        "X = df.drop('Price', axis=1).values\n",
        "y = df['Price'].values.reshape(-1, 1)\n",
        "\n",
        "# 4. Normalize features\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y)\n",
        "\n",
        "# 5. Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.FloatTensor(y)\n",
        "\n",
        "print(f\"✓ Data prepared for training\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Input features: {X.shape[1]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **How does data relate to optimizer choice?** Look at the feature ranges in this dataset: Year spans roughly 2004-2015, while Mileage ranges from ~50,000-200,000. How might these vastly different scales affect an optimizer that uses a single fixed learning rate (like SGD) versus one that adapts learning rates per-parameter (like Adam)? \n",
        "> \n",
        "> Keep this question in mind as you compare the loss curves later."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define the model architecture\n",
        "\n",
        "> Note: This step mirrors the exact same model creation as for [demo 6](/cd1818-intro-to-deep-learning/6-training/demo6-coding-the-5-step-training-loop-pytorch.ipynb).\n",
        "\n",
        "We'll create a simple feedforward neural network with two hidden layers. This architecture is appropriate for small tabular data like ours: enough capacity to learn complex pricing patterns without overfitting on 1,000 training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PricePredictionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward neural network for regression.\n",
        "    \n",
        "    Architecture: 7 inputs -> 64 neurons -> 32 neurons -> 1 output\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=7):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        \n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \"\"\"\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation on output for regression\n",
        "        return x\n",
        "\n",
        "print(\"✓ Model architecture defined\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Experimental design tip**: We're using the exact same model architecture (2,625 parameters) for both runs; this is called a \"controlled experiment\". The only variable we change is the optimizer. If we also changed the architecture or initialization, we couldn't be sure whether differences came from the optimizer or something else. \n",
        "> \n",
        "> This discipline is crucial when doing ML research or debugging production models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train with both optimizers and compare\n",
        "\n",
        "Now you'll run a controlled experiment: train the same model architecture twice—once with SGD, once with Adam—and compare their learning dynamics."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1: Define the training function\n",
        "\n",
        "First, let's create a reusable training function that implements the 5-step training loop. This function will work with any optimizer we pass to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, criterion, X_tensor, y_tensor, num_epochs=100):\n",
        "    \"\"\"\n",
        "    Train a model using the 5-step PyTorch training loop.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network to train\n",
        "        optimizer: The optimizer to use (SGD, Adam, etc.)\n",
        "        criterion: The loss function\n",
        "        X_tensor: Training features\n",
        "        y_tensor: Training labels\n",
        "        num_epochs: Number of training epochs\n",
        "        \n",
        "    Returns:\n",
        "        List of loss values for each epoch\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    \n",
        "    print(f\"Training with {optimizer.__class__.__name__}...\\n\")\n",
        "    print(f\"{'Epoch':>6} | {'Training Loss':>14}\")\n",
        "    print(\"-\" * 25)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # TODO: Implement the 5-step training loop\n",
        "        # Hint: It follows this pattern:\n",
        "        #   1. Pass data through model → 2. Calculate error → 3. Clear old gradients \n",
        "        #   → 4. Compute new gradients → 5. Update weights\n",
        "        # Don't forget to track the epoch's loss in `losses` at the end too!\n",
        "        # Reference: https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop\n",
        "        \n",
        "        # Add your code here\n",
        "\n",
        "        # Print progress every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"{epoch + 1:6d} | {loss.item():14.4f}\")\n",
        "    \n",
        "    print(f\"\\n✓ Training complete!\")\n",
        "    print(f\"Final loss: {losses[-1]:.4f}\\n\")\n",
        "    \n",
        "    return losses\n",
        "\n",
        "print(\"✓ Training function defined\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why use a function?** By wrapping the training loop in a function, we avoid code duplication and make it easy to experiment with different optimizers while keeping the training logic consistent. \n",
        "> \n",
        "> This is a common pattern in deep learning experiments."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2: Train with SGD\n",
        "\n",
        "Let's start by training the model using standard Stochastic Gradient Descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Create a fresh model for SGD training\n",
        "model_sgd = PricePredictionModel(input_size=7)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# TODO: 2. Create SGD optimizer with an appropriate learning rate\n",
        "# Hint: What two things does an optimizer need? (1) Which parameters to update, and (2) How big the steps should be.\n",
        "#       For learning rate, consider: SGD typically uses values in the range of 0.001 to 0.1\n",
        "# # Reference: https://pytorch.org/docs/stable/optim.html\n",
        "optimizer_sgd =  # Add your code here\n",
        "\n",
        "print(\"✓ SGD optimizer created\")\n",
        "print(f\"Optimizer: {optimizer_sgd.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Run SGD training\n",
        "sgd_losses = train_model(\n",
        "    model=model_sgd,\n",
        "    optimizer=optimizer_sgd,\n",
        "    criterion=criterion,\n",
        "    X_tensor=X_tensor,\n",
        "    y_tensor=y_tensor,\n",
        "    num_epochs=100\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What to expect**: SGD's loss should decrease steadily but slowly. It might take many epochs to converge, and the curve may show some oscillation. This is normal for basic gradient descent without momentum or adaptive learning rates."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3: Train with Adam\n",
        "\n",
        "Now let's train the exact same model architecture using the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Create a fresh model for Adam training (same architecture, different weights)\n",
        "model_adam = PricePredictionModel(input_size=7)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# TODO: 2. Create Adam optimizer with an appropriate learning rate\n",
        "# Hint: The syntax is very similar to SGD. What's different? Adam typically uses smaller learning rates.\n",
        "# Reference: https://pytorch.org/docs/stable/optim.html\n",
        "optimizer_adam =  # Add your code here\n",
        "\n",
        "print(\"✓ Adam optimizer created\")\n",
        "print(f\"Optimizer: {optimizer_adam.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Train with Adam\n",
        "adam_losses = train_model(\n",
        "    model=model_adam,\n",
        "    optimizer=optimizer_adam,\n",
        "    criterion=criterion,\n",
        "    X_tensor=X_tensor,\n",
        "    y_tensor=y_tensor,\n",
        "    num_epochs=100\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What to expect**: Adam's loss should drop much faster initially compared to SGD. You'll likely see a steep decline in the first 20-30 epochs, then gradual refinement. This rapid early convergence is why Adam has become the default choice for most deep learning tasks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4: Visualize and compare the optimizers\n",
        "\n",
        "Now let's plot both loss curves side-by-side to see the difference in training dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot both loss curves for comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(sgd_losses, label='SGD (LR=0.01)', linewidth=2, color='#E63946', alpha=0.8)\n",
        "plt.plot(adam_losses, label='Adam (LR=0.001)', linewidth=2, color='#2E86AB', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "plt.title('Optimizer Comparison: SGD vs. Adam', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'optimizer_comparison.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPTIMIZER COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nSGD:\")\n",
        "print(f\"  Initial loss: {sgd_losses[0]:.4f}\")\n",
        "print(f\"  Final loss:   {sgd_losses[-1]:.4f}\")\n",
        "print(f\"  Reduction:    {(sgd_losses[0] - sgd_losses[-1]) / sgd_losses[0] * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\nAdam:\")\n",
        "print(f\"  Initial loss: {adam_losses[0]:.4f}\")\n",
        "print(f\"  Final loss:   {adam_losses[-1]:.4f}\")\n",
        "print(f\"  Reduction:    {(adam_losses[0] - adam_losses[-1]) / adam_losses[0] * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\nFinal Loss Comparison:\")\n",
        "if adam_losses[-1] < sgd_losses[-1]:\n",
        "    improvement = (sgd_losses[-1] - adam_losses[-1]) / sgd_losses[-1] * 100\n",
        "    print(f\"  Adam achieved {improvement:.1f}% lower final loss than SGD\")\n",
        "else:\n",
        "    improvement = (adam_losses[-1] - sgd_losses[-1]) / adam_losses[-1] * 100\n",
        "    print(f\"  SGD achieved {improvement:.1f}% lower final loss than Adam\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Reading the comparison**: When looking at the two optimizers' losses, focus on these three things:\n",
        "> 1. **Early convergence**: Which optimizer drops faster in the first 20-30 epochs?\n",
        "> 2. **Stability**: Which curve is smoother vs. more oscillatory?\n",
        "> 3. **Final performance**: Which achieves lower loss by epoch 100?\n",
        "> \n",
        "> In a production setting where models are retrained frequently (like our car marketplace), these differences are critical. If Adam reaches a good loss in ~30 epochs while SGD needs ~70, that gap can be the difference between finishing retraining overnight vs. stretching into multiple days. \n",
        "> \n",
        "> This directly impacts how fresh your deployed models are and how quickly your pipeline can react to new data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.5: Experiment with learning rates\n",
        "\n",
        "SGD has a reputation for being sensitive to learning rate choice—too high and training becomes unstable, too low and it barely learns. But when tuned well, SGD can achieve excellent results, which is why it remains popular in production systems. Can you find a better learning rate for your SGD model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Experiment with different learning rates\n",
        "# Hint: Try a range of values from 0.1 to 0.0001\n",
        "learning_rates_to_try = []  # Add your choices here\n",
        "sgd_experiments = {}\n",
        "\n",
        "for lr in learning_rates_to_try:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing SGD with learning rate: {lr}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    # Create model\n",
        "    model_sgd_exp = PricePredictionModel(input_size=7)\n",
        "\n",
        "    # TODO: Define optimizer with current learning rate\n",
        "    # Hint: Remember that optimizers need the model's parameters and a learning rate.\n",
        "    optimizer_sgd_exp =  # Add your code here  \n",
        "    \n",
        "    # Train and store results\n",
        "    losses = train_model(\n",
        "        model=model_sgd_exp,\n",
        "        optimizer=optimizer_sgd_exp,\n",
        "        criterion=criterion,\n",
        "        X_tensor=X_tensor,\n",
        "        y_tensor=y_tensor,\n",
        "        num_epochs=100\n",
        "    )\n",
        "    sgd_experiments[f'SGD (LR={lr})'] = losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot all your SGD experiments alongside Adam\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot original SGD and Adam\n",
        "plt.plot(sgd_losses, label='SGD (LR=0.01)', linewidth=2, color='#E63946', alpha=0.7)\n",
        "plt.plot(adam_losses, label='Adam (LR=0.001)', linewidth=2, color='#2E86AB', alpha=0.9)\n",
        "\n",
        "# Plot your experimental SGD runs\n",
        "colors = ['#F4A261', '#E9C46A', '#F77F00', '#D62828']\n",
        "for idx, (label, losses) in enumerate(sgd_experiments.items()):\n",
        "    plt.plot(losses, label=label, linewidth=2, linestyle='--', \n",
        "             color=colors[idx % len(colors)], alpha=0.7)\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "plt.title('SGD Learning Rate Experiments vs. Adam', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'learning_rate_experiments.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LEARNING RATE EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nOriginal:\")\n",
        "print(f\"  SGD (LR=0.01):    Final loss = {sgd_losses[-1]:.4f}\")\n",
        "print(f\"  Adam (LR=0.001):  Final loss = {adam_losses[-1]:.4f}\")\n",
        "print(f\"\\nYour experiments:\")\n",
        "for label, losses in sgd_experiments.items():\n",
        "    print(f\"  {label:20s} Final loss = {losses[-1]:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **The production reality**: In real projects, you rarely get hyperparameter tuning \"right\" on the first try. But, learning rate choice can dramatically change an optimizer's behavior. Some values make SGD converge quickly, others cause it to crawl or even diverge. This sensitivity is why Adam became popular: its adaptive learning rates reduce the need for this manual tuning.\n",
        "> \n",
        "> The lesson: don't judge an optimizer by a single configuration."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Collect your thoughts\n",
        "\n",
        "Now that you've implemented training loops and experimented with optimizers, reflect on the key concepts by answering these questions.\n",
        "\n",
        "### TODO: Analysis Question 1\n",
        "\n",
        "Why do we call `optimizer.zero_grad()` at the start of each training iteration? What would happen if we forgot this step?\n",
        "\n",
        "**Hint**: Think about what `.backward()` does to the gradients. Does it replace them or add to them? What would happen if old gradients kept accumulating?\n",
        "\n",
        "_Write your answer here:_\n",
        "\n",
        "\n",
        "### TODO: Analysis Question 2\n",
        "\n",
        "What does `loss.backward()` do behind the scenes? Why don't we need to manually calculate gradients for each of the 2,625 parameters?\n",
        "\n",
        "**Hint**: Review what you learned about backpropagation and the chain rule. How does PyTorch's computational graph enable automatic differentiation?\n",
        "\n",
        "_Write your answer here:_\n",
        "\n",
        "\n",
        "### TODO: Analysis Question 3\n",
        "\n",
        "Based on your initial experiments (before learning rate tuning), did Adam or SGD reach lower loss faster? What specific features of Adam's algorithm enable this faster convergence?\n",
        "\n",
        "**Hint**: Think about momentum (smoothing gradients over time) and adaptive learning rates (adjusting per-parameter). How do these help with our mixed-scale features (Year vs. Mileage)?\n",
        "\n",
        "_Write your answer here:_\n",
        "\n",
        "\n",
        "### TODO: Analysis Question 4\n",
        "\n",
        "After your learning rate experiments in Step 4.5, what did you discover about SGD's performance? How much does learning rate choice matter compared to the choice of optimizer algorithm?\n",
        "\n",
        "**Hint**: Compare your best SGD result with Adam's performance. Could you make SGD competitive? What does this tell you about hyperparameter tuning vs. algorithm selection?\n",
        "\n",
        "_Write your answer here:_\n",
        "\n",
        "\n",
        "### TODO: Analysis Question 5\n",
        "\n",
        "Given what you've learned, when would you choose SGD over Adam in a real project? When would you stick with Adam?\n",
        "\n",
        "**Hint**:  Consider factors like: time available for tuning, production performance requirements, computational resources, and domain conventions (e.g., computer vision often uses SGD with momentum).\n",
        "\n",
        "_Write your answer here:_\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've completed a controlled experiment comparing two fundamental optimizers in deep learning.\n",
        "\n",
        "**What you've accomplished:**\n",
        "\n",
        "- [x] **Implemented the 5-step training loop** from scratch\n",
        "- [x] **Trained models with SGD and Adam** to compare optimizer behavior\n",
        "- [x] **Experimented with learning rates** to see how tuning affects SGD's performance\n",
        "- [x] **Analyzed training dynamics** through loss curves and convergence patterns\n",
        "- [x] **Understood the mechanics** of gradient computation, backpropagation, and weight updates\n",
        "\n",
        "**Critical insights:**\n",
        "\n",
        "- **The 5-step training loop is universal**: Whether you use SGD, Adam, or any other optimizer, the structure stays the same: forward → loss → zero_grad → backward → step\n",
        "- **Optimizer + learning rate choice significantly affects training**: Same model, same data, but dramatically different learning curves\n",
        "- **Adam's advantage is reliability**: While SGD can match Adam's performance with proper tuning, Adam's adaptive learning rates make it work well \"out of the box\" with less manual tuning\n",
        "- **Feature scales matter**: Datasets with mixed scales (like Year vs. Mileage) particularly benefit from Adam's per-parameter learning rates\n",
        "\n",
        "Understanding the training loop and optimizer behavior is fundamental to deep learning. The same model architecture can succeed or fail based on how you configure training. Adam's ease of use makes it the default choice for most tasks, but SGD with careful tuning remains competitive (especially in production systems where that extra performance matters!).\n",
        "\n",
        "**Next steps to explore:** Test SGD with momentum, add a learning-rate scheduler, compare AdamW, and experiment with other optimizers like RMSprop, AdaGrad, and NAdam to observe how different optimization strategies affect training."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
