{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo 7: Turning Raw Data into Trainable Tensors\n",
        "\n",
        "Every neural network starts with data, but raw data rarely looks like what a network needs. Before training can begin, you must transform messy CSV files into clean, standardized, batched tensors.\n",
        "\n",
        "> **Overview**: Build a complete data pipeline from scratch: load a real CSV dataset with Pandas, handle missing values, split data properly, standardize features with scikit-learn, and convert everything into batched PyTorch tensors.\n",
        "> \n",
        "> **Scenario**: Your telecom company has 4,000+ customer records with missing values, mixed scales, and categorical data. Before any model can predict churn, you need to transform this mess into clean, training-ready tensors.\n",
        "> \n",
        "> **Goal**: Understand the complete data flow from raw CSV to training-ready mini-batches, and see how proper preprocessing enables reliable model training.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, Pandas, scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's start by importing our libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Pandas version: 2.3.3\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load the data\n",
        "\n",
        "We'll use the [aai510-group1/telco-customer-churn](https://huggingface.co/datasets/aai510-group1/telco-customer-churn) dataset from Hugging Face, which contains 4,000+ real customer records from a telecommunications company.\n",
        "\n",
        "The full dataset has 49 features, but we'll focus on 10 key predictors that are most relevant for understanding customer churn.\n",
        "\n",
        "> **The preprocessing workflow ahead:**\n",
        "> \n",
        "> Here's a compact view of the full pipeline we'll build:\n",
        "> \n",
        "> 1. **Load data** → Convert Hugging Face dataset to Pandas DataFrame\n",
        "> 2. **Create subset** → Select only the 10 features we are interested into\n",
        "> 3. **Handle missing values** → Fill Internet Type nulls with \"None\"\n",
        "> 4. **Encode categories** → Convert Contract and Internet Type to numbers\n",
        "> 5. **Split data** → Separate into train (70%) and validation (30%) sets\n",
        "> 6. **Standardize features** → Scale to mean=0, std=1 (fit on train only)\n",
        "> 7. **Convert to tensors** → Transform NumPy arrays to PyTorch float32 tensors\n",
        "> 8. **Create TensorDataset** → Pair features with targets\n",
        "> 9. **Build DataLoaders** → Enable batching, shuffling, and efficient iteration\n",
        "> \n",
        "> Each step prepares the data for the next, transforming messy CSV into training-ready batches."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1: Load the data\n",
        "\n",
        "We'll load the telco customer churn dataset from Hugging Face and convert it to a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09a8acbb75bd49119783da465ef8dd35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "160bc43cc188489c93599eada7e5d9b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e7a04fb35b94783bbd70995c101c81f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca4a6550be3a40b3840fc994df6fcc9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "235a286cb7e3450dad4c79f27a9eda31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/4225 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62f802f66b6048d6a919ea710b538462",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/1409 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "176ccabf81d84d2b8c6c9886c3751f96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1409 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Dataset loaded: 4225 customer records\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the dataset from Hugging Face\n",
        "dataset = load_dataset('aai510-group1/telco-customer-churn', split='train')\n",
        "\n",
        "# Convert to Pandas DataFrame for easier manipulation\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(f\"✓ Dataset loaded: {len(df)} customer records\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2: Select key features\n",
        "\n",
        "From the 49 available features, we'll focus on 10 predictors that best capture customer churn patterns with a balanced mix of:\n",
        " - Demographics (age, dependents)\n",
        " - Service history (tenure, contract type)\n",
        " - Service details (internet type)\n",
        " - Financial metrics (charges)\n",
        " - Engagement indicators (satisfaction, referrals). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (4225, 10)\n",
            "\n",
            "First few rows:\n",
            "   Age  Dependents  Tenure in Months        Contract Internet Type  \\\n",
            "0   72           0                25        Two Year   Fiber Optic   \n",
            "1   27           0                35  Month-to-Month   Fiber Optic   \n",
            "2   59           0                46  Month-to-Month          None   \n",
            "3   25           0                27        One Year           DSL   \n",
            "4   31           0                58        One Year         Cable   \n",
            "\n",
            "   Monthly Charge  Total Charges  Satisfaction Score  Number of Referrals  \\\n",
            "0           88.40        2191.15                   3                    1   \n",
            "1           95.50        3418.20                   3                    0   \n",
            "2           19.60         851.20                   5                    3   \n",
            "3           45.85        1246.40                   4                    3   \n",
            "4           60.30        3563.80                   2                    1   \n",
            "\n",
            "   Churn  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      0  \n",
            "4      1  \n"
          ]
        }
      ],
      "source": [
        "# 2. Select the 10 key features we'll use for prediction\n",
        "feature_columns = [\n",
        "    'Age',                          # Demographic\n",
        "    'Dependents',                   # Demographic\n",
        "    'Tenure in Months',             # Service history\n",
        "    'Contract',                     # Service details (categorical)\n",
        "    'Internet Type',                # Service details (categorical - has missing values!)\n",
        "    'Monthly Charge',               # Financial\n",
        "    'Total Charges',                # Financial\n",
        "    'Satisfaction Score',           # Engagement\n",
        "    'Number of Referrals',          # Engagement\n",
        "]\n",
        "\n",
        "target_column = 'Churn'  # Binary: 0 = Stayed, 1 = Churned\n",
        "\n",
        "# Keep only the columns we need\n",
        "df = df[feature_columns + [target_column]]\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Feature selection in practice:** Feature selection is iterative. You'd experiment with more/fewer features, engineer new ones (like charges-per-month), transform skewed distributions, and refine based on model performance. \n",
        "> \n",
        "> This demo focuses on the preprocessing mechanics that apply regardless of which features you choose."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3: Handle missing values\n",
        "\n",
        "Real-world data always has issues. Let's investigate what we're dealing with before making any transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Types and Missing Values:\n",
            "============================================================\n",
            "             Column    Type  Missing  Missing %\n",
            "                Age   int64        0       0.00\n",
            "         Dependents   int64        0       0.00\n",
            "   Tenure in Months   int64        0       0.00\n",
            "           Contract  object        0       0.00\n",
            "      Internet Type  object      886      20.97\n",
            "     Monthly Charge float64        0       0.00\n",
            "      Total Charges float64        0       0.00\n",
            " Satisfaction Score   int64        0       0.00\n",
            "Number of Referrals   int64        0       0.00\n",
            "              Churn   int64        0       0.00\n"
          ]
        }
      ],
      "source": [
        "# Check data types and missing values\n",
        "print(\"Data Types and Missing Values:\")\n",
        "print(\"=\" * 60)\n",
        "info_df = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Type': df.dtypes.values,\n",
        "    'Missing': df.isnull().sum().values,\n",
        "    'Missing %': (df.isnull().sum().values / len(df) * 100).round(2)\n",
        "})\n",
        "print(info_df.to_string(index=False))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Key observations from the data:**\n",
        "> \n",
        "> - **Mixed data types**: We have both numeric features (Age, charges) and categorical features (Contract, Internet Type) that will need different preprocessing approaches\n",
        "> - **Missing values**: Internet Type is the only feature with missing values.\n",
        "> <details>\n",
        "> <summary><b>What do the missing values in Internet Type mean?</b></summary>\n",
        "> \n",
        "> The ~1,500 missing values in Internet Type aren't errors or data collection failures: they represent customers who don't have internet service at all. The missingness itself carries information: \"no internet type\" means \"no internet subscription.\"\n",
        "> \n",
        "> This is why we'll **fill with \"None\"** rather than drop these rows. Dropping would remove 21% of our data and lose valuable patterns (customers without internet might have different churn behavior). Creating a \"None\" category preserves this information and lets the model learn from it.\n",
        "> \n",
        "> **General principle:** When missingness is meaningful (not random), preserve it as a feature. When it's random noise or data errors, consider dropping or imputing with statistics (mean, median, mode).\n",
        "> \n",
        "> </details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values after handling:\n",
            "Age                    0\n",
            "Dependents             0\n",
            "Tenure in Months       0\n",
            "Contract               0\n",
            "Internet Type          0\n",
            "Monthly Charge         0\n",
            "Total Charges          0\n",
            "Satisfaction Score     0\n",
            "Number of Referrals    0\n",
            "Churn                  0\n",
            "dtype: int64\n",
            "\n",
            "✓ All missing values handled!\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values in Internet Type\n",
        "# Fill with \"None\" to indicate \"no internet service\"\n",
        "df['Internet Type'] = df['Internet Type'].fillna('None')\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"Missing values after handling:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n✓ All missing values handled!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4: Encode categorical features\n",
        "\n",
        "Neural networks need numeric inputs. Let's convert our categorical features (Contract, Internet Type) into numbers using one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical features:\n",
            "Contract types: ['Two Year' 'Month-to-Month' 'One Year']\n",
            "Internet types: ['Fiber Optic' 'None' 'DSL' 'Cable']\n",
            "\n",
            "✓ Encoding complete!\n",
            "Original shape: (4225, 10)\n",
            "After encoding: (4225, 13)\n",
            "\n",
            "New columns created: 3 additional features\n"
          ]
        }
      ],
      "source": [
        "# Check unique values in categorical columns\n",
        "print(\"Categorical features:\")\n",
        "print(f\"Contract types: {df['Contract'].unique()}\")\n",
        "print(f\"Internet types: {df['Internet Type'].unique()}\")\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df_encoded = pd.get_dummies(df, columns=['Contract', 'Internet Type'], drop_first=True)\n",
        "\n",
        "print(f\"\\n✓ Encoding complete!\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "print(f\"After encoding: {df_encoded.shape}\")\n",
        "print(f\"\\nNew columns created: {df_encoded.shape[1] - df.shape[1]} additional features\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why one-hot encoding?** Each categorical value becomes a binary column (0 or 1). We use `drop_first=True` to avoid the \"dummy variable trap\" where columns are perfectly correlated. \n",
        "> \n",
        "> For example, if a customer isn't \"Month-to-Month\" or \"One Year\", they must be \"Two Year\". So, we don't need all three columns."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5: Split data (the Golden Rule in action)\n",
        "\n",
        "Before any preprocessing that learns from data, we must split into training and validation sets. This prevents **information leakage**, i.e., when test data accidentally influences training.\n",
        "\n",
        "> **Why no test set?** For this demo, train/val is sufficient to learn the preprocessing workflow. In a real project where you'd deploy a model, you'd use a three-way split (train/val/test) to get an honest final performance estimate. The preprocessing steps remain identical regardless of how many splits you create."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before split:\n",
            "Features shape: (4225, 12)\n",
            "Target shape: (4225,)\n",
            "Target distribution: {0: 3104, 1: 1121}\n",
            "\n",
            "============================================================\n",
            "After split:\n",
            "Training set: 2957 samples (70.0%)\n",
            "Validation set: 1268 samples (30.0%)\n",
            "\n",
            "Training target distribution: {0: 2172, 1: 785}\n",
            "Validation target distribution: {0: 932, 1: 336}\n",
            "\n",
            "✓ Data split complete! (Golden rule: Split BEFORE preprocessing)\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target\n",
        "X = df_encoded.drop('Churn', axis=1)\n",
        "y = df_encoded['Churn']\n",
        "\n",
        "print(\"Before split:\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# Split into training (70%) and validation (30%) sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.30,      # 30% for validation\n",
        "    random_state=42,     # For reproducibility\n",
        "    stratify=y           # Maintain the same churn ratio in both sets\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"After split:\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTraining target distribution: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Validation target distribution: {y_val.value_counts().to_dict()}\")\n",
        "print(\"\\n✓ Data split complete! (Golden rule: Split BEFORE preprocessing)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **The golden rule in practice**: Notice we split BEFORE standardization. If we calculated mean and standard deviation on the entire dataset, information from validation examples would leak into our training process. By splitting first, we ensure the validation set truly represents unseen data.\n",
        ">\n",
        "> **Why stratify?** The `stratify=y` parameter ensures both sets have the same churn rate (~26.5%). Without this, we might accidentally get 30% churners in training but only 20% in validation, making comparisons unreliable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6: Standardize features\n",
        "\n",
        "Our features have wildly different scales (Age: 20-80, Total Charges: 100-8,000). Standardization transforms them to have mean=0 and std=1, helping gradient descent converge faster and more reliably."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature scales BEFORE standardization:\n",
            "============================================================\n",
            "        Age  Dependents  Tenure in Months  Monthly Charge  Total Charges  \\\n",
            "mean  46.57        0.24             32.52           64.52        2287.90   \n",
            "std   16.69        0.43             24.60           30.09        2267.20   \n",
            "min   19.00        0.00              1.00           18.40          18.80   \n",
            "max   80.00        1.00             72.00          118.75        8672.45   \n",
            "\n",
            "      Satisfaction Score  Number of Referrals  \n",
            "mean                3.25                 2.00  \n",
            "std                 1.21                 3.06  \n",
            "min                 1.00                 0.00  \n",
            "max                 5.00                11.00  \n"
          ]
        }
      ],
      "source": [
        "# Check the current feature scales\n",
        "print(\"Feature scales BEFORE standardization:\")\n",
        "print(\"=\" * 60)\n",
        "print(X_train.describe().loc[['mean', 'std', 'min', 'max']].round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Standardization complete!\n",
            "\n",
            "Feature scales AFTER standardization (training set):\n",
            "============================================================\n",
            "       Age  Dependents  Tenure in Months  Monthly Charge  Total Charges  \\\n",
            "mean  0.00        0.00              0.00            0.00           0.00   \n",
            "std   1.00        1.00              1.00            1.00           1.00   \n",
            "min  -1.65       -0.56             -1.28           -1.53          -1.00   \n",
            "max   2.00        1.79              1.61            1.80           2.82   \n",
            "\n",
            "      Satisfaction Score  Number of Referrals  Contract_One Year  \\\n",
            "mean               -0.00                 0.00              -0.00   \n",
            "std                 1.00                 1.00               1.00   \n",
            "min                -1.86                -0.66              -0.51   \n",
            "max                 1.44                 2.94               1.95   \n",
            "\n",
            "      Contract_Two Year  Internet Type_DSL  Internet Type_Fiber Optic  \\\n",
            "mean              -0.00               0.00                       0.00   \n",
            "std                1.00               1.00                       1.00   \n",
            "min               -0.60              -0.54                      -0.88   \n",
            "max                1.66               1.84                       1.14   \n",
            "\n",
            "      Internet Type_None  \n",
            "mean                0.00  \n",
            "std                 1.00  \n",
            "min                -0.52  \n",
            "max                 1.91  \n"
          ]
        }
      ],
      "source": [
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# CRITICAL: Fit the scaler ONLY on training data\n",
        "# This calculates mean and std from training set only\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform both sets using the training statistics\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "print(\"\\n✓ Standardization complete!\")\n",
        "print(\"\\nFeature scales AFTER standardization (training set):\")\n",
        "print(\"=\" * 60)\n",
        "# Convert back to DataFrame for easier viewing\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "print(X_train_scaled_df.describe().loc[['mean', 'std', 'min', 'max']].round(2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **When to use StandardScaler vs. alternatives:**\n",
        "> - **StandardScaler (our choice)**: Best for tabular data with different units and scales, robust to outliers since it uses mean/std rather than min/max\n",
        "> - **MinMaxScaler**: Better when you need bounded outputs [0,1] or when data has no extreme outliers\n",
        "> - **RobustScaler**: Use when data has many outliers, since it uses median/IQR instead of mean/std\n",
        "> \n",
        "> Find more information at [sklearn.preprocessing](https://scikit-learn.org/stable/api/sklearn.preprocessing.html)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.7: Convert to PyTorch tensors\n",
        "\n",
        "NumPy arrays are great for preprocessing, but PyTorch models need tensors. Let's convert our standardized data into the right format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Tensor conversion complete!\n",
            "\n",
            "Training tensors:\n",
            "  Features: torch.Size([2957, 12]) | dtype: torch.float32\n",
            "  Targets:  torch.Size([2957]) | dtype: torch.float32\n",
            "\n",
            "Validation tensors:\n",
            "  Features: torch.Size([1268, 12]) | dtype: torch.float32\n",
            "  Targets:  torch.Size([1268]) | dtype: torch.float32\n",
            "\n",
            "First training example:\n",
            "Features (first 5): tensor([-0.1538,  1.7940,  1.0360,  0.0757,  0.7422])\n",
            "Target: 0.0 (Stayed)\n"
          ]
        }
      ],
      "source": [
        "# Convert features to float32 tensors (standard for neural networks)\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# Convert targets to float32 tensors (for binary classification with BCELoss)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "print(\"✓ Tensor conversion complete!\")\n",
        "print(\"\\nTraining tensors:\")\n",
        "print(f\"  Features: {X_train_tensor.shape} | dtype: {X_train_tensor.dtype}\")\n",
        "print(f\"  Targets:  {y_train_tensor.shape} | dtype: {y_train_tensor.dtype}\")\n",
        "print(\"\\nValidation tensors:\")\n",
        "print(f\"  Features: {X_val_tensor.shape} | dtype: {X_val_tensor.dtype}\")\n",
        "print(f\"  Targets:  {y_val_tensor.shape} | dtype: {y_val_tensor.dtype}\")\n",
        "\n",
        "# Quick sanity check: print first example\n",
        "print(\"\\nFirst training example:\")\n",
        "print(f\"Features (first 5): {X_train_tensor[0, :5]}\")\n",
        "print(f\"Target: {y_train_tensor[0].item()} ({'Churned' if y_train_tensor[0].item() == 1 else 'Stayed'})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why float32?** Neural networks typically use 32-bit floating-point precision as a balance between accuracy and memory/speed (and modern GPUs are optimized for float32 operations!). BEWARE: Using the wrong dtype (like float64 or int) can cause cryptic errors during training.\n",
        ">\n",
        "> <details><summary><b> What about tensor shapes: are they as expected?</b></summary>\n",
        "> <br>\n",
        "> \n",
        "> **Yes, they're exactly right!** \n",
        "> - The features tensor has shape `(4930, 13)`, meaning 4,930 training examples with 13 features each (after one-hot encoding).\n",
        "> - The target tensor has shape `(4930,)` with one label per example, i.e., a 1D tensor with 4,930 values.\n",
        "> \n",
        "> *Key pattern:* The first dimension always represents the number of examples (the \"batch dimension\"), and the second dimension represents features per example. This `(num_examples, num_features)` format is standard across all deep learning frameworks.\n",
        ">\n",
        "> </details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.8: Create TensorDataset (pairing features with targets)\n",
        "\n",
        "[TensorDataset](https://docs.pytorch.org/docs/stable/data.html) is PyTorch's way of keeping features and targets synchronized. It wraps our tensors into a dataset that can be easily batched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ TensorDatasets created!\n",
            "\n",
            "Training dataset: 2957 examples\n",
            "Validation dataset: 1268 examples\n",
            "\n",
            "Single dataset item:\n",
            "  Features shape: torch.Size([12])\n",
            "  Target shape: torch.Size([])\n",
            "  Target value: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "print(\"✓ TensorDatasets created!\")\n",
        "print(f\"\\nTraining dataset: {len(train_dataset)} examples\")\n",
        "print(f\"Validation dataset: {len(val_dataset)} examples\")\n",
        "\n",
        "# Show what a single dataset item looks like\n",
        "features, target = train_dataset[0]\n",
        "print(\"\\nSingle dataset item:\")\n",
        "print(f\"  Features shape: {features.shape}\")\n",
        "print(f\"  Target shape: {target.shape}\")\n",
        "print(f\"  Target value: {target.item()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What TensorDataset does**: It creates a simple wrapper that returns `(features, target)` pairs. When you access `train_dataset[0]`, you get the features and target for the first example. This synchronization is crucial: when we shuffle or batch the data, the features and targets stay properly aligned."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.9: Build DataLoaders (the bridge to training)\n",
        "\n",
        "[DataLoader](https://docs.pytorch.org/docs/stable/data.html) handles the heavy lifting: batching data, shuffling for training, and efficiently feeding examples to your model during training loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ DataLoaders created!\n",
            "\n",
            "Training loader:\n",
            "  Total examples: 2957\n",
            "  Batch size: 32\n",
            "  Number of batches: 93\n",
            "  Shuffle: True\n",
            "\n",
            "Validation loader:\n",
            "  Total examples: 1268\n",
            "  Batch size: 32\n",
            "  Number of batches: 40\n",
            "  Shuffle: False\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 32  # Process 32 examples at a time\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,          # Shuffle training data each epoch\n",
        "    drop_last=False        # Keep the last incomplete batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,         # Don't shuffle validation data\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(\"✓ DataLoaders created!\")\n",
        "print(f\"\\nTraining loader:\")\n",
        "print(f\"  Total examples: {len(train_dataset)}\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Number of batches: {len(train_loader)}\")\n",
        "print(f\"  Shuffle: True\")\n",
        "\n",
        "print(f\"\\nValidation loader:\")\n",
        "print(f\"  Total examples: {len(val_dataset)}\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Number of batches: {len(val_loader)}\")\n",
        "print(f\"  Shuffle: False\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why shuffle training but not validation?** Shuffling training data prevents the model from learning spurious patterns based on the order of examples. If all churned customers appeared at the end, the model might learn time-based patterns rather than true churn indicators. \n",
        ">\n",
        "> We DON'T shuffle validation data because we want consistent, reproducible evaluation. The order doesn't matter for computing metrics, and keeping it fixed makes debugging easier.\n",
        ">\n",
        "><details><summary><b>How many batches will we get during training?</b></summary>\n",
        ">\n",
        "> *Calculation:* With 2,957 training examples and `batch_size=32`, we get: 2,957 ÷ 32 = 92.4 → **93 batches**\n",
        "> \n",
        "> Since we set `drop_last=False`, the final batch contains the remaining 13 examples (2,957 - 92×32 = 13). If we had set `drop_last=True`, we'd have 92 full batches and those last 13 examples would be discarded.\n",
        "> \n",
        "> *Why this matters:* The number of batches determines how many gradient updates happen per epoch. More batches = more frequent updates but smaller gradient estimates per update.\n",
        "> \n",
        "</details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Inspect a batch\n",
        "\n",
        "Let's look at what actually gets fed into your neural network during training. This is the moment where CSV data becomes training-ready tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One training batch:\n",
            "============================================================\n",
            "Batch features shape: torch.Size([32, 12])\n",
            "  → 32 examples in this batch\n",
            "  → 12 features per example\n",
            "\n",
            "Batch targets shape: torch.Size([32])\n",
            "  → 32 labels (one per example)\n",
            "\n",
            "First 3 examples in batch:\n",
            "Features (first 5 columns):\n",
            "tensor([[-1.5325, -0.5574,  1.6052,  1.1593,  2.2048],\n",
            "        [ 0.2058, -0.5574,  1.2393, -1.4633, -0.4234],\n",
            "        [ 0.5055, -0.5574,  0.4668, -0.3498,  0.0672]])\n",
            "\n",
            "Targets:\n",
            "tensor([0., 0., 0.])\n",
            "\n",
            "Batch composition:\n",
            "  Churned: 8 (25.0%)\n",
            "  Stayed: 24 (75.0%)\n"
          ]
        }
      ],
      "source": [
        "# Get one batch from the training loader\n",
        "batch_features, batch_targets = next(iter(train_loader))\n",
        "\n",
        "print(\"One training batch:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Batch features shape: {batch_features.shape}\")\n",
        "print(f\"  → {batch_features.shape[0]} examples in this batch\")\n",
        "print(f\"  → {batch_features.shape[1]} features per example\")\n",
        "print(f\"\\nBatch targets shape: {batch_targets.shape}\")\n",
        "print(f\"  → {batch_targets.shape[0]} labels (one per example)\")\n",
        "\n",
        "print(\"\\nFirst 3 examples in batch:\")\n",
        "print(f\"Features (first 5 columns):\")\n",
        "print(batch_features[:3, :5])\n",
        "print(f\"\\nTargets:\")\n",
        "print(batch_targets[:3])\n",
        "\n",
        "# Show target distribution in this batch\n",
        "churned_in_batch = (batch_targets == 1).sum().item()\n",
        "stayed_in_batch = (batch_targets == 0).sum().item()\n",
        "print(f\"\\nBatch composition:\")\n",
        "print(f\"  Churned: {churned_in_batch} ({churned_in_batch/len(batch_targets)*100:.1f}%)\")\n",
        "print(f\"  Stayed: {stayed_in_batch} ({stayed_in_batch/len(batch_targets)*100:.1f}%)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Iterating through DataLoaders**: We used `next(iter(train_loader))` to manually grab one batch for inspection. During actual training, you'd simply loop: `for batch_features, batch_targets in train_loader:` and the DataLoader automatically handles batching, shuffling, and resetting after each epoch.\n",
        ">\n",
        "><details><summary><b>Is this batch composition (25% churned, 75% stayed) guaranteed by shuffling?</b></summary>\n",
        "> \n",
        "> This batch happens to be close to our overall dataset distribution (~26.5% churned, ~73.5% stayed). Shuffling randomizes the order, so batches are generally representative, though individual batches will vary naturally.\n",
        "> \n",
        "> *Without shuffling*, we might see all churned customers clustered together in consecutive batches, causing the model to learn unstable, order-based patterns rather than true churn indicators.\n",
        "> </details>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualize the complete pipeline\n",
        "\n",
        "Let's trace one example all the way through our pipeline to see every transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRACKING ONE CUSTOMER THROUGH THE PIPELINE\n",
            "============================================================\n",
            "\n",
            "[Stage 1] Original CSV data:\n",
            "  Features: {'Age': 27, 'Avg Monthly GB Download': 0, 'Avg Monthly Long Distance Charges': 43.5, 'Churn Category': None, 'Churn Reason': None, 'Churn Score': 34, 'City': 'Thousand Palms', 'CLTV': 3854, 'Contract': 'Month-to-Month', 'Country': 'United States', 'Customer ID': '3061-BCKYI', 'Customer Status': 'Stayed', 'Dependents': 0, 'Device Protection Plan': 0, 'Gender': 'Male', 'Internet Service': 0, 'Internet Type': None, 'Lat Long': '33.849263, -116.382778', 'Latitude': 33.849263, 'Longitude': -116.382778, 'Married': 0, 'Monthly Charge': 19.9, 'Multiple Lines': 0, 'Number of Dependents': 0, 'Number of Referrals': 0, 'Offer': 'Offer D', 'Online Backup': 0, 'Online Security': 0, 'Paperless Billing': 0, 'Partner': 0, 'Payment Method': 'Credit Card', 'Phone Service': 1, 'Population': 6242, 'Premium Tech Support': 0, 'Quarter': 'Q3', 'Referred a Friend': 0, 'Satisfaction Score': 3, 'Senior Citizen': 0, 'State': 'California', 'Streaming Movies': 0, 'Streaming Music': 0, 'Streaming TV': 0, 'Tenure in Months': 14, 'Total Charges': 283.75, 'Total Extra Data Charges': 0, 'Total Long Distance Charges': 609.0, 'Total Refunds': 0.0, 'Total Revenue': 892.75, 'Under 30': 1, 'Unlimited Data': 0, 'Zip Code': '92276'}\n",
            "  Target: Churn = 0\n",
            "\n",
            "[Stage 2] After selecting 10 key features:\n",
            "  Features: {'Age': 27, 'Dependents': 0, 'Tenure in Months': 14, 'Contract': 'Month-to-Month', 'Internet Type': 'None', 'Monthly Charge': 19.9, 'Total Charges': 283.75, 'Satisfaction Score': 3, 'Number of Referrals': 0}\n",
            "  Kept: 10 columns (9 features + 1 target)\n",
            "  Dropped: 39 less relevant features\n",
            "\n",
            "[Stage 3] After handling missing values:\n",
            "  Internet Type: 'None' (filled if was null)\n",
            "\n",
            "[Stage 4] After one-hot encoding:\n",
            "  Shape changed: 10 → 13 features\n",
            "  Categorical features became: 3 binary columns\n",
            "\n",
            "[Stage 5] After splitting: Ended up in Training set\n",
            "\n",
            "[Stage 6] After standardization (first 5 features):\n",
            "  Before: [np.int64(27) np.int64(0) np.int64(14) np.float64(19.9) np.float64(283.75)]\n",
            "  After:  [-1.17280843 -0.55742887 -0.75298782 -1.48320483 -0.88412525]\n",
            "  → Values now centered around 0 with unit variance\n",
            "\n",
            "[Stage 7] As PyTorch tensor:\n",
            "  dtype: float32\n",
            "  shape: (13,) - ready for neural network input layer\n",
            "\n",
            "[Stage 8] Paired in TensorDataset:\n",
            "  Features and target kept synchronized\n",
            "  Can access as: dataset[844]\n",
            "\n",
            "[Stage 9] In a DataLoader batch:\n",
            "  Will be stacked with 31 other examples\n",
            "  Batch shape: (32, 13)\n",
            "  → First dimension is batch size, second is features\n",
            "\n",
            "============================================================\n",
            "✓ Complete transformation: CSV → Select → Clean → Encode → Split → Standardize → Tensor → Dataset → Batch\n"
          ]
        }
      ],
      "source": [
        "# Pick one customer from the original data\n",
        "example_idx = 42\n",
        "original_customer_full = pd.DataFrame(dataset).iloc[example_idx]\n",
        "original_customer = df.iloc[example_idx]\n",
        "\n",
        "print(\"TRACKING ONE CUSTOMER THROUGH THE PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Stage 1: Original data (from Step 2: Load)\n",
        "print(\"\\n[Stage 1] Original CSV data:\")\n",
        "print(f\"  Features: {original_customer_full.drop('Churn').to_dict()}\")\n",
        "print(f\"  Target: Churn = {original_customer_full['Churn']}\")\n",
        "\n",
        "# Stage 2: After feature selection (from Step 3: Select Features)\n",
        "print(\"\\n[Stage 2] After selecting 10 key features:\")\n",
        "print(f\"  Features: {original_customer.drop('Churn').to_dict()}\")\n",
        "print(f\"  Kept: {len(original_customer)} columns (9 features + 1 target)\")\n",
        "print(f\"  Dropped: {49 - len(original_customer)} less relevant features\")\n",
        "\n",
        "# Stage 3: After handling missing values (from Step 4: Handle Missing)\n",
        "print(\"\\n[Stage 3] After handling missing values:\")\n",
        "print(f\"  Internet Type: '{original_customer['Internet Type']}' (filled if was null)\")\n",
        "\n",
        "# Stage 4: After encoding (from Step 5: Encode Categories)\n",
        "encoded_customer = df_encoded.iloc[example_idx]\n",
        "print(\"\\n[Stage 4] After one-hot encoding:\")\n",
        "print(f\"  Shape changed: {len(original_customer)} → {len(encoded_customer)} features\")\n",
        "print(f\"  Categorical features became: {len(encoded_customer) - len(original_customer)} binary columns\")\n",
        "\n",
        "# Stage 5: After splitting (from Step 6: Split)\n",
        "# Check if it's in training or validation set\n",
        "if example_idx in X_train.index:\n",
        "    split_name = \"Training\"\n",
        "    position_in_split = X_train.index.get_loc(example_idx)\n",
        "    scaled_features = X_train_scaled[position_in_split]\n",
        "else:\n",
        "    split_name = \"Validation\"\n",
        "    position_in_split = X_val.index.get_loc(example_idx)\n",
        "    scaled_features = X_val_scaled[position_in_split]\n",
        "\n",
        "print(f\"\\n[Stage 5] After splitting: Ended up in {split_name} set\")\n",
        "\n",
        "# Stage 6: After standardization (from Step 7: Standardize)\n",
        "print(f\"\\n[Stage 6] After standardization (first 5 features):\")\n",
        "print(f\"  Before: {encoded_customer.drop('Churn').values[:5]}\")\n",
        "print(f\"  After:  {scaled_features[:5]}\")\n",
        "print(f\"  → Values now centered around 0 with unit variance\")\n",
        "\n",
        "# Stage 7: As a tensor (from Step 8: Convert to Tensors)\n",
        "print(f\"\\n[Stage 7] As PyTorch tensor:\")\n",
        "print(f\"  dtype: float32\")\n",
        "print(f\"  shape: (13,) - ready for neural network input layer\")\n",
        "\n",
        "# Stage 8: In TensorDataset (from Step 9: Create TensorDataset)\n",
        "print(f\"\\n[Stage 8] Paired in TensorDataset:\")\n",
        "print(f\"  Features and target kept synchronized\")\n",
        "print(f\"  Can access as: dataset[{position_in_split}]\")\n",
        "\n",
        "# Stage 9: In a batch (from Step 10: Build DataLoaders)\n",
        "print(f\"\\n[Stage 9] In a DataLoader batch:\")\n",
        "print(f\"  Will be stacked with 31 other examples\")\n",
        "print(f\"  Batch shape: (32, 13)\")\n",
        "print(f\"  → First dimension is batch size, second is features\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Complete transformation: CSV → Select → Clean → Encode → Split → Standardize → Tensor → Dataset → Batch\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why each step matters**: Without encoding, the network couldn't process \"Fiber Optic\" as input. Without splitting first, our validation metrics would be unreliable. Without standardization, gradient descent would converge slowly and erratically. Without batching, training would be impossibly inefficient. \n",
        "> \n",
        "> Each transformation solves a specific obstacle between \"data exists\" and \"model can learn\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've built a complete data pipeline from raw CSV to training-ready batches. This workflow—load, clean, split, standardize, convert, batch—is the foundation of every deep learning project.\n",
        "\n",
        "**What you've learned:**\n",
        "- [x] **The complete data pipeline** - Transform raw CSV files into training-ready batches through a systematic workflow: load → clean → split → scale → convert → batch\n",
        "- [x] **The golden rule in action** - Always split before preprocessing to prevent information leakage; fit transformations on training data only, then apply to validation\n",
        "- [x] **Why preprocessing matters** - Handling missing values, encoding categories, and standardizing features aren't optional steps—they determine whether gradient descent can learn efficiently\n",
        "- [x] **The bridge to PyTorch** - TensorDataset pairs features with targets, DataLoader handles batching and shuffling, and together they transform static data into flowing training examples\n",
        "\n",
        "> **Key insight**: The data pipeline isn't just a preprocessing step; it's the foundation that determines whether your model can learn at all. Get the dtypes wrong, leak information from test to train, or skip standardization, and even the best model architecture will struggle. Build a solid pipeline like this one, and you've solved half the problem before training even begins.\n",
        "\n",
        "##### Next steps:\n",
        "\n",
        "- [ ] **What if your data has outliers?** Some customers might have extreme values (e.g., 10 years tenure, $10,000 monthly charges). How would you detect and handle these outliers before they distort your scaling?\n",
        "- [ ] **How do you handle temporal data?** If your dataset has a time component (signup date, usage history), how would you create time-based features or ensure your train/val split respects chronological order?\n",
        "- [ ] **What happens with new categorical values?** If a new customer has a contract type you've never seen, how does your one-hot encoding handle it? Should you add an \"unknown\" category?\n",
        "- [ ] **What about class imbalance strategies?** With 73% stayed vs 27% churned, should you use techniques like oversampling, undersampling, or class weights during training?\n",
        "- [ ] **Can you build a reusable preprocessing function?** Wrap this entire pipeline into a function that takes raw data and returns DataLoaders, making it easy to preprocess any similar dataset (especially during model inference!)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
