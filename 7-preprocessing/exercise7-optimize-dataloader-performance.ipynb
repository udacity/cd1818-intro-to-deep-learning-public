{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 7: Optimize the Performance of Your DataLoaders\n",
        "\n",
        "Building a data pipeline is one thing; making it efficient is another. Time to explore how different preprocessing methods affect your input values, then experiment with shuffling, batching, and parallel loading to measure performance.\n",
        "\n",
        "> **Overview**: Discover how preprocessing choices and DataLoader settings impact training efficiency. Compare StandardScaler vs. MinMaxScaler, experiment with batch sizes and shuffling, and measure the performance improvements from advanced optimizations.\n",
        "> \n",
        "> **Scenario**: You've built a churn prediction model that works well in testing. Now it needs to process customer updates efficiently each night for production deployment. Your goal: Optimize the data loading to handle real-world scale.\n",
        "> \n",
        "> **Goal**: Build intuition for how preprocessing and DataLoader settings affect both data quality and training speed, then make informed decisions about which settings to use for different scenarios.\n",
        "> \n",
        "> **Tools**: Python, PyTorch, Pandas, scikit-learn\n",
        "> \n",
        "> **Estimated Time**: 15-20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Let's import our libraries and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and split the data\n",
        "\n",
        "> Note: This step mirrors the exact same dataset and dataset processing (up to step 5) as for [demo 7](/cd1818-intro-to-deep-learning/7-preprocessing/demo7-turning-raw-data-into-trainable-tensors.ipynb).\n",
        "\n",
        "We'll use the [aai510-group1/telco-customer-churn](https://huggingface.co/datasets/aai510-group1/telco-customer-churn) dataset from Hugging Face, which contains 4,000+ real customer records from a telecommunications company.\n",
        "\n",
        "The full dataset has 49 features, but we'll focus on 10 key predictors that are most relevant for understanding customer churn.\n",
        "\n",
        "> **The preprocessing workflow ahead:**\n",
        "> \n",
        "> Here's a compact view of the full pipeline we'll build:\n",
        "> \n",
        "> 1. **Load data** → Convert Hugging Face dataset to Pandas DataFrame\n",
        "> 2. **Create subset** → Select only the 10 features we are interested into\n",
        "> 3. **Handle missing values** → Fill Internet Type nulls with \"None\"\n",
        "> 4. **Encode categories** → Convert Contract and Internet Type to numbers\n",
        "> 5. **Split data** → Separate into train (70%) and validation (30%) sets\n",
        "> \n",
        "> Each step prepares the data for the next, transforming messy CSV into training-ready batches.\n",
        "\n",
        "**IMPORTANT: Feel free to skip if you've gone through the demo**. Just know we're loading the telco-customer-churn dataset (4,000+ customers), handling missing values, encoding categories, and splitting into train/val sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load dataset\n",
        "dataset = load_dataset('aai510-group1/telco-customer-churn', split='train')\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# 2. Select features\n",
        "feature_columns = [\n",
        "    'Age', 'Dependents', 'Tenure in Months', 'Contract', 'Internet Type',\n",
        "    'Monthly Charge', 'Total Charges', 'Satisfaction Score', 'Number of Referrals'\n",
        "]\n",
        "target_column = 'Churn'\n",
        "df = df[feature_columns + [target_column]]\n",
        "\n",
        "# 3. Handle missing values\n",
        "df['Internet Type'] = df['Internet Type'].fillna('None')\n",
        "\n",
        "# 4. Encode categorical features\n",
        "df_encoded = pd.get_dummies(df, columns=['Contract', 'Internet Type'], drop_first=True)\n",
        "\n",
        "# 5. Split data\n",
        "X = df_encoded.drop('Churn', axis=1)\n",
        "y = df_encoded['Churn']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"✓ Data loaded and preprocessed\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"  Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"  Features: {X_train.shape[1]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Baseline established**: With clean, preprocessed data in hand, you're ready to experiment with different scaling methods and DataLoader settings to optimize pipeline performance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Experiment with different scalers\n",
        "\n",
        "\n",
        "You'll create a reusable function for building data pipelines, then use it with two different scalers (StandardScaler and MinMaxScaler) to compare how they affect your input values.\n",
        "\n",
        "This modular approach lets you easily experiment with different preprocessing methods while keeping the rest of your pipeline consistent."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1: Build a reusable scaling pipeline\n",
        "\n",
        "Create a function that handles the complete transformation: scaling → tensors → TensorDataset → DataLoader. This function will work with any sklearn scaler, letting you easily swap between StandardScaler, MinMaxScaler, or others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the reusable scaling pipeline\n",
        "\n",
        "def create_scaled_pipeline(X_train, X_val, y_train, y_val, scaler, batch_size=32):\n",
        "    \"\"\"\n",
        "    Create a DataLoader pipeline with scaled features.\n",
        "    \n",
        "    Hint: This function works with any sklearn scaler (StandardScaler, MinMaxScaler, etc.).\n",
        "    The scaler should already be initialized before calling this function.\n",
        "\n",
        "    IMPORTANT: Keep shuffling to False for both sets here as we'll experiment with shuffling in a later step!\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_val: Feature matrices\n",
        "        y_train, y_val: Target arrays\n",
        "        scaler: An sklearn scaler object (e.g., StandardScaler(), MinMaxScaler())\n",
        "        batch_size: Number of samples per batch\n",
        "    \n",
        "    Returns:\n",
        "        train_loader: DataLoader for training set\n",
        "        val_loader: DataLoader for validation set\n",
        "    \"\"\"\n",
        "    # TODO: Fit the scaler on training data and transform both sets\n",
        "    # Hint: Remember the golden rule of deep learning!\n",
        "    # Reference: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "    X_train_scaled =  # Add your code here\n",
        "    X_val_scaled =  # Add your code here\n",
        "    \n",
        "    # TODO: Convert NumPy arrays to PyTorch tensors\n",
        "    # Hint: What dtype is expected by default?\n",
        "    # Reference: https://pytorch.org/docs/stable/tensors.html\n",
        "    X_train_tensor =  # Add your code here\n",
        "    X_val_tensor =  # Add your code here\n",
        "    y_train_tensor =  # Add your code here\n",
        "    y_val_tensor =  # Add your code here\n",
        "    \n",
        "    # TODO: Create TensorDatasets to pair features with targets\n",
        "    # Hint: TensorDataset wraps tensors so features and targets stay synchronized.\n",
        "    # Reference: https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n",
        "    train_dataset =  # Add your code here\n",
        "    val_dataset =  # Add your code here\n",
        "    \n",
        "    # TODO: Create DataLoaders\n",
        "    # Hint: Use the batch_size and shuffle parameters passed to the function\n",
        "    # Reference: https://docs.pytorch.org/docs/stable/data.html\n",
        "    train_loader =  # Add your code here\n",
        "    val_loader =  # Add your code here\n",
        "    \n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2: Create baseline pipeline with StandardScaler\n",
        "\n",
        "Let's create a baseline DataLoader using StandardScaler and measure its performance. You'll implement the standardization step and create the DataLoader with basic settings.\n",
        "\n",
        "This baseline will serve as your reference point for all optimizations in the following steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Standardize the features using StandardScaler\n",
        "\n",
        "# TODO: Initialize StandardScaler\n",
        "# Hint: Use StandardScaler from sklearn.preprocessing.\n",
        "# Reference: https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
        "standard_scaler =  # Add your code here\n",
        "\n",
        "# Create StandardScaler pipeline\n",
        "baseline_train_loader, baseline_val_loader = create_scaled_pipeline(\n",
        "    X_train, X_val, y_train, y_val, \n",
        "    scaler=standard_scaler,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(\"✓ StandardScaler pipeline created!\")\n",
        "print(f\"  Batch size: 32\")\n",
        "print(f\"  Number of training batches: {len(baseline_train_loader)}\")\n",
        "print(f\"  Number of validation batches: {len(baseline_val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Inspect baseline data values in a batch to understand what StandardScaler produces and why it matters for training.\n",
        "\n",
        "# TODO: Get one batch from the baseline DataLoader\n",
        "# Think about how to get the first item from an iterator.\n",
        "# Reference: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "batch_features, batch_targets =  # Add your code here\n",
        "\n",
        "# Examine the value range for the batch\n",
        "print(\"StandardScaler feature statistics:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Mean across batch: {batch_features.mean(dim=0)[:5]}\")\n",
        "print(f\"Std across batch:  {batch_features.std(dim=0)[:5]}\")\n",
        "print(f\"Min value in batch: {batch_features.min().item():.3f}\")\n",
        "print(f\"Max value in batch: {batch_features.max().item():.3f}\")\n",
        "print(f\"\\nFirst example (first 5 features): {batch_features[0, :5]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Examining the output**: Look at the mean and std values across the batch. \n",
        "> - *What range do most values fall into?* Compare this to the original feature scales you saw in the demo (Age: 20-80, Total Charges: 100-8,000). \n",
        "> - *How does StandardScaler change the distribution?* Keep these patterns in mind when you compare to MinMaxScaler in Step 4."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3: Create alternative pipeline with MinMaxScaler\n",
        "\n",
        "Now create a second pipeline using MinMaxScaler instead. Compare the value distributions to see how different scaling methods affect your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Standardize the features using MinMaxScaler\n",
        "\n",
        "# TODO: Initialize MinMaxScaler\n",
        "# Hint: Use MinMaxScaler from sklearn.preprocessing.\n",
        "# Reference: https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
        "minmax_scaler =  # Add your code here\n",
        "\n",
        "# Create MinMaxScaler pipeline\n",
        "normalized_train_loader, normalized_val_loader = create_scaled_pipeline(\n",
        "    X_train, X_val, y_train, y_val, \n",
        "    scaler=minmax_scaler,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(\"✓ MinMaxScaler pipeline created!\")\n",
        "print(f\"  Batch size: 32\")\n",
        "print(f\"  Number of training batches: {len(normalized_train_loader)}\")\n",
        "print(f\"  Number of validation batches: {len(normalized_val_loader)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Same number of batches**: Notice that regardless of scaler choice, you still get 93 training batches and 40 validation batches. The scaler only affects the VALUES in your tensors, not the structure of your DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Examine how MinMaxScaler's output differs from StandardScaler and consider which might be more suitable for this dataset.\n",
        "\n",
        "# TODO: Get one batch from the mixmax DataLoader\n",
        "# Think about how to get the first item from an iterator.\n",
        "# Reference: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "norm_batch_features, norm_batch_targets =  # Add your code here\n",
        "\n",
        "print(\"Normalized (MinMaxScaler) feature statistics:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Mean across batch: {norm_batch_features.mean(dim=0)[:5]}\")\n",
        "print(f\"Std across batch:  {norm_batch_features.std(dim=0)[:5]}\")\n",
        "print(f\"Min value in batch: {norm_batch_features.min().item():.3f}\")\n",
        "print(f\"Max value in batch: {norm_batch_features.max().item():.3f}\")\n",
        "print(f\"\\nFirst example (first 5 features): {norm_batch_features[0, :5]}\")\n",
        "print(f\"Batch target distribution: {norm_batch_targets.sum().item()}/{len(norm_batch_targets)} churned\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparison:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"StandardScaler range: [{batch_features.min().item():.3f}, {batch_features.max().item():.3f}]\")\n",
        "print(f\"MinMaxScaler range:   [{norm_batch_features.min().item():.3f}, {norm_batch_features.max().item():.3f}]\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Compare the distributions**: Look at how the two scalers produce very different value ranges and distributions. Consider: \n",
        "> - Which range might help gradient descent learn more effectively? \n",
        "> - Does having both negative and positive values matter? \n",
        "> - How do the mean and std values differ between the two approaches?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  TODO - Analysis Question 1\n",
        "\n",
        "**Based on the value ranges you observed:**\n",
        "\n",
        "Which scaling method (StandardScaler or MinMaxScaler) produces more stable numeric ranges for training? Why might one be preferred over the other for this dataset? (2-3 sentences)\n",
        "\n",
        "_Write your answer here:_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Add shuffling and measure efficiency\n",
        "\n",
        "Now that you understand preprocessing choices, it's time to optimize for training. You'll extend your pipeline function to handle shuffling properly (for training and/or validation?), then measure how DataLoader settings affect loading speed."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1: Extend the pipeline to handle shuffling\n",
        "\n",
        "Modify your pipeline function to shuffle the right data set(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Extend the pipeline function to handle shuffling correctly\n",
        "\n",
        "def create_scaled_pipeline_extended(X_train, X_val, y_train, y_val, scaler, **dataloader_kwargs):\n",
        "    \"\"\"\n",
        "    Create a DataLoader pipeline with scaled features and proper shuffling.\n",
        "\n",
        "    We now use `dataloader_kwargs` to pass all DataLoader parameters (e.g., batch_size, drop_last, ...), giving full \n",
        "    flexibility without modifying the function signature.\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_val: Feature matrices\n",
        "        y_train, y_val: Target arrays\n",
        "        scaler: An sklearn scaler object (e.g., StandardScaler(), MinMaxScaler())\n",
        "        dataloader_kwargs: Extra arguments passed directly to DataLoader (including batch size)\n",
        "    \n",
        "    Returns:\n",
        "        train_loader: DataLoader for training set (shuffled)\n",
        "        val_loader: DataLoader for validation set (not shuffled)\n",
        "    \"\"\"\n",
        "    # TODO: Create the complete pipeline from (X_train, X_val) to (train_loader, val_loader)\n",
        "    # Hint: Feel free to reuse the code from `create_scaled_pipeline`, and just hardcode the right shuffling values for train and validation sets\n",
        "    # Reference: https://discuss.pytorch.org/t/how-does-shuffle-in-data-loader-work/49756\n",
        "\n",
        "    # Add your code here\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "print(\"✓ Extended pipeline function with shuffling created!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2: Create shuffled pipeline\n",
        "\n",
        "Use your updated function to create DataLoaders with shuffling. Compare the batch contents to see how shuffling changes which examples appear together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataLoader with correct shuffling enabled\n",
        "# Observe how shuffling affects the order of examples\n",
        "\n",
        "# TODO: Choose your preferred scaler\n",
        "shuffled_scaler =  # Add your code\n",
        "\n",
        "# Create DataLoaders with shuffling enabled\n",
        "shuffled_train_loader, shuffled_val_loader = create_scaled_pipeline_extended(\n",
        "    X_train, X_val, y_train, y_val,\n",
        "    scaler=shuffled_scaler,\n",
        "    batch_size=32\n",
        ")\n",
        "print(\"✓ Shuffled DataLoader created!\")\n",
        "\n",
        "# TODO: Compare first batch from baseline (no shuffle) vs. shuffled\n",
        "# HINT: Take the first batch from iterating from each loader\n",
        "# Reference: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "baseline_batch, _ =  # Add your code\n",
        "shuffled_batch, _ =  # Add your code\n",
        "\n",
        "print(\"\\nFirst example from baseline (no shuffle):\")\n",
        "print(baseline_batch[0, :5])\n",
        "print(\"\\nFirst example from shuffled loader:\")\n",
        "print(shuffled_batch[0, :5])\n",
        "print(\"\\nNotice how shuffling changes which examples appear first!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Observing the shuffle effect**: The feature values should look different because shuffling randomizes the order of examples. Each time you iterate through the shuffled loader, you'll see a different order. This prevents the model from learning patterns based on the sequence of examples rather than the actual features.\n",
        "> \n",
        "> Consider: What would happen if all churned customers appeared consecutively in your training data? How would that affect the model's learning?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO - Analysis Question 2\n",
        "\n",
        "**Explain your shuffling strategy in 2-3 sentences.**\n",
        "\n",
        "Think about what happens if the model sees all \"churned\" customers first, then all \"stayed\" customers. How would this affect learning vs. evaluation? (2-3 sentences)\n",
        "\n",
        "_Write your answer here:_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Experiment with batch sizes\n",
        "\n",
        "Now that you understand preprocessing and shuffling, let's explore how batch size affects training efficiency. You'll measure how different batch sizes impact iteration speed and understand the trade-offs between throughput and memory usage."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1: Check device availability\n",
        "\n",
        "First, let's check whether we're running on GPU or CPU. This affects which optimizations will be most beneficial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why device matters for DataLoader optimization**: GPU training benefits significantly from larger batch sizes (better parallelism) and pin_memory (faster CPU→GPU transfer).\n",
        "> On CPU, these optimizations have minimal or negative impact due to overhead. Your exact results will vary based on your hardware, but the principles remain the same!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2: Create benchmarking function\n",
        "\n",
        "Let's build a function that accurately measures DataLoader performance with proper warmup and averaging. This function will simulate realistic training by including a dummy model forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define helper to test batch sizes\n",
        "def test_batch_size(batch_size, loader, device=device, num_warmup=2, num_runs=3):\n",
        "    \"\"\"\n",
        "    Time multiple iterations through a DataLoader and return average.\n",
        "    Works with both CPU and GPU.\n",
        "    \n",
        "    Args:\n",
        "        batch_size: Batch size (for reporting)\n",
        "        loader: DataLoader to benchmark\n",
        "        device: torch.device for GPU/CPU\n",
        "        num_warmup: Number of warmup iterations (to initialize CUDA kernels)\n",
        "        num_runs: Number of timed runs to average\n",
        "    \n",
        "    Returns:\n",
        "        mean_time, std_time: Average and standard deviation of iteration times\n",
        "    \"\"\"\n",
        "    times = []\n",
        "\n",
        "    # Create a simple dummy model to simulate real training\n",
        "    dummy_model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(12, 64),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(64, 1)\n",
        "    ).to(device)\n",
        "\n",
        "    # Warmup runs (important for CUDA initialization, harmless on CPU)\n",
        "    for _ in range(num_warmup):\n",
        "        for batch_features, batch_targets in loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_targets = batch_targets.to(device)\n",
        "            _ = dummy_model(batch_features)  # Simulate forward pass\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
        "    \n",
        "    # Timed runs\n",
        "    for _ in range(num_runs):\n",
        "        start = time.time()\n",
        "        for batch_features, batch_targets in loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_targets = batch_targets.to(device)\n",
        "            _ = dummy_model(batch_features)  # Simulate forward pass\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "        iteration_time = time.time() - start\n",
        "        times.append(iteration_time)\n",
        "    \n",
        "    return np.mean(times), np.std(times)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **How the benchmark works:**\n",
        "> \n",
        "> This function measures realistic DataLoader performance by simulating actual training:\n",
        "> \n",
        "> 1. **Dummy model**: Creates a simple 2-layer neural network to simulate forward passes (without this, we'd only measure Python iteration overhead, not actual data processing)\n",
        "> 2. **Warmup runs**: First iterations are always slower due to CUDA kernel compilation and memory allocation—we run these separately to avoid skewing measurements\n",
        "> 3. **Multiple timed runs**: Averages 3 runs to reduce variance from system noise (background processes, OS scheduling)\n",
        "> 4. **GPU synchronization**: `torch.cuda.synchronize()` ensures we wait for GPU operations to complete before stopping the timer (GPU operations are asynchronous!)\n",
        "> \n",
        "> The result: accurate measurements of how fast your DataLoader can feed data to a training model, not just how fast Python can iterate."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3: Compare different batch sizes\n",
        "\n",
        "Test how different batch sizes _(small, medium, large)_ affect iteration speed and the number of batches. Measure the trade-offs between throughput and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoaders with different batch sizes and measure iteration time\n",
        "\n",
        "# TODO: Choose your batch sizes\n",
        "# HINT: Compare small, medium (32), and large batch sizes\n",
        "batch_sizes = []  # Add your choices here\n",
        "\n",
        "# Test different batch sizes\n",
        "results = {}\n",
        "print(\"Testing different batch sizes:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    # TODO: Create DataLoader with specified batch_size\n",
        "    # HINT: \n",
        "    # - Should you use StandardScaler or MinMaxScaler?\n",
        "    # - Should you use `create_scaled_pipeline` or `create_scaled_pipeline_extended`?\n",
        "    scaler =  # Add your code here\n",
        "    train_loader, val_loader =  # Add your code here\n",
        "\n",
        "    # Time both loaders\n",
        "    train_mean, train_std = test_batch_size(bs, train_loader, device)\n",
        "    val_mean, val_std = test_batch_size(bs, val_loader, device)\n",
        "    \n",
        "    results[bs] = {\n",
        "        'train_time': train_mean,\n",
        "        'train_std': train_std,\n",
        "        'val_time': val_mean,\n",
        "        'val_std': val_std,\n",
        "        'num_train_batches': len(train_loader),\n",
        "        'num_val_batches': len(val_loader),\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nBatch size {bs}:\")\n",
        "    print(f\"  Training batches: {results[bs]['num_train_batches']}\")\n",
        "    print(f\"  Training time: {results[bs]['train_time']:.4f} ± {results[bs]['train_std']:.4f} seconds\")\n",
        "    print(f\"  Time per batch: {results[bs]['train_time']/len(train_loader)*1000:.2f} ms/batch\")\n",
        "    print(f\"  Validation batches: {results[bs]['num_val_batches']}\")\n",
        "    print(f\"  Validation time: {results[bs]['val_time']:.4f} ± {results[bs]['val_std']:.4f} seconds\")\n",
        "    print(f\"  Time per batch: {results[bs]['val_time']/len(val_loader)*1000:.2f} ms/batch\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Reading the metrics**: \"Time per batch\" increases with batch size (because each batch has more work), but \"total time\" decreases (because fewer iterations). \n",
        "> \n",
        "> For production, we care about total time per epoch!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO - Analysis Question 3\n",
        "\n",
        "**Based on your batch size experiments, answer the following:**\n",
        "\n",
        "1. **Which batch size gave you the best performance (fastest total time)?** Look at the total time values and identify which batch size was fastest.\n",
        "\n",
        "2. **Why does larger batch size generally improve speed?** Think about the relationship between number of batches and Python iteration overhead.\n",
        "\n",
        "3. **What limits how large you can make the batch size?** Consider both hardware constraints and training considerations.\n",
        "\n",
        "_Write your answer here:_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Other DataLoaders settings for efficiency**: DataLoaders have additional settings that can speed up training on larger datasets. Especially important are:\n",
        "> - `num_workers`: How many CPU worker processes load data in parallel (0 = no parallelism).  \n",
        "> - `pin_memory`: When `True` and using a GPU, copies batches to the GPU faster.  \n",
        "> \n",
        "> These advanced optimizations are most beneficial when:\n",
        "> - Loading data from disk (images, videos, large files)\n",
        "> - Performing real-time data augmentation or preprocessing\n",
        "> - Working with large datasets (100K+ samples)\n",
        "> - GPU is waiting idle for data to arrive\n",
        "> \n",
        "> On small, pre-loaded datasets like ours, the multiprocessing overhead may exceed the benefits. But, for production models on large datasets, these settings can reduce training time by 30-50%!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Collect your thoughts\n",
        "\n",
        "Now that you've experimented with different preprocessing methods and DataLoader settings, let's synthesize what you've learned by answering these questions based on your experiments.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: Analysis Question 4\n",
        "\n",
        "**Which preprocessing method (StandardScaler vs. MinMaxScaler) would you choose for this dataset, and why?**\n",
        "\n",
        "_Write your answer here:_\n",
        "\n",
        "\n",
        "### TODO: Analysis Question 5\n",
        "\n",
        "**What's the most important lesson you learned about DataLoader optimization?**\n",
        "\n",
        "_Write your answer here:_\n",
        "\n",
        "### TODO: Analysis Question 6\n",
        "\n",
        "**If you had to train on a dataset 100x larger than this one, which three settings would you change first and why?**\n",
        "\n",
        "_Write your answer here:_\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've explored how preprocessing choices and DataLoader settings impact both data quality and training efficiency.\n",
        "\n",
        "**What you've accomplished:**\n",
        "\n",
        "- [x] **Compared preprocessing methods** - Explored how StandardScaler vs. MinMaxScaler affect value ranges and stability\n",
        "- [x] **Experimented with shuffling** - Discovered why training data needs shuffling but validation doesn't\n",
        "- [x] **Tested batch size trade-offs** - Measured how different batch sizes affect iteration speed and memory usage\n",
        "- [x] **Discovered advanced optimizations** - Discussed about num_workers and pin_memory for faster data loading\n",
        "\n",
        "**Critical insights:**\n",
        "\n",
        "- **Preprocessing matters for training stability**: StandardScaler's zero-centered distribution typically works better for gradient descent than MinMaxScaler's [0,1] range\n",
        "- **Shuffling prevents order-based learning**: Without shuffling, models learn spurious patterns based on data order rather than true features\n",
        "- **Batch size balances speed and memory**: Larger batches are faster per epoch but use more memory; smaller batches update more frequently but take longer\n",
        "- **Advanced optimizations scale with dataset size**: num_workers and pin_memory show minimal gains on small datasets but can dramatically speed up training on large-scale data\n",
        "\n",
        "Efficient data handling isn't about finding one \"best\" setting; it's about understanding the trade-offs and choosing configurations that match your specific constraints (dataset size, hardware, training time budget). Preprocessing, shuffling, batching, and loader optimizations interact to shape training stability and efficiency, giving you the tools to design data pipelines that scale.\n",
        "\n",
        "> **Next steps to explore**: Try training your model with different preprocessing choices, batch sizes, and optimizers, and test how your data pipeline holds up as you increase the dataset size."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
