{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi Layer Perceptron for Diabetes Risk Prediction\n",
    "\n",
    "Welcome to your Deep Learning project! In this notebook, you’ll take on the role of a data scientist building a neural network that predicts diabetes risk, helping healthcare providers prioritize patients for diagnostic testing.\n",
    "\n",
    "### What you'll build\n",
    "\n",
    "By completing this notebook, you will _(demonstrate ability to)_:\n",
    "\n",
    "- Design MLP architectures with appropriate depth, width, and activations\n",
    "- Preprocess data with proper splitting, normalization, and batching\n",
    "- Implement training loops with forward/backward passes and optimization\n",
    "- Apply evaluation metrics suited for specialized applications\n",
    "- Diagnose overfitting/underfitting using loss curves\n",
    "- Improve models systematically through hyperparameter tuning and regularization\n",
    "- Interpret performance in context with actionable recommendations\n",
    "\n",
    "**Dataset**: [CDC Diabetes Health Indicators](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) (balanced subset of 50,000 patients, 21 features)  \n",
    "**Estimated time**: 5-6 hours  \n",
    "\n",
    "> *Have more questions about the project?* Review the [README.md](../README.md) for full project context, setup instructions, and deliverables.\n",
    "\n",
    "### Ready to get started?\n",
    "\n",
    "This notebook is divided into 8 sections: \n",
    "\n",
    "```Setup → Data Loading → Data Preprocessing → Model Design → Model Training → Model Evaluation → Model Improvements → Conclusion```\n",
    "\n",
    "Follow them in sequence to complete your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Set up the environment\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "# This ensures that your results are consistent across runs\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "print(f\"Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why reproducibility matters**: Setting random seeds ensures that your results are consistent across different runs. This is critical for debugging and comparing different model configurations. \n",
    "> <br>In production systems, reproducibility helps with model versioning and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # Set cuDNN to deterministic mode for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Moving execution to device**: Transferring your model and tensors to the GPU can drastically improve performance.  \n",
    "> <br>Use `.to(device)` for models and tensors to ensure everything runs on the same hardware.  \n",
    "> <br>*Example:*  \n",
    "> ```model = model.to(device)```   \n",
    "> ```inputs = inputs.to(device)```  \n",
    "> <br>**Note**: Mixing CPU and GPU tensors in operations will raise errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load and explore the dataset\n",
    "\n",
    "Understanding your data is the first critical step in any deep learning project. In this step, you will load the dataset and perform exploratory data analysis to understand your dataset.\n",
    "\n",
    "The CDC diabetes dataset is provided as a CSV file in the [`data/`](../data/) directory. You'll use [pandas](https://pandas.pydata.org/) to load it into a DataFrame for analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the dataset\n",
    "\n",
    "The dataset contains health indicators from the CDC's Behavioral Risk Factor Surveillance System. Each row represents a patient with 21 health-related features and a binary diabetes diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "df = pd.read_csv('data/diabetes_data.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Display basic dataset information\n",
    "# HINT: You need to inspect the DataFrame's structure to understand what columns exist, \n",
    "#       their data types, and whether any values are missing. Pandas DataFrames have \n",
    "#       methods that provide a concise summary of this information in a single call.\n",
    "#       Think about what you'd need to know before preprocessing the data.\n",
    "# REFERENCE: https://pandas.pydata.org/docs/reference/frame.html\n",
    "# This will help you understand the structure of your data before preprocessing\n",
    "\n",
    "# ------- Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Understanding the features\n",
    "\n",
    "The dataset contains 21 health and lifestyle indicators. Let's examine what each feature represents and its clinical significance:\n",
    "\n",
    "| Feature | Description | Type | Clinical Significance |\n",
    "|---------|-------------|------|----------------------|\n",
    "| **Diabetes_binary** | Diabetes diagnosis | Binary (0/1) | **Target variable** |\n",
    "| HighBP | High blood pressure diagnosis | Binary | Strong risk factor |\n",
    "| HighChol | High cholesterol diagnosis | Binary | Cardiovascular risk |\n",
    "| CholCheck | Cholesterol check in past 5 years | Binary | Preventive care indicator |\n",
    "| BMI | Body Mass Index | Continuous | Key obesity indicator |\n",
    "| Smoker | Smoking history | Binary | Lifestyle risk factor |\n",
    "| Stroke | History of stroke | Binary | Cardiovascular complication |\n",
    "| HeartDiseaseorAttack | Coronary heart disease or MI | Binary | Cardiovascular complication |\n",
    "| PhysActivity | Physical activity in past 30 days | Binary | Protective factor |\n",
    "| Fruits | Fruit consumption (1+ per day) | Binary | Dietary indicator |\n",
    "| Veggies | Vegetable consumption (1+ per day) | Binary | Dietary indicator |\n",
    "| HvyAlcoholConsump | Heavy alcohol consumption | Binary | Lifestyle risk factor |\n",
    "| AnyHealthcare | Any healthcare coverage | Binary | Access to care |\n",
    "| NoDocbcCost | Could not see doctor due to cost | Binary | Healthcare barrier |\n",
    "| GenHlth | General health (1-5 scale) | Ordinal | Self-reported health status |\n",
    "| MentHlth | Mental health (days not good in past 30) | Continuous | Mental health indicator |\n",
    "| PhysHlth | Physical health (days not good in past 30) | Continuous | Physical health indicator |\n",
    "| DiffWalk | Difficulty walking or climbing stairs | Binary | Mobility indicator |\n",
    "| Sex | Biological sex | Binary | Demographic factor |\n",
    "| Age | Age category (1-13) | Ordinal | Strong demographic predictor |\n",
    "| Education | Education level (1-6) | Ordinal | Socioeconomic indicator |\n",
    "| Income | Income level (1-8) | Ordinal | Socioeconomic indicator |\n",
    "\n",
    "For a complete overview of the dataset, refer to the [`data/data_dictionary.md`](data/data_dictionary.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Feature dictionaries in the real-world**: In practice, you'll often create these yourself by interviewing domain experts. Always document: feature meaning, type, measurement method, and known limitations.\n",
    "> <br> Think of the feature dictionary as your model's \"instruction manual\"; without it, even brilliant ML work becomes unusable in production.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Perform exploratory data analysis (EDA)\n",
    "\n",
    "Understanding your data involves checking for missing values, reviewing feature distributions, understanding target variable distribution, and identifying patterns that will inform your modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Check for missing values in the dataset\n",
    "# HINT: Missing values can break model training. You need to check each column for null/NaN values.\n",
    "#       Pandas has methods to detect null values, and you can combine these with aggregation \n",
    "#       functions to count them. Consider chaining methods together.\n",
    "# REFERENCE: https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "# Missing values require special handling and can impact model performance\n",
    "\n",
    "missing_values =  # ------- Add your code here\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Handling missing values** Even though this dataset has no missing values, it's crucial to check for them in any data workflow since they can disrupt model training, bias results, or break computations. Common strategies to handle missing values include:\n",
    ">   - *Removal:* Drop rows or columns if the proportion of missing data is small.  \n",
    ">   - *Imputation:* Fill with mean, median, mode, or domain-specific defaults.  \n",
    ">   - *Model-based methods:* Predict missing values using other features.  \n",
    "> Always make sure to also analyze *why* data is missing: it can reveal data collection issues or hidden patterns!  \n",
    ">\n",
    "> *Reference:* [ Working with missing data in Pandas](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Analyze and visualize the target variable distribution\n",
    "# HINT: For classification tasks, you need to understand class balance. How many samples \n",
    "#       are in each class (0 = no diabetes, 1 = diabetes)? Pandas Series (single columns) \n",
    "#       have methods to count the frequency of each unique value. You can also calculate \n",
    "#       percentages by normalizing these counts. Use matplotlib to visualize your key findings.\n",
    "# REFERENCE: https://pandas.pydata.org/docs/reference/series.html\n",
    "# Understanding class balance is critical for choosing appropriate metrics\n",
    "\n",
    "target_distribution =  # ------- Add your code here\n",
    "target_percentage =  # ------- Add your code here\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(f\"No Diabetes (0): {target_distribution[0]:,} ({target_percentage[0]:.2f}%)\")\n",
    "print(f\"Diabetes (1): {target_distribution[1]:,} ({target_percentage[1]:.2f}%)\")\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "target_distribution.plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'])\n",
    "ax.set_xlabel('Diabetes Status', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(['No Diabetes', 'Diabetes'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding the balanced dataset:** You have noticed this dataset has a 50-50 split between diabetic and non-diabetic patients. The original CDC data actually has ~14% diabetes prevalence, reflecting real population statistics.\n",
    ">\n",
    "> Why did we balance it?\n",
    "> - *Simplifies learning:* Lets you focus on neural network fundamentals without juggling class imbalance complexity\n",
    "> - *Valid strategy:* Downsampling the majority class is common in production when you have enough minority samples (we have 7,000+ diabetic cases!)\n",
    "> - *Practical benefits:* Simpler code, faster training, often performs just as well\n",
    ">\n",
    "> Alternatives to deal with class imbalance involve upsampling and/or cost-sensitive learning.\n",
    ">\n",
    "> *Production note:* You'd typically train on balanced data but evaluate on the real distribution. Here we use balanced for both to keep things simple, but remember to test on realistic proportions before deployment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Examine basic statistical properties of the dataset\n",
    "# HINT: Statistical summaries (mean, std, min, max, quartiles) help you understand feature \n",
    "#       distributions and identify potential outliers. DataFrames have built-in methods that \n",
    "#       compute these statistics across all numerical columns at once.\n",
    "# REFERENCE: https://pandas.pydata.org/docs/reference/frame.html\n",
    "# Pay attention to the ranges of different features since this informs normalization needs\n",
    "\n",
    "# ------- Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding statistical summaries** Statistical summaries provide a quick snapshot of your dataset’s structure and variability.  \n",
    ">\n",
    "> - *Central tendency:* Metrics like mean and median reveal typical values but can be distorted by skewed data or outliers.  \n",
    "> - *Spread:* Standard deviation and quartiles highlight how dispersed the data is, where wide spreads may indicate inconsistent scales or high variability.  \n",
    "> - *Range and extremes:* Min/max values help spot potential data entry errors or outliers needing review.  \n",
    "> - *Feature type awareness:*  \n",
    ">   - Continuous features benefit from scale checks and normalization.  \n",
    ">   - Ordinal or categorical features should be interpreted by the distribution of categories, not numeric statistics.  \n",
    ">\n",
    "> *Why this matters:* Understanding the shape, spread, and nature of your data guides preprocessing decisions like scaling, encoding, and handling outliers for reliable modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of key continuous features\n",
    "continuous_features = ['BMI', 'MentHlth', 'PhysHlth', 'Age', 'GenHlth']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(continuous_features):\n",
    "    axes[idx].hist(df[feature], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(feature, fontsize=11)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Interpreting the distribution plots** These histograms reveal how each continuous feature is distributed:  \n",
    "> - *Shape:* Skewed or symmetric distributions indicate how values are spread.  \n",
    "> - *Spread:* Wide distributions suggest high variability; scaling will be needed.  \n",
    "> - *Peaks:* Multiple peaks can hint at subgroups or hidden patterns in the data.  \n",
    "> - *Outliers:* Extreme values can distort model learning; worth investigating.  \n",
    "> - *Continuity:* Features with few distinct values might behave more like categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Analyze correlation between features and the target variable\n",
    "# HINT: Correlation coefficients measure linear relationships between variables (-1 to +1).\n",
    "#       You need to compute pairwise correlations between all features, then focus on \n",
    "#       correlations between each feature and your target variable. DataFrames can compute \n",
    "#       correlation matrices, and you can select specific columns from the result.\n",
    "# REFERENCE: https://pandas.pydata.org/docs/reference/frame.html\n",
    "# Strong correlations indicate features that may be predictive of diabetes\n",
    "\n",
    "# Compute correlation with target\n",
    "correlations =  # ------- Add your code here\n",
    "\n",
    "print(\"Feature Correlations with Diabetes:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize top correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "correlations[1:].plot(kind='barh', ax=ax)  # Exclude self-correlation\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_title('Feature Correlations with Diabetes Status', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding correlations** Correlations show how features move together, not cause and effect.  \n",
    "> - *Positive* → variables increase together; *Negative* → one rises as the other falls.  \n",
    "> - Strong values (±1) imply tighter linear links; near 0 means weak or no linear relation.  \n",
    "> - Watch for *multicollinearity* and remember that *nonlinear patterns* won’t appear here.  \n",
    ">  \n",
    "> *IMPORTANT*: Use correlations to spot patterns, not to draw conclusions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Collect key observations from EDA\n",
    "\n",
    "Document 5-10 key observations from the exploratory data analysis. Consider: class balance, feature ranges, correlations, missing data, and data quality.\n",
    "\n",
    "*TODO 6: Write your observations as bullet points here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – Understanding the dataset</h4></summary>\n",
    "  \n",
    "  Before proceeding to preprocessing, ensure you understand:\n",
    "\n",
    "  - [ ] The target variable distribution and its implications for evaluation\n",
    "  - [ ] Which features show strong correlations with diabetes\n",
    "  - [ ] The need for feature scaling due to different value ranges\n",
    "  - [ ] The absence of missing values (simplifies preprocessing)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Preprocess the dataset\n",
    "\n",
    "Proper data preprocessing is essential for neural network training. Raw data often needs to be split, normalized, and batched before it can be used effectively. This section transforms your dataset into a format optimized for PyTorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Separate features and target\n",
    "\n",
    "Machine learning models require splitting your data into input features (X) and target labels (y). The features are what the model uses to make predictions, while the target is what you're trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: Separate features (X) and target variable (y)\n",
    "# HINT: Identify the column representing the diabetes outcome, and store it separately.\n",
    "#       The remaining columns will form your feature set.\n",
    "#       DataFrames allow you to exclude columns or select specific ones using built-in methods.\n",
    "# REFERENCE: https://pandas.pydata.org/docs/reference/frame.html\n",
    "# Features are used for predicting the target variable.\n",
    "\n",
    "X =  # ------- Add your code here\n",
    "y =  # ------- Add your code here\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create train/validation/test splits\n",
    "\n",
    "Following best practices, you'll split the data into three sets:\n",
    "\n",
    "- Training set (60%): Used to train the model (learn patterns, update weights)\n",
    "- Validation set (20%): Used during development to tune hyperparameters and monitor overfitting\n",
    "- Test set (20%): Final evaluation on completely unseen data (simulates real-world deployment)\n",
    "\n",
    "This is a best practice because separating validation from testing ensures that model tuning doesn’t “leak” information from your final evaluation, leading to more trustworthy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8: Create train/validation/test splits with stratification by populating the empty variables below\n",
    "# HINT: scikit-learn provides utilities for splitting data while preserving class proportions.\n",
    "#       You'll need to split in two stages: first separate training from temp, then split \n",
    "#       temp into validation and test. Each split should maintain the same diabetes/no-diabetes \n",
    "#       ratio as the original dataset. Look for a parameter that handles stratification.\n",
    "# REFERENCE https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n",
    "# While less critical with balanced data, stratification is still good practice and prevents any sampling bias.\n",
    "\n",
    "X_train = y_train = X_val = y_val = X_test = y_test = None\n",
    "\n",
    "# ------- Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify split sizes and class distributions\n",
    "print(\"Split Sizes:\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(f\"Training - Diabetes prevalence: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Validation - Diabetes prevalence: {y_val.mean()*100:.2f}%\")\n",
    "print(f\"Test - Diabetes prevalence: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – Validate the data splits</h4></summary>\n",
    "  \n",
    "  Before continuing, make sure your dataset splits meet the expected proportions:\n",
    "\n",
    "  - [ ] Training set contains approximately 60% of the data\n",
    "  - [ ] Validation and test sets each contain approximately 20% of the data\n",
    "  - [ ] Class presence is as expected (same ratio as for the full dataset) in all three splits\n",
    "\n",
    "  If they aren’t, revisit your splitting logic before moving forward.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalize features\n",
    "\n",
    "Neural networks perform better when input features are on similar scales. Standardization transforms features to have zero mean and unit variance, which helps gradients flow properly during training and speeds up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: Normalize features with a scaler, and populate the empty variables below with the scaled feature sets\n",
    "# HINT: Follow these general steps:\n",
    "#         1. Create a scaling object from a preprocessing library\n",
    "#         2. Fit it **only** on the training features to learn scaling parameters\n",
    "#         3. Apply the same transformation to validation and test features\n",
    "#       This maintains data consistency and prevents information leakage from validation or test sets.\n",
    "# REFERENCE: https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\n",
    "# Normalization helps gradients converge more efficiently and prevents bias toward features with larger scales.\n",
    "\n",
    "X_train_scaled = X_val_scaled = X_test_scaled = None\n",
    "\n",
    "# ------- Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Don't forget the golden rule!** Only fit the scaler on training data to prevent data leakage, then use that fitted scaler to transform all three splits.   \n",
    "> If you fit the scaler on validation or test data, their statistics (mean, standard deviation) would influence the training process. This is called *data leakage*, and leads to overly optimistic performance estimates. \n",
    "> <br><br>Think of it this way: In real deployment, you only have access to training data when building your model. The test set represents future, unseen entries that you should not have visibility on!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convert to PyTorch tensors\n",
    "\n",
    "PyTorch requires data to be in tensor format and batched for efficient GPU processing. DataLoaders handle batching, shuffling, and parallel data loading automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 10: Convert scaled NumPy arrays to PyTorch tensors by populating the missing variables below\n",
    "# HINT: Features should be floating-point tensors for \n",
    "#       neural network computations, while binary classification targets work with long tensors.\n",
    "#       PyTorch provides tensor creation functions for different data types.\n",
    "# REFERENCE: https://pytorch.org/docs/stable/torch.html#tensors\n",
    "# Why different tensor types? Cross-entropy loss expects Long (integer) labels for class indices\n",
    "\n",
    "X_train_tensor = X_val_tensor = X_test_tensor = None\n",
    "y_train_tensor = y_val_tensor = y_test_tensor = None\n",
    "\n",
    "# ------- Add your code here\n",
    "\n",
    "print(\"Conversion to PyTorch tensors completed!\")\n",
    "print(f\"\\nTraining features: {X_train_tensor.shape}, dtype: {X_train_tensor.dtype}\")\n",
    "print(f\"Training labels: {y_train_tensor.shape}, dtype: {y_train_tensor.dtype}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why convert to tensors?** Neural networks run on *tensors*, not NumPy arrays, because tensors can live on the GPU and support automatic differentiation.  \n",
    "> Converting ensures your data can be used efficiently during training and lets PyTorch handle gradients and fast parallel math.  \n",
    ">  \n",
    "> *IMPORTANT*: Avoid switching back and forth between NumPy and tensors: each conversion moves data between memory spaces and slows things down.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 11: Create DataLoaders for train, validation, and test sets\n",
    "# HINT: PyTorch provides utilities in torch.utils.data to wrap tensors into datasets and create batching iterators.\n",
    "#       You'll need to: (1) combine feature and label tensors into dataset objects, and \n",
    "#       (2) wrap datasets in loaders with appropriate batch sizes. Training data should be \n",
    "#       shuffled, but validation/test should not.\n",
    "# REFERENCE: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# Think about whether to apply difference DataLoader parameters for the three splits.\n",
    "\n",
    "train_loader = val_loader = test_loader = None\n",
    "\n",
    "# ------- Add your code here\n",
    "\n",
    "print(f\"Number of batches:\")\n",
    "print(f\"Training: {len(train_loader)} batches\")\n",
    "print(f\"Validation: {len(val_loader)} batches\")\n",
    "print(f\"Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DataLoader setup by examining one batch\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(\"Sample batch from training DataLoader:\")\n",
    "    print(f\"Features shape: {X_batch.shape}\")\n",
    "    print(f\"Labels shape: {y_batch.shape}\")\n",
    "    print(f\"\\nFeature sample (first 5 values): {X_batch[0, :5]}\")\n",
    "    print(f\"Label sample (first 10): {y_batch[:10]}\")\n",
    "    break  # Only examine first batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch processing efficiency**: DataLoaders enable efficient mini-batch gradient descent by automatically batching your data. This allows for more stable gradient estimates than single-sample updates (SGD) while being more memory-efficient than using the entire dataset at once (batch gradient descent). Shuffling the training data each epoch prevents the model from learning the order of examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – Validate the dataLoader output</h4></summary>\n",
    "  \n",
    "  Before training, confirm that your batches are correctly structured and preprocessed:\n",
    "  \n",
    "  - [ ] **Feature batch shape:** `(64, 21)` → 64 samples × 21 features  \n",
    "  - [ ] **Label batch shape:** `(64,)` → 64 labels  \n",
    "  - [ ] **Feature values:** normalized \n",
    "  - [ ] **Labels:** binary values (**0 or 1**)  \n",
    "  \n",
    "  If any shapes or values look off, revisit your preprocessing or batching steps before proceeding.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Design the model architecture\n",
    "\n",
    "Now it's time to design your neural network! You'll build a multi-layer perceptron (MLP): a feed-forward neural network that learns from health indicators to predict diabetes risk. The goal is to define an architecture that balances **expressiveness** (ability to learn complex patterns) and **efficiency** (training speed and generalization).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Design considerations\n",
    "\n",
    "Before implementing the model, consider these architectural decisions:\n",
    "\n",
    "**Input Layer**:\n",
    "- Size: Must match the number of features in the dataset (21 for our diabetes data)\n",
    "\n",
    "**Hidden Layers**:\n",
    "- **Depth**: Deeper networks can capture more complex patterns but increase the risk of overfitting and slow down training\n",
    "- **Width**: Choose enough neurons to represent useful feature interactions without overcomplicating the model  \n",
    "- **Activation**: Non-linear activations (like ReLU) allow the model to learn complex relationships beyond simple linear boundaries  \n",
    "\n",
    "**Output Layer**:\n",
    "- Size: Single neuron for binary classification (outputs probability of diabetes)\n",
    "- Activation: Sigmoid function squashes output to range [0, 1], representing probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implement the neural network\n",
    "\n",
    "You'll create a Multi-Layer Perceptron (MLP) using PyTorch's `nn.Module` class. This approach lets you define each layer and the forward pass explicitly, giving you full control over your model’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 12: Define your DiabetesClassifier neural network class\n",
    "# HINT: In PyTorch, models are defined as classes that inherit from nn.Module.\n",
    "#       Within the constructor, specify your linear layers and activations.\n",
    "#       Then implement the forward() method to describe how data flows through them.\n",
    "#       Remember: Add non-linear activations between layers to help the network learn complex patterns;\n",
    "#       also think about the loss function you'll use as it will inform your model output.\n",
    "# REFERENCE: https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# Define your architecture thoughtfully based on design considerations.\n",
    "\n",
    "class DiabetesClassifier(nn.Module):\n",
    "\n",
    "    # ------- Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Instantiate and inspect the model\n",
    "\n",
    "Once the architecture is defined, create an instance of your model and inspect its structure. Understanding your model's parameters and layers helps ensure it's built as intended and gives you insight into its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 13: Create an instance of your model, and set it to run on your device\n",
    "# HINT: Initialize your class just like any Python object. You can use the default\n",
    "#       hidden layer sizes or adjust them for experimentation.\n",
    "#       After instantiation, set the model on the device. Then, printing the model will display its structure.\n",
    "# REFERENCE: https://discuss.pytorch.org/t/how-to-print-a-model-after-load-it/9879/7\n",
    "\n",
    "model = # ------- Add your code here\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Display parameter breakdown by layer\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:20s} - Shape: {str(param.shape):20s} - Parameters: {param.numel():,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding model inspection** Inspecting your model helps confirm that the architecture matches your design intent, both in structure and in parameter count.  \n",
    ">  \n",
    "> - *Check layer flow:* Verify inputs and outputs connect as expected, especially when stacking layers or combining modules.  \n",
    "> - *Parameter awareness:* Knowing where parameters concentrate helps spot over- or under-parameterized designs early.  \n",
    "> - *Debugging aid:* If training behaves unexpectedly, inspection can reveal mismatched layer sizes, missing activations, or frozen parameters.  \n",
    "> - *Efficiency check:* Smaller models train faster but may underfit; larger ones capture complexity but risk overfitting.  \n",
    ">  \n",
    "> Regular inspection builds intuition about how architectural choices impact learning and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test the forward pass\n",
    "\n",
    "Before training, confirm that your model processes input data correctly. This step ensures that tensor shapes are compatible and that the output has the expected dimensions for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 14: Test the forward pass with a sample batch\n",
    "# HINT: Retrieve one batch from your DataLoader and pass it through the model, after moving it to device.\n",
    "#       Check that the output tensor’s shape matches expectations (one probability per sample).\n",
    "#       This step validates that your forward() method and layer dimensions are correct.\n",
    "# REFERENCE: Review \"forward propagation in PyTorch\" and \"DataLoader iteration examples\".\n",
    "# Expected output shape: (batch_size, 1)\n",
    "\n",
    "# Get a sample batch\n",
    "for X_batch, y_batch in train_loader:\n",
    "    # Move data to device\n",
    "    X_batch = X_batch.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output =  # ------- Add your code here\n",
    "    \n",
    "    print(\"Forward Pass Test:\")\n",
    "    print(f\"Input shape: {X_batch.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"\\nSample output (first 10):\")\n",
    "    print(output[:10].squeeze().detach().cpu().numpy())\n",
    "    print(f\"\\nActual labels (first 10):\")\n",
    "    print(y_batch[:10].numpy())\n",
    "    \n",
    "    break  # Only test with first batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Interpreting the forward pass test** If the model runs without errors and outputs a tensor with the expected shape, your architecture and data pipeline are aligned.  \n",
    "> Unexpected shapes, NaNs, or all-identical predictions can signal setup issues worth fixing before training.  \n",
    ">  \n",
    "> Passing this check means you’re ready to move on to loss computation and optimization.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – Verify Model Architecture</h4></summary>\n",
    "  \n",
    "  Before training, ensure your model is correctly defined:\n",
    "  \n",
    "  - [ ] **Model instantiated** without errors  \n",
    "  - [ ] **Architecture matches expectations**: Input size = # features, hidden layers = right complexity for task, output = supports binary classification  \n",
    "  - [ ] **Model moved to correct device** (GPU if available)  \n",
    "  \n",
    "  If anything looks incorrect, revisit your model definition before proceeding to training.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Train the model\n",
    "\n",
    "Training a neural network involves repeatedly cycling through the data, computing predictions, calculating loss, and updating weights through backpropagation. This section implements the complete training loop with proper validation monitoring."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  Define loss function and optimizer\n",
    "\n",
    "The loss function measures how wrong your model's predictions are, while the optimizer determines how to update weights to reduce this error.\n",
    "\n",
    "For binary classification with probabilistic outputs (values between 0 and 1), which loss function is most appropriate? Which optimizer is commonly recommended as a strong default for neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 15: Define loss function and optimizer\n",
    "# HINT: Choose a binary classification loss that measures the difference \n",
    "#       between predicted probabilities and true labels.\n",
    "#       Then, initialize an optimizer that updates model weights efficiently.\n",
    "#       Remember to pass model parameters to the optimizer and select a reasonable learning rate.\n",
    "# REFERENCE: https://docs.pytorch.org/docs/stable/nn.html#loss-functions, https://docs.pytorch.org/docs/stable/optim.html\n",
    "# Tip: Adaptive optimizers adjust learning rates automatically, which makes them reliable defaults.\n",
    "\n",
    "criterion =   # ------- Add your code here\n",
    "optimizer =   # ------- Add your code here\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"Number of parameter groups: {len(optimizer.param_groups)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Defining the training setup for binary classification:** For binary classification, you need a loss that compares predicted probabilities against binary targets.  \n",
    "> When choosing an optimizer, consider:  \n",
    "> - How *stable* you want learning to be (adaptive methods help when tuning is tricky).  \n",
    "> - How much *control* you need over learning rates or momentum.  \n",
    ">  \n",
    "> The “best” choice often depends on your data size, feature scale, and how smoothly the model learns; experiment and observe training behavior.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implement the training loop\n",
    "\n",
    "The training loop orchestrates the entire learning process: forward pass, loss computation, backward pass, and weight updates. Validation during training helps detect overfitting early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 16: Complete the training function by implementing the complete training loop\n",
    "# HINT: A complete training loop has two phases per epoch:\n",
    "#       \n",
    "#       TRAINING PHASE: For each epoch:\n",
    "#       - Iterate through training batches\n",
    "#       - Move batches to device\n",
    "#       - For each batch: clear old gradients → forward pass → compute loss → \n",
    "#         backward pass → update weights\n",
    "#       - Track average training loss and other relevant performance metrics\n",
    "#       \n",
    "#       VALIDATION PHASE:\n",
    "#       - Set model to evaluation mode (disables dropout, batchnorm updates)\n",
    "#       - Iterate through validation batches WITHOUT computing gradients\n",
    "#       - Calculate average validation loss and other relevant performance metrics\n",
    "#       \n",
    "#       The PyTorch training pattern involves specific method calls on the model, optimizer, \n",
    "#       and loss. \n",
    "#\n",
    "#       LOGGING:\n",
    "#       - Use the `print_every` argument to conditionally print progress (epoch index, avg train/val loss).\n",
    "#\n",
    "# REFERENCE: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                num_epochs: int = 100,\n",
    "                print_every: int = 10) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a PyTorch model with validation monitoring.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on (CPU or GPU)\n",
    "        num_epochs: Number of training epochs\n",
    "        print_every: Print progress every N epochs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (num_epochs, train_losses, val_losses) - the losses contain one value per epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists to track metrics over epochs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # ------- Add your code here\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    return num_epochs, train_losses, val_losses\n",
    "\n",
    "num_epochs, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Beware of gradient accumulation!** By default, PyTorch accumulates gradients across batches. Without resetting them, gradients from previous iterations are added to those from the current batch, which can lead to incorrect weight updates. A reset step is needed before each backward pass to prevent this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize training progress\n",
    "\n",
    "Loss curves are your primary debugging tool for neural networks. They reveal whether your model is learning properly, overfitting, or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss', linewidth=2, color='#3498db')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss', linewidth=2, color='#e74c3c')\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "plt.title('Training and Validation Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nLoss Statistics:\")\n",
    "print(f\"Initial Train Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Initial Val Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"\\nLoss Reduction:\")\n",
    "print(f\"Training: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}% decrease\")\n",
    "print(f\"Validation: {((val_losses[0] - val_losses[-1]) / val_losses[0] * 100):.1f}% decrease\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Interpret the loss curves\n",
    "\n",
    "Understanding loss curves is critical for diagnosing model performance. Here are the patterns to look for:\n",
    "\n",
    " Pattern | Loss Behavior | Interpretation | Possible Fixes |\n",
    "|----------|----------------|----------------|----------------|\n",
    "| **Healthy Training** | • Both training & validation loss decrease and converge<br>• Small gap between them | Model is learning generalizable patterns | — |\n",
    "| **Overfitting** | • Training loss keeps decreasing<br>• Validation loss plateaus or increases<br>• Large gap between curves | Model memorizes training data instead of generalizing | Add regularization (dropout, weight decay)<br>Reduce model complexity<br>Collect more data |\n",
    "| **Underfitting** | • Both losses remain high<br>• Little or no improvement over epochs | Model lacks capacity or training | Increase model complexity<br>Train longer<br>Tune learning rate<br>Improve input features |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnose your model's performance based on loss curves. Answer these questions by analyzing the plot above, either through code or markdown:\n",
    " 1. What pattern do you see? Does the plot show healthy training, overfitting, or underfitting?\n",
    " 2. How do you know? _Describe the behavior of the training loss line vs. the validation loss line. (e.g., \"Training loss is [decreasing/high] while validation loss is [decreasing/increasing/high]\")._\n",
    " 3. What does this imply? What is the model doing wrong (or right)? _(e.g., \"The model is memorizing the training data but failing to generalize...\")_\n",
    "\n",
    "*------- TODO 17: Add your answer here:*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – Assess Training Progressing</h4></summary>\n",
    "  \n",
    "  Before moving to evaluation, verify that training completed successfully:\n",
    "  \n",
    "  - [ ] **Training completed** without errors across all epochs  \n",
    "  - [ ] **Loss curves analyzed** and any overfitting or underfitting noted  \n",
    "  - [ ] **Model ready** for comprehensive evaluation on test set  \n",
    "  \n",
    "  If training seems problematic (e.g., losses not decreasing), don't worry about it now - that's what optimizations in step 6 will focus on improving!\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the model\n",
    "\n",
    "Training loss tells you how well the model fits training data, but comprehensive evaluation requires measuring performance on unseen test data using metrics relevant to your application. \n",
    "\n",
    "For medical screening, certain metrics matter more than others. For the baseline model, just check that performance is good-enough (significantly better than random guessing)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define the evaluation logic\n",
    "\n",
    "We'll create now a parameterized function that evaluates any model and returns comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 18: Implement a function to evaluate your trained model on new data\n",
    "# HINT: This function should test how well your model performs by:\n",
    "#       1. Setting the model to evaluation mode (to disable dropout, etc.)\n",
    "#       2. Looping through the evaluation DataLoader without tracking gradients\n",
    "#       3. Collecting predictions and true labels for each batch\n",
    "#       4. Converting these results to NumPy arrays for metric calculations\n",
    "#       5. Computing relevant performance metrics for the use case\n",
    "#       6. Returning all computed values in a single dictionary for easy analysis\n",
    "#       This function helps summarize how well your trained model generalizes to unseen data.\n",
    "# REFERENCE: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Remember: Our model outputs probabilities (0-1), threshold at 0.5 for binary classification\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   data_loader: DataLoader, \n",
    "                   device: torch.device,\n",
    "                   threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on a dataset and return comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        data_loader: DataLoader containing the evaluation dataset\n",
    "        device: Device to run evaluation on (CPU or GPU)\n",
    "        threshold: Decision threshold for classification (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - predictions: numpy array of predicted probabilities\n",
    "            - true_labels: numpy array of true labels\n",
    "            - pred_labels: numpy array of predicted binary labels\n",
    "            - metrics\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    results = []\n",
    "\n",
    "    # ------- Add your code here\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function created successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why accuracy isn't enough (even with balanced data):** While accuracy is now meaningful with our 50-50 split (unlike with imbalanced data where it'd be misleading), it still doesn't tell the whole story in medical applications.\n",
    "> \n",
    "> Consider: A model with 75% accuracy could have 90% recall but only 60% precision—great at catching diabetic patients but with many false alarms. Or it could have 90% precision but 60% recall—very accurate when it predicts diabetes, but missing many cases.\n",
    "> \n",
    "> *Bottom line:* Choose metrics that provide a clear view over this trade-off; this is critical in healthcare where different errors have different costs!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluate on test set\n",
    "\n",
    "Now we'll use our evaluation function to assess the baseline model's performance on the held-out test set. This gives us an unbiased estimate of real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 19: Evaluate the model on the test set using the evaluation function\n",
    "# HINT: Call evaluate_model() with your trained model and test_loader.\n",
    "#       Extract the metrics from the returned dictionary and display them.\n",
    "\n",
    "# ------- Add your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choosing the top metric:** Which is worse for your stakeholders: missing a diabetic patient or triggering an unnecessary test? This determines whether you prioritize recall or precision, and informs threshold selection (we'll use 0.5 as default, but you could adjust it)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize the confusion matrix\n",
    "\n",
    "The confusion matrix shows exactly where your model succeeds and fails, breaking down predictions into true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 20: Create and visualize the confusion matrix\n",
    "# HINT: Use sklearn to calculate and visualize the confusion matrix.\n",
    "#       This approach automatically labels axes (true vs. predicted) and provides a clean layout.\n",
    "#       Optionally, you can adjust color maps or figure size for better readability.\n",
    "# REFERENCE: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "\n",
    "disp =   # ------- Add your code here\n",
    "disp.plot(cmap='Blues', colorbar=True)\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Interpreting your confusion matrix** The confusion matrix makes the trade-offs between recall and precision visible. By examining the balance between false positives and false negatives, you can decide which matters more for your real-world goal; for example, catching every diabetic case even if it means more false alarms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Analyze ROC curve and threshold selection\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve helps you visualize the tradeoff between sensitivity (catching true cases) and specificity (avoiding false alarms) at different decision thresholds.\n",
    "\n",
    "*Why this matters:*\n",
    "- The default threshold is 0.5 (predict diabetes if probability > 0.5)\n",
    "- You might want a different threshold based on your priorities:\n",
    "  - Lower threshold (e.g., 0.3) → Higher recall, catch more diabetics, but more false alarms\n",
    "  - Higher threshold (e.g., 0.7) → Higher precision, fewer false alarms, but miss more cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 21: Generate and plot the ROC curve\n",
    "# HINT: The ROC curve shows how your model’s ability to distinguish between classes \n",
    "#       changes as you vary the classification threshold: the closer the curve hugs the top-left corner, \n",
    "#       the better the model separates positives and negatives. Then, the Area Under the Curve (AUC) quantifies \n",
    "#       this performance: higher AUC also means better separability. This as a rule-of-thumb:\n",
    "#            * 0.90–1.00 → Excellent discrimination\n",
    "#            * 0.80–0.89 → Good discrimination\n",
    "#            * 0.70–0.79 → Fair discrimination\n",
    "#            * Below 0.70 → Poor discrimination\n",
    "#       Use this plot and AUC value to reason about which threshold best aligns with your real-world trade-offs.\n",
    "# REFERENCE: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds =  # ------- Add your code here\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='#3498db', linewidth=2.5, label=f'Model ROC (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "# Mark the current operating point (threshold = 0.5)\n",
    "current_idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.scatter(fpr[current_idx], tpr[current_idx], color='red', s=150, zorder=5, \n",
    "            label=f'Current Threshold (0.5)\\nTPR={tpr[current_idx]:.3f}, FPR={fpr[current_idx]:.3f}')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curve - Evaluating Threshold Tradeoffs', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nROC Curve Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Interpretation:\")\n",
    "if roc_auc >= 0.90:\n",
    "    print(\"  - Excellent discrimination ability\")\n",
    "elif roc_auc >= 0.80:\n",
    "    print(\"  - Good discrimination ability\")\n",
    "elif roc_auc >= 0.70:\n",
    "    print(\"  - Fair discrimination ability\")\n",
    "else:\n",
    "    print(\"  - Poor discrimination ability\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding the ROC curve**: Each point on the ROC curve represents a different probability threshold for classification. Points in the upper-left corner represent high sensitivity (catching most cases) with low false positive rates (few false alarms). The diagonal line represents random guessing. Depending on your prioritization, you might adjust the threshold as follows:\n",
    "> - If missing positives is costly, lower the threshold to get the model to label more cases as \"positive\" → higher true positive rate, higher false positive rate.\n",
    "> - If false alarms are costly, raise the threshold to get the model to be more strict in labeling cases as \"positives\" → lower true positive rate, lower false positive rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Interpret your results\n",
    "\n",
    "Beyond raw metrics, it's essential to interpret your model's performance from a healthcare perspective. Let's analyze what these results mean for clinical deployment and patient care."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on your model's performance from a healthcare perspective. Answer these questions based on your evaluation results:\n",
    " 1. Which metric is most important for diabetes screening and why? _(Hint: Think about the cost of a False Negative vs. a False Positive)._\n",
    " 2. Would you recommend using this model in practice? Under what conditions?\n",
    " 3. What probability threshold would you choose instead of 0.5, and why?\n",
    "\n",
    "HINT: This model is quite simple since you haven't performed yet hyperparameter tuning, regularization, or feature engineering. Feel free to move forward if you achieve...\n",
    "\n",
    "*------- TODO 22: Add your answer here:*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – What to Keep in Mind About Your Model Performance </h4></summary>\n",
    "  \n",
    "  Before moving forward, record:\n",
    "  - **Current metrics**: Accuracy ___, Precision ___, Recall ___, F1 ___, ...\n",
    "  - **Training behavior**: Overfitting (train << val loss)? Underfitting (both high)?\n",
    "  - **Main issue**: Which needs fixing most—false positives, false negatives, or overall performance?\n",
    "  \n",
    "  These observations will guide your improvement strategy in Step 6!\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Improve and tune the model\n",
    "\n",
    "Based on your evaluation results, you will now systematically improve the model. This demonstrates the iterative nature of machine learning: evaluate, diagnose, improve, and repeat. \n",
    "\n",
    "You'll experiment with multiple techniques and track results to identify the most impactful improvements.\n",
    "\n",
    "**Your Goal**: Increase model performance against baseline by >=5% on your top metric.\n",
    "\n",
    "> **Feel free to change any training parameters: num_epochs, optimizer, ...**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Create experiment tracking system\n",
    "\n",
    "Before running experiments, let's create a system to automatically track and compare results. This eliminates manual result entry and makes it easy to identify the best-performing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment tracking dictionary\n",
    "experiment_results = {}\n",
    "\n",
    "def track_experiment(name: str, \n",
    "                     model: nn.Module,\n",
    "                     train_losses: List[float],\n",
    "                     val_losses: List[float],\n",
    "                     test_results: Dict[str, float],\n",
    "                     notes: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Track experiment results for later comparison.\n",
    "    \n",
    "    Args:\n",
    "        name: Experiment name\n",
    "        model: Trained model\n",
    "        train_losses: List of training losses per epoch\n",
    "        val_losses: List of validation losses per epoch\n",
    "        test_results: Dictionary from evaluate_model()\n",
    "        notes: Optional notes about the experiment\n",
    "    \"\"\"\n",
    "   # Keep only numeric metrics so we don't try to tabulate arrays, dicts, etc.\n",
    "    numeric_metrics = {\n",
    "        k: float(v) for k, v in test_results.items()\n",
    "        if isinstance(v, numbers.Number)\n",
    "    }\n",
    "\n",
    "    experiment_results[name] = {\n",
    "        'final_train_loss': float(train_losses[-1]),\n",
    "        'final_val_loss': float(val_losses[-1]),\n",
    "        'min_val_loss': float(min(val_losses)),\n",
    "        'loss_gap': float(abs(train_losses[-1] - val_losses[-1])),\n",
    "        'metrics': numeric_metrics,           # store all metrics here\n",
    "        'notes': notes,\n",
    "        'train_losses': train_losses,         # keep full histories if desired\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "    print(f\"✓ Experiment '{name}' tracked successfully!\")\n",
    "\n",
    "\n",
    "def display_experiment_comparison(sort_by: Optional[str] = \"f1\",\n",
    "                                  descending: bool = True) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Display a comparison table of all tracked experiments.\n",
    "\n",
    "    Args:\n",
    "        sort_by: Metric name to sort by (e.g., 'f1', 'roc_auc', 'accuracy').\n",
    "                 If None or not present, will fall back to:\n",
    "                   1) 'roc_auc' if present\n",
    "                   2) any available metric (alphabetical)\n",
    "        descending: Sort order for the chosen metric.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with experiment results.\n",
    "    \"\"\"\n",
    "    if not experiment_results:\n",
    "        print(\"No experiments tracked yet!\")\n",
    "        return None\n",
    "\n",
    "    # Collect the union of all metric names across experiments\n",
    "    all_metric_names = set()\n",
    "    for res in experiment_results.values():\n",
    "        all_metric_names.update(res.get('metrics', {}).keys())\n",
    "    all_metric_names = sorted(all_metric_names)  # stable ordering\n",
    "\n",
    "    # Determine sort metric\n",
    "    chosen_sort = None\n",
    "    if sort_by in all_metric_names:\n",
    "        chosen_sort = sort_by\n",
    "    elif all_metric_names:\n",
    "        chosen_sort = all_metric_names[0]  # fallback to first available\n",
    "    # If no metrics at all, we’ll sort by final_val_loss\n",
    "\n",
    "    # Build table rows\n",
    "    rows = []\n",
    "    for name, res in experiment_results.items():\n",
    "        row = {\n",
    "            'Experiment': name,\n",
    "            'Val Loss': f\"{res['final_val_loss']:.4f}\",\n",
    "            'Loss Gap': f\"{res['loss_gap']:.4f}\",\n",
    "        }\n",
    "        # Add metrics (formatted)\n",
    "        for m in all_metric_names:\n",
    "            val = res['metrics'].get(m, None)\n",
    "            row[m.upper() if m.islower() else m] = (f\"{val:.4f}\" if isinstance(val, numbers.Number) else \"\")\n",
    "        # Also keep raw for sorting\n",
    "        row['_sort_val'] = (\n",
    "            res['metrics'].get(chosen_sort)\n",
    "            if chosen_sort is not None else res['final_val_loss']\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Sort\n",
    "    if chosen_sort is not None:\n",
    "        df = df.sort_values('_sort_val', ascending=not descending)\n",
    "    else:\n",
    "        # No metrics available: sort by Val Loss ascending\n",
    "        df = df.sort_values('Val Loss', ascending=True, key=lambda s: s.astype(float))\n",
    "\n",
    "    # Clean up helper column\n",
    "    df = df.drop(columns=['_sort_val'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Track baseline experiment\n",
    "track_experiment(\n",
    "    name='Baseline',\n",
    "    model=model,\n",
    "    train_losses=train_losses,\n",
    "    val_losses=val_losses,\n",
    "    test_results=test_results,\n",
    "    notes='Initial model with default hyperparameters'\n",
    ")\n",
    "\n",
    "print(\"\\nExperiment tracking system initialized!\")\n",
    "print(\"Use track_experiment() after training each variation.\")\n",
    "print(\"Use display_experiment_comparison() to see all results.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Running a systematic experiment:** For each experiment, follow this workflow:\n",
    "> 1. **Define change**: What are you testing? (e.g., dropout=0.3, lr=0.0001)\n",
    "> 2. **Create model**: Create new model class (if architecture changes) and instantiate with new configuration, e.g., `DiabetesClassifierWithDropout(dropout_prob=0.3).to(device)`\n",
    "> 3. **Train**: Run training loop, track losses with `train_model()`\n",
    "> 4. **Evaluate**: Calculate test metrics (accuracy, precision, recall, F1) with `evaluate_model()`\n",
    "> 5. **Record**: Add results to experiment tracker dictionary with `track_experiment()`\n",
    "> 6. **Visualize**: Print key experiment metrics for quick analysis\n",
    "> \n",
    "> This systematic approach helps you understand what works and why!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Experiment 1: Add dropout regularization\n",
    "\n",
    "Dropout randomly deactivates neurons during training, forcing the network to learn robust features that don't rely on specific neurons. This reduces overfitting and improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 23: Create a model with dropout layers and train it\n",
    "# HINT: To reduce overfitting, insert dropout layers between your hidden layer activations. \n",
    "#       Think about where dropout would make the most impact — typically after nonlinear activations.\n",
    "#       Experiment with different dropout probabilities (e.g., around 0.3) to find a balance \n",
    "#       between regularization and model capacity.\n",
    "#       Use your existing training function to maintain consistency in training and evaluation.\n",
    "# REFERENCE: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "\n",
    "print(\"Experiment 1: Training model with Dropout\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------- Add your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How dropout works**: During training, dropout randomly sets a fraction of neuron activations to zero. This prevents the network from relying too heavily on any single neuron and encourages redundancy. During evaluation, dropout is automatically disabled, and all neurons contribute to predictions. This simple technique is remarkably effective at reducing overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Experiment 2: Tune learning rate\n",
    "\n",
    "The learning rate controls how large the weight updates are during training. Too high and training becomes unstable; too low and convergence is painfully slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 24: Experiment with different learning rates\n",
    "# HINT: Try at least 3 learning rates: one lower (0.0001), one higher (0.01), and baseline (0.001).\n",
    "#       Use the same model architecture for fair comparison.\n",
    "#       Track how quickly each converges and their final performance.\n",
    "# REFERENCE: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "print(\"Experiment 2: Learning Rate Tuning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------- Add your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choosing a learning rate:** The optimal learning rate depends on your model, dataset, and optimizer. A good starting point is often *1e-3*.\n",
    "> - If training diverges, lower lr (×0.1).\n",
    "> - If loss plateaus too early, raise lr (×2–10).\n",
    "> \n",
    "> *Tip*: Adam is generally more forgiving, while SGD benefits from careful tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Experiment 3: Adjust network architecture\n",
    "\n",
    "Network architecture (depth, width, and layer size) affects model capacity — its ability to learn complex patterns. Too simple and it underfits; too complex and it overfits (especially with limited data).\n",
    "\n",
    "> **Consider running this experiment multiple times with different architectures**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 25: Define a network architecture that best fits the experienced training behavior and performance\n",
    "# HINT: Think about two main design directions:\n",
    "#       - Make the model too simple, and the model underfits\n",
    "#       - Make the model too complex, and it overfits (especially with limited data)\n",
    "#       Compare how these choices influence parameter count, training behavior, and generalization.\n",
    "# REFERENCE: Course module on network architecture design\n",
    "\n",
    "print(\"Experiment 3: Training with tailored architecture\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------- Add your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Model capacity tradeoffs**: Larger models (more parameters) can learn more complex patterns but are more prone to overfitting, especially with limited data. For tabular data like ours, simpler architectures often perform just as well or better than deep networks. The key is finding the right balance for your dataset size and complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Compare all experiments\n",
    "\n",
    "Now let's synthesize findings from all improvement experiments to identify which techniques had the biggest impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive comparison of all experiments\n",
    "print(\"Comprehensive Experiment Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll experiments sorted by F1-Score (best to worst):\\n\")\n",
    "\n",
    "comparison_df = display_experiment_comparison()\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best configuration\n",
    "best_experiment = comparison_df.iloc[0]['Experiment']\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Best Configuration: {best_experiment}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(\"  - Review the 'Loss Gap' column to identify which techniques reduced overfitting\")\n",
    "print(\"  - Consider the tradeoff between Precision and Recall for your use case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss curves for top 3 experiments\n",
    "print(\"\\nVisualizing Loss Curves for Top 3 Experiments\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "top_3_experiments = comparison_df.head(3)['Experiment'].values\n",
    "\n",
    "for idx, exp_name in enumerate(top_3_experiments):\n",
    "    exp_data = experiment_results[exp_name]\n",
    "    epochs = range(1, len(exp_data['train_losses']) + 1)\n",
    "    \n",
    "    axes[idx].plot(epochs, exp_data['train_losses'], label='Train Loss', linewidth=2, color='#3498db')\n",
    "    axes[idx].plot(epochs, exp_data['val_losses'], label='Val Loss', linewidth=2, color='#e74c3c')\n",
    "    axes[idx].set_xlabel('Epoch', fontsize=11)\n",
    "    axes[idx].set_ylabel('Loss', fontsize=11)\n",
    "    axes[idx].set_title(f'{exp_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Brainstorming checkpoint**: Reflect on what you learned:\n",
    "> - Which technique provided the most improvement?\n",
    "> - Did any techniques hurt performance?\n",
    "> - How did different techniques address different issues (overfitting vs. underfitting)?\n",
    "> - What would you try next if you had more time?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6.5 Reflect on experiments and brainstorm additional improvements\n",
    "\n",
    "You've now systematically tested three improvement techniques. It's time to demonstrate your understanding by reflecting on results and proposing what to try next. In real-world ML projects, you'll brainstorm many ideas but only implement the most promising ones due to time and resource constraints.\n",
    "\n",
    "#### Part A: Reflect on your experimental results\n",
    "\n",
    "**Analyze your experiment results by answering the following questions. Reference specific metrics, loss curves, and configuration details from your experiments.**\n",
    "\n",
    "**Questions for reflection:**\n",
    "\n",
    "1. **Which experiment performed best overall, and why?**\n",
    "\n",
    "2. **What patterns did you observe across experiments?**\n",
    "   - Did dropout reduce the train-validation gap? By how much?\n",
    "   - Which learning rate provided the best balance of convergence speed and stability?\n",
    "   - Did wider or deeper architectures help? Or did they overfit?\n",
    "\n",
    "3. **What is your model's biggest remaining weakness?**\n",
    "   - Is there significant overfitting? (Train-val gap > 0.10)\n",
    "   - Are false negatives or false positives the bigger problem?\n",
    "   - Has validation loss plateaued, suggesting you've hit a performance ceiling?\n",
    "\n",
    "4. **What have you learned about this dataset and problem?**\n",
    "   - Is the 21-feature set sufficient, or do you suspect important information is missing?\n",
    "   - Is class imbalance (86% vs 14%) still causing problems despite your experiments?\n",
    "   - Are there diminishing returns from additional model complexity?\n",
    "\n",
    "*------- TODO 26: Write your answer here:*\n",
    "\n",
    "\n",
    "#### Part B: Brainstorm additional improvements\n",
    "\n",
    "Based on your reflection above, now propose *1-2 specific improvements* you would try next from the list. Choose from the techniques below and provide detailed justification for each.\n",
    "\n",
    "1. **Class Weights** - Give more importance to the minority class during training\n",
    "2. **Threshold Tuning** - Adjust the 0.5 decision boundary based on ROC analysis\n",
    "3. **Early Stopping** - Stop training when validation loss stops improving\n",
    "4. **Learning Rate Scheduling** - Gradually decrease learning rate during training\n",
    "5. **Weight Decay (L2 Regularization)** - Add penalty for large weights to reduce overfitting\n",
    "6. **Data Augmentation** - Oversample minority class or use SMOTE for synthetic examples\n",
    "7. **Different Activation Functions** - Experiment with LeakyReLU, ELU, ReLU, etc.\n",
    "\n",
    "> **Important note on feature engineering:** Feature engineering is excluded from this list as it falls outside the scope of neural network optimization techniques. In production, that would be a priority since creating interaction terms _(e.g., BMI × Age, HighBP × HeartDisease)_ or polynomial features could yield 5-10% performance gains, often more than architectural changes.\n",
    "\n",
    "**For each technique you select (1-2 total), provide a detailed analysis by answering:**\n",
    "\n",
    "1. **What specific problem from your reflection does your selection address?**\n",
    "   - Connect directly to weaknesses you identified in Part A\n",
    "   - Reference specific metrics from your experiment results\n",
    "   - Example: \"Our recall is 0.7345, just barely above target, and we have 387 false negatives\"\n",
    "\n",
    "2. **Why is this technique appropriate for this problem?**\n",
    "   - Explain the mechanism: how does this technique work?\n",
    "   - Why would it solve your specific problem?\n",
    "   - Example: \"Class weights force the model to pay more attention to minority class examples during training by increasing their loss contribution\"\n",
    "\n",
    "3. **What results do you expect?**\n",
    "   - Be specific: which metrics should improve and by roughly how much?\n",
    "   - What trade-offs might occur? (e.g., precision vs recall)\n",
    "   - Example: \"Recall should increase to 0.75-0.78 because the model will work harder to identify positive cases. Precision may drop slightly from 0.66 to 0.62 due to more false positives, but F1-score should still improve overall\"\n",
    "\n",
    "4. **How would you implement this? (Implementation complexity)**\n",
    "   - Easy (1-2 line change), Medium (new component), or Hard (major refactor)\n",
    "   - If Easy or Medium, show the code snippet or describe the modification\n",
    "   - Example: \"Easy - Change loss to: `nn.BCEWithLogitsLoss(pos_weight=torch.tensor([3.0]))`\"\n",
    "\n",
    "*------- TODO 27: Write your answer here:*\n",
    "\n",
    "\n",
    "#### Part C: How about combining techniques?\n",
    "\n",
    "Based on all your experiments, you've seen that different techniques can help solve overfitting but combining them is where most improvements can be unlocked. \n",
    "\n",
    "Propose one combined experiment you would run next. Justify your choice and state your expected results.\n",
    "\n",
    "*------- TODO 28: Write your answer here:*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary><h4>Checkpoint – Model Improvement Complete</h4></summary>\n",
    "  \n",
    "  You have systematically improved your model through experimentation:\n",
    "  \n",
    "  - [ ] **Multiple experiments conducted** with different techniques  \n",
    "  - [ ] **Results automatically tracked** for easy comparison  \n",
    "  - [ ] **Best configuration identified** based on key metrics  \n",
    "  - [ ] **Insights documented** about what worked/didn't work and why  \n",
    "  - [ ] **Recommended 1-2 additional improvements** with clear reasoning\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion & Next Steps\n",
    "\n",
    "Congratulations on completing this hands-on deep learning project! You’ve successfully applied neural network fundamentals to a structured data classification task, demonstrating not only technical competence but also an understanding of data preprocessing, model evaluation, and performance optimization.\n",
    "\n",
    "The techniques and workflows you’ve built here extend far beyond this dataset. The same methods can be applied to credit risk assessment, customer churn prediction, quality control, and more challenges in other structured-data domains. To continue your deep learning journey, explore new datasets, experiment with model architectures, and iterate on performance and interpretability.\n",
    "\n",
    "#### Want to make this project portfolio-ready?\n",
    "\n",
    "To showcase this project effectively:\n",
    "\n",
    "1. Clean your notebook: Clear markdown, add an executive summary, and improve on documentation.\n",
    "2. Publish on GitHub: Include model results and key visuals.\n",
    "3. Prepare a short pitch: 2-minute overview of results, impact, and challenges.\n",
    "4. Highlight strengths: Real-world relevance, handling imbalance, model improvement, metric choice, and deployment readiness.\n",
    "\n",
    "> **Remember: Machine learning is iterative.** Every model can be improved, every dataset hides deeper insights, and every project sharpens your intuition as a data scientist. Keep experimenting, stay curious, and keep building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
